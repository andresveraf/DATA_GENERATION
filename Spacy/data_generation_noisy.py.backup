"""
Multi-Country Latin American PII Training Data Generator with Advanced Noise Generation
=======================================================================================
Change the size of the training and development datasets
RUN : python3 data_generation_noisy.py --mode create-dataset --country all --train-size 1000f --dev-size 200 --noise --noise-level 0.5

This module generates realistic customer data for multiple Latin American countries with 
controlled noise patterns specifically designed for Named Entity Recognition (NER) training. 
It creates datasets with labeled entities for customer service and financial NLP applications, 
with enhanced noise generation capabilities that preserve entity boundaries.

Key Features:
- Multi-country PII data generation for Chile, Mexico, Brazil, and Uruguay
- Advanced E1010 overlapping span error prevention (ZERO errors guaranteed)
- Controlled noise injection that preserves entity boundaries
- Named Entity Recognition (NER) annotations with conflict resolution
- spaCy-compatible training data creation (100K+ examples)
- Excel export functionality for data review and validation
- Command-line interface with multiple modes and country selection
- Statistics tracking and reporting

Supported Countries:
- Chile (CL): RUT format, +56 phones, CLP currency, Chilean Spanish
- Mexico (MX): CURP/RFC formats, +52 phones, MXN currency, Mexican Spanish
- Brazil (BR): CPF/RG formats, +55 phones, BRL currency, Portuguese
- Uruguay (UY): Cédula format, +598 phones, UYU currency, Uruguayan Spanish

Supported Entity Types:
- CUSTOMER_NAME: Full names with country-specific conventions
- ID_NUMBER: Country-specific ID formats (RUT/CURP/CPF/Cédula)
- ADDRESS: Country-specific address formats
- PHONE_NUMBER: Country-specific phone formats
- EMAIL: Email addresses with country-appropriate domains
- AMOUNT: Monetary amounts with country currencies
- SEQ_NUMBER: Sequential reference numbers

Enhanced Noise Features:
- Realistic typos and misspellings per country
- Country-specific abbreviations and contractions
- Document formatting variations per country
- Text structure complexity
- Controlled noise that preserves training data quality

Critical E1010 Fix:
- Longest-match-first entity prioritization
- Position overlap prevention with used_positions tracking
- Advanced conflict resolution algorithm
- Empty entity filtering and validation
- Guaranteed zero overlapping span errors

Author: Andrés Vera Figueroa
Date: August 2025
Purpose: Large-scale PII detection model training for Latin American documents
Critical requirement: Zero E1010 errors guaranteed
"""

import random
import spacy
from spacy.tokens import DocBin
from typing import Tuple, Dict, List, Any, Optional
import json
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import argparse
import re
import sys

# Global sequence counter for generating unique sequential IDs
_sequence_counter = 10000

def get_next_sequence() -> int:
    """
    Generate the next sequential number for record identification.
    
    Returns:
        int: The next sequence number (increments from 10000)
    """
    global _sequence_counter
    _sequence_counter += 1
    return _sequence_counter

# -----------------
# Multi-Country Customer Names Database
# -----------------

# Country-specific data organization
COUNTRY_DATA = {
    'chile': {
        'first_names': [
            # Masculine names (common in Chile)
            "AGUSTÍN", "ALEJANDRO", "ALONSO", "ÁLVARO", "ANDRÉS", "ÁXEL", "BAUTISTA", "BENJAMÍN", "BRUNO", "CALEB",
            "CAMILO", "CARLOS", "CRISTÓBAL", "CRISTIAN", "DAMIÁN", "DANIEL", "DAVID", "DIEGO", "EDUARDO", "ELÍAS",
            "EMILIANO", "EMMANUEL", "ENRIQUE", "ESTEBAN", "ETHAN", "FEDERICO", "FERNANDO", "FRANCISCO", "GABRIEL",
            "GAEL", "GASPAR", "GERMÁN", "GUSTAVO", "HERNÁN", "IAN", "IGNACIO", "ISIDORO", "IVÁN", "JAIR", "JAIRO",
            "JASON", "JEREMY", "JHON", "JOAQUÍN", "JORGE", "JUAN", "JULIÁN", "KEVIN", "KIAN", "LEÓN", "LEONARDO",
            "LIAM", "LORENZO", "LUCCA", "LUIS", "MARCELO", "MARCO", "MARTÍN", "MATÍAS", "MATEO", "MAURICIO",
            "MAXIMILIANO", "MIGUEL", "NICOLÁS", "OLIVER", "OMAR", "ORLANDO", "PATRICIO", "PAULO", "PEDRO", "RAFAEL",
            "RAMIRO", "RICARDO", "ROBERTO", "RODRIGO", "RUBÉN", "SAMUEL", "SANTIAGO", "SEBASTIÁN", "SIMÓN", "THIAGO",
            "TOBÍAS", "TOMÁS", "VALENTINO", "VÍCTOR", "VICENTE", "WALTER", "XANDER", "ZAHIR",
            
            # Feminine names (common in Chile)
            "AGUSTINA", "AINHOA", "AITANA", "ALBA", "ALEJANDRA", "ALEXA", "ALEXANDRA", "ALMENDRA", "AMANDA", "AMELIA",
            "ANAÍS", "ANTONELLA", "ANTONIA", "ARANTXA", "ARIADNA", "AROHA", "AZUL", "BELÉN", "BLANCA", "BRISA",
            "CAMILA", "CARLA", "CAROLINA", "CATALINA", "CELIA", "CLARA", "CLAUDIA", "CONSTANZA", "DANIELA", "DÉBORA",
            "DIANA", "DOMINIQUE", "ELISA", "ELIZABETH", "EMILIA", "EMMA", "ESMERALDA", "ESTEFANÍA", "FERNANDA",
            "FLORENCIA", "FRANCISCA", "GABRIELA", "GIOVANNA", "ISABELLA", "IVANNA", "JAVIERA", "JIMENA", "JOSEFINA",
            "JUANITA", "JULIETA", "KARINA", "KARLA", "KATIA", "KIARA", "LARA", "LAURA", "LAYLA", "LILA", "LUCIANA",
            "LUISA", "LUNA", "MACARENA", "MAGDALENA", "MANUELA", "MARÍA", "MARTINA", "MATILDA", "MÍA", "MILA",
            "MIREYA", "NATALIA", "NEREA", "NICOLE", "NOELIA", "OLIVIA", "PALOMA", "PAOLA", "PAULINA", "PAZ",
            "PENÉLOPE", "RENATA", "ROCÍO", "ROSA", "ROMINA", "ROSARIO", "SALOMÉ", "SAMANTHA", "SARA", "SOFÍA", "SOL",
            "TAMARA", "VALENTINA", "VALERIA", "VANIA", "VERÓNICA", "VICTORIA", "VIOLETA", "XIMENA", "YASNA",
            "YOLANDA", "ZOE"
        ],
    },
    'mexico': {
        'first_names': [
            # Traditional Mexican masculine names
            "ALEJANDRO", "ANDRÉS", "ANTONIO", "CARLOS", "DANIEL", "DAVID", "DIEGO", "EDUARDO", "EMILIO", "FERNANDO",
            "FRANCISCO", "GABRIEL", "GUSTAVO", "HUGO", "IGNACIO", "JAVIER", "JESÚS", "JORGE", "JOSÉ", "JUAN",
            "JULIO", "LEONARDO", "LUIS", "MANUEL", "MARCO", "MARIO", "MIGUEL", "PABLO", "PEDRO", "RAFAEL",
            "RAMÓN", "RAÚL", "RICARDO", "ROBERTO", "RODOLFO", "SALVADOR", "SANTIAGO", "SERGIO", "VÍCTOR",
            
            # Traditional Mexican feminine names
            "ADRIANA", "ALEJANDRA", "ANA", "ANDREA", "ÁNGELA", "BEATRIZ", "CARMEN", "CAROLINA", "CLAUDIA", "CRISTINA",
            "DANIELA", "ELENA", "ELIZABETH", "ESPERANZA", "FERNANDA", "GABRIELA", "GUADALUPE", "ISABELLA", "JESSICA",
            "JULIA", "LAURA", "LETICIA", "LUCÍA", "MARÍA", "MARTHA", "MÓNICA", "NATALIA", "PATRICIA", "ROSA",
            "SANDRA", "SOFÍA", "SUSANA", "TERESA", "VALERIA", "VERÓNICA", "VICTORIA", "YOLANDA",
            
            # Indigenous Mexican names
            "XÓCHITL", "ITZEL", "YARETZI", "CITLALI", "NAYELI", "ARELY", "ITZAYANA", "XITLALI", "YANELI", "ANAHÍ",
            "XIMENA", "YAMILET", "CITLALY", "NAOMI", "QUETZALI", "TLÁLOC", "CUAUHTÉMOC", "NEZAHUALCÓYOTL",
            "MOCTEZUMA", "TENOCHTITLAN", "TONATIUH", "MALINTZIN", "IZEL", "ITZÁMIN", "XITLA"
        ],
    },
    'brazil': {
        'first_names': [
            # Brazilian Portuguese masculine names
            "ALEXANDRE", "ANDRÉ", "ANTÔNIO", "BRUNO", "CARLOS", "DANIEL", "EDUARDO", "FÁBIO", "FERNANDO", "GABRIEL",
            "GUSTAVO", "HENRIQUE", "JOÃO", "JOSÉ", "LEONARDO", "LUCAS", "LUÍS", "MARCELO", "MARCOS", "MATEUS",
            "PEDRO", "RAFAEL", "RICARDO", "RODRIGO", "THIAGO", "VINÍCIUS", "CAIO", "FELIPE", "GUILHERME", "IGOR",
            "LEANDRO", "MAURÍCIO", "PAULO", "RENATO", "SÉRGIO", "WELLINGTON", "WASHINGTON", "WESLEY",
            
            # Brazilian Portuguese feminine names  
            "ADRIANA", "ANA", "ANDREA", "BEATRIZ", "BIANCA", "CAMILA", "CAROLINA", "CRISTIANE", "DANIELA", "FERNANDA",
            "GABRIELA", "JULIANA", "LARISSA", "LETÍCIA", "LUCIANA", "MARCIA", "MARIA", "MÔNICA", "PATRÍCIA", "PAULA",
            "PRISCILA", "RAFAELA", "SANDRA", "TATIANA", "VANESSA", "VIVIANE", "DÉBORA", "FLÁVIA", "JÉSSICA", "KARINA",
            "LUANA", "RENATA", "SIMONE", "SOLANGE", "TÂNIA", "CLÁUDIA", "ELIANE", "FABIANA", "GISELE", "HELENA"
        ],
    },
    'uruguay': {
        'first_names': [
            # Uruguayan masculine names (similar to Argentine/Chilean with some variations)
            "AGUSTÍN", "ALEJANDRO", "ANDRÉS", "ANTONIO", "CARLOS", "DANIEL", "DIEGO", "EDUARDO", "FERNANDO", "FRANCISCO",
            "GABRIEL", "GONZALO", "GUSTAVO", "IGNACIO", "JAVIER", "JOAQUÍN", "JORGE", "JOSÉ", "JUAN", "LEONARDO",
            "LUIS", "MANUEL", "MARCELO", "MARIO", "MARTÍN", "MATÍAS", "MIGUEL", "NICOLÁS", "PABLO", "PEDRO",
            "RAFAEL", "RAMIRO", "RICARDO", "ROBERTO", "RODRIGO", "SANTIAGO", "SEBASTIÁN", "VÍCTOR", "WALTER",
            
            # Uruguayan feminine names
            "ADRIANA", "ALEJANDRA", "ANA", "ANDREA", "BEATRIZ", "CAROLINA", "CLAUDIA", "CRISTINA", "DANIELA", "ELENA",
            "FERNANDA", "GABRIELA", "GRACIELA", "ISABEL", "LAURA", "LETICIA", "LUCÍA", "MARÍA", "MARTHA", "MÓNICA",
            "NATALIA", "PATRICIA", "PAULA", "ROSA", "SANDRA", "SILVIA", "SOFÍA", "SUSANA", "VALERIA", "VERÓNICA",
            "VIRGINIA", "VIVIANA", "ALEJANDRA", "CECILIA", "FLORENCIA", "MAGDALENA", "MACARENA", "VALENTINA"
        ],
    }
}

# Backwards compatibility - keep Chilean data accessible
chilean_first_names = COUNTRY_DATA['chile']['first_names']

# Add second names to country data
COUNTRY_DATA['chile']['second_names'] = [
    # Masculine second names
    "CARLOS", "JOSÉ", "LUIS", "ANTONIO", "MANUEL", "FRANCISCO", "MIGUEL", "RAFAEL", "FERNANDO", "RICARDO",
    "ALBERTO", "EDUARDO", "ALEJANDRO", "ANDRÉS", "ROBERTO", "PEDRO", "DANIEL", "GABRIEL", "DIEGO", "SEBASTIÁN",
    "PABLO", "ARTURO", "ENRIQUE", "JOAQUÍN", "NICOLÁS", "FELIPE", "IGNACIO", "ESTEBAN", "RODRIGO", "PATRICIO",
    
    # Feminine second names
    "JOSÉ", "MARÍA", "ISABEL", "CRISTINA", "ELENA", "TERESA", "PATRICIA", "CARMEN", "ROSA", "ANA",
    "LAURA", "BEATRIZ", "ESPERANZA", "GUADALUPE", "DOLORES", "PILAR", "MERCEDES", "SOLEDAD", "AMPARO", "ROCÍO",
    "CONCEPCIÓN", "INMACULADA", "ÁNGELES", "REMEDIOS", "VICTORIA", "GLORIA", "PAZ", "FE", "CARIDAD", "NIEVES"
]

COUNTRY_DATA['mexico']['second_names'] = [
    # Mexican masculine second names
    "MARÍA", "JOSÉ", "LUIS", "ANTONIO", "MANUEL", "FRANCISCO", "MIGUEL", "RAFAEL", "CARLOS", "JESÚS",
    "GUADALUPE", "ÁNGEL", "RAMÓN", "ALEJANDRO", "FERNANDO", "JAVIER", "ALBERTO", "EDUARDO", "ENRIQUE", "SALVADOR",
    
    # Mexican feminine second names  
    "JOSÉ", "MARÍA", "GUADALUPE", "CARMEN", "TERESA", "ISABEL", "ESPERANZA", "ROSA", "ELENA", "PATRICIA",
    "CONCEPCIÓN", "DOLORES", "SOCORRO", "LUZ", "AMPARO", "REFUGIO", "PILAR", "SOLEDAD", "REMEDIOS", "TRINIDAD"
]

COUNTRY_DATA['brazil']['second_names'] = [
    # Brazilian masculine second names (often use "de" constructions)
    "JOSÉ", "JOÃO", "ANTÔNIO", "FRANCISCO", "CARLOS", "PAULO", "PEDRO", "LUCAS", "LUÍS", "MARCOS",
    "RAFAEL", "DANIEL", "MARCELO", "BRUNO", "RODRIGO", "FERNANDO", "GUSTAVO", "EDUARDO", "GABRIEL", "LEONARDO",
    
    # Brazilian feminine second names
    "MARIA", "ANA", "FRANCISCA", "ANTÔNIA", "ADRIANA", "JULIANA", "MÁRCIA", "FERNANDA", "PATRÍCIA", "ALINE",
    "CRISTINA", "CAMILA", "CARLA", "REGINA", "VERA", "LÚCIA", "HELENA", "SILVIA", "MÔNICA", "PAULA"
]

COUNTRY_DATA['uruguay']['second_names'] = [
    # Uruguayan masculine second names
    "JOSÉ", "MARÍA", "CARLOS", "LUIS", "ANTONIO", "MANUEL", "FRANCISCO", "MIGUEL", "RAFAEL", "ALBERTO",
    "EDUARDO", "ALEJANDRO", "ANDRÉS", "ROBERTO", "PEDRO", "DANIEL", "GABRIEL", "DIEGO", "SEBASTIÁN", "PABLO",
    
    # Uruguayan feminine second names
    "MARÍA", "JOSÉ", "ISABEL", "CRISTINA", "ELENA", "TERESA", "PATRICIA", "CARMEN", "ROSA", "ANA",
    "LAURA", "BEATRIZ", "ESPERANZA", "MERCEDES", "SOLEDAD", "AMPARO", "ROCÍO", "VICTORIA", "GLORIA", "PAZ"
]

# Backwards compatibility
chilean_second_names = COUNTRY_DATA['chile']['second_names']

# Keep legacy 'names' for backward compatibility
first_names = chilean_first_names
second_names = chilean_second_names

# Add surnames to country data
COUNTRY_DATA['chile']['surnames'] = [
    # Most common Chilean surnames
    "GONZÁLEZ", "MUÑOZ", "ROJAS", "DÍAZ", "PÉREZ", "SOTO", "CONTRERAS", "SILVA", "MARTÍNEZ", "SEPÚLVEDA",
    "MORALES", "RODRÍGUEZ", "LÓPEZ", "ARAYA", "FUENTES", "HERNÁNDEZ", "TORRES", "ESPINOZA", "FLORES",
    "CASTILLO", "REYES", "VALENZUELA", "VARGAS", "RAMÍREZ", "GUTIÉRREZ", "HERRERA", "ÁLVAREZ", "VÁSQUEZ",
    "TAPIA", "SÁNCHEZ", "FERNÁNDEZ", "CARRASCO", "CORTÉS", "GÓMEZ", "JARA", "VERGARA", "RIVERA", "NÚÑEZ",
    "BRAVO", "FIGUEROA", "RIQUELME", "MOLINA", "VERA", "SANDOVAL", "GARCÍA", "VEGA", "MIRANDA", "ROMERO",
    "ORTIZ", "SALAZAR", "CAMPOS", "ORELLANA", "OLIVARES", "GARRIDO", "PARRA", "GALLARDO", "SAAVEDRA",
    "ALARCON", "AGUILERA", "PEÑA", "ZÚÑIGA", "RUIZ", "MEDINA", "GUZMÁN", "ESCOBAR", "NAVARRO", "PIZARRO",
    "GODOY", "CÁCERES", "HENRÍQUEZ", "ARAVENA", "MORENO", "LEIVA", "SALINAS", "VIDAL", "LAGOS", "VALDÉS",
    "RAMOS", "MALDONADO", "JIMÉNEZ", "YÁÑEZ", "BUSTOS", "ORTEGA", "PALMA", "CARVAJAL", "PINO", "ALVARADO",
    "PAREDES", "GUERRERO", "MORA", "POBLETE", "SÁEZ", "VENEGAS", "SANHUEZA", "BUSTAMANTE", "TORO",
    "NAVARRETE", "CÁRDENAS", "CORNEJO", "ESPINOSA", "IBARRA", "LAGOS", "MENA", "ÓRDENES", "PARADA",
    "PUEBLA", "QUEZADA", "ROBLES", "SEGOVIA", "URRUTIA", "VILLANUEVA", "ANDRADE", "CARVALLO", "DONOSO"
]

COUNTRY_DATA['mexico']['surnames'] = [
    # Most common Mexican surnames
    "HERNÁNDEZ", "GARCÍA", "MARTÍNEZ", "LÓPEZ", "GONZÁLEZ", "RODRÍGUEZ", "PÉREZ", "SÁNCHEZ", "RAMÍREZ", "CRUZ",
    "FLORES", "GÓMEZ", "MORALES", "VÁZQUEZ", "JIMÉNEZ", "RUIZ", "HERNÁN", "DÍAZ", "MORENO", "MUÑOZ",
    "ÁLVAREZ", "ROMERO", "GUTIÉRREZ", "TORRES", "MENDOZA", "VARGAS", "CASTILLO", "ORTEGA", "REYES", "DELGADO",
    "GUERRERO", "MEDINA", "AGUILAR", "RAMOS", "CERVANTES", "HERRERA", "LARA", "DOMÍNGUEZ", "CASTRO", "VARELA",
    "ORTIZ", "RUBIO", "MARÍN", "IGLESIAS", "NUÑEZ", "PEÑA", "RÍOS", "ALONSO", "GARRIDO", "GALLEGO",
    # Indigenous Mexican surnames
    "XÓLOTL", "ITURBIDE", "CUAUHTÉMOC", "MOCTEZUMA", "TLACAELEL", "NEZAHUALCÓYOTL", "TONATIUH", "COATLICUE"
]

COUNTRY_DATA['brazil']['surnames'] = [
    # Most common Brazilian surnames
    "SILVA", "SANTOS", "OLIVEIRA", "SOUZA", "RODRIGUES", "FERREIRA", "ALVES", "PEREIRA", "LIMA", "GOMES",
    "RIBEIRO", "CARVALHO", "ALMEIDA", "LOPES", "SOARES", "FERNANDES", "VIEIRA", "BARBOSA", "ROCHA", "DIAS",
    "MONTEIRO", "CARDOSO", "REIS", "ARAÚJO", "CAVALCANTI", "NASCIMENTO", "AZEVEDO", "COSTA", "PINTO", "TEIXEIRA",
    "MENDES", "MOREIRA", "CORREIA", "MARTINS", "RAMOS", "NUNES", "FREITAS", "CAMPOS", "MIRANDA", "FONSECA",
    "MACHADO", "MOURA", "MELO", "CUNHA", "PIRES", "CASTRO", "ANDRADE", "COELHO", "FARIAS", "BATISTA"
]

COUNTRY_DATA['uruguay']['surnames'] = [
    # Most common Uruguayan surnames (mix of Spanish and some Italian influence)
    "RODRÍGUEZ", "GONZÁLEZ", "GARCÍA", "LÓPEZ", "MARTÍNEZ", "PÉREZ", "FERNÁNDEZ", "SÁNCHEZ", "DÍAZ", "ÁLVAREZ",
    "ROMERO", "VARGAS", "CASTRO", "RAMOS", "MORALES", "ORTEGA", "DELGADO", "JIMÉNEZ", "RUIZ", "HERNÁNDEZ",
    "SILVA", "TORRES", "FLORES", "VEGA", "MEDINA", "AGUILAR", "HERRERA", "MENDOZA", "GUERRERO", "NÚÑEZ",
    "PEÑA", "RÍOS", "GÓMEZ", "CONTRERAS", "GUTIÉRREZ", "REYES", "ESTRADA", "PAREDES", "DOMÍNGUEZ", "LARA",
    # Italian influence surnames common in Uruguay
    "FERRARI", "ROSSI", "BRUNO", "MARTINO", "ROMANO", "RICCI", "COSTA", "MAZZA", "RUSSO", "GRECO"
]

# Backwards compatibility  
chilean_surnames = COUNTRY_DATA['chile']['surnames']

# Add addresses and cities to country data
COUNTRY_DATA['chile']['streets'] = [
    # Santiago - Main avenues and streets
    "Av. Libertador Bernardo O'Higgins",  # Main avenue in Santiago
    "Av. Apoquindo",                      # Upscale area in Las Condes
    "Av. Vitacura",                       # Major avenue in Vitacura
    "Av. Los Leones",                     # Important street in Providencia
    "Av. Providencia",                    # Main avenue in Providencia
    "Calle San Diego",                    # Historic street in Santiago Centro
    "Calle Lira",                         # Traditional street
    "Calle Portugal",                     # Street in Santiago Centro
    "Pasaje Los Álamos",                  # Residential passage
    "Pasaje El Roble",                    # Small residential street
    "Calle Merced",                       # Historic downtown street
    "Av. La Florida",                     # Avenue in La Florida commune
    "Calle Pío Nono",                     # Famous street in Bellavista
    "Calle Suecia",                       # Street in Ñuñoa
    "Calle Santa Isabel",                 # Street in Santiago Centro
    
    # Additional Chilean streets for variety
    "Av. Irarrázaval",                    # Important east-west avenue
    "Av. Tobalaba",                       # Major north-south avenue
    "Calle Huérfanos",                    # Historic downtown street
    "Av. Pedro de Valdivia",              # Avenue in Ñuñoa/Providencia
    "Calle Ahumada",                      # Pedestrian street downtown
    "Av. Manuel Montt",                   # Street in Providencia
    "Calle Bellavista",                   # Street in Bellavista neighborhood
    "Av. Vicuña Mackenna",                # Major diagonal avenue
    "Calle Nueva de Lyon",                # Street in Providencia
    "Av. Salvador",                       # Avenue in Providencia/Ñuñoa
    "Calle Román Díaz",                   # Residential street
    "Av. Kennedy",                        # Avenue in Las Condes
    "Calle Las Flores",                   # Residential street
    "Av. Américo Vespucio",               # Ring road around Santiago
    "Calle Los Aromos",                   # Residential street name
]

COUNTRY_DATA['mexico']['streets'] = [
    # Mexican streets with colonias and typical names
    "Av. Insurgentes Sur",                # Major avenue in Mexico City
    "Av. Reforma",                        # Famous avenue in Mexico City
    "Calle Francisco I. Madero",          # Historic street in Centro
    "Av. Juárez",                         # Important avenue
    "Calle 16 de Septiembre",             # Independence Day street
    "Av. Universidad",                    # University avenue
    "Calle Puebla",                       # Street named after state
    "Av. Chapultepec",                    # Street in Roma Norte
    "Calle Orizaba",                      # Street in Roma Norte colonia
    "Av. Álvaro Obregón",                 # Avenue named after president
    "Calle Insurgentes Norte",            # Northern section of Insurgentes
    "Av. Revolución",                     # Avenue commemorating the Revolution
    "Eje Central Lázaro Cárdenas",        # Major north-south axis
    "Calle Durango",                      # Street in Colonia Roma
    "Calle Londres",                      # Street in Zona Rosa
    "Av. Coyoacán",                       # Avenue to Coyoacán
    "Calle Medellín",                     # Street in Colonia Roma
    "Av. Cuauhtémoc",                     # Avenue named after Aztec emperor
    "Calle Monterrey",                    # Street in Colonia Roma
    "Av. División del Norte"              # Major south avenue
]

COUNTRY_DATA['brazil']['streets'] = [
    # Brazilian streets with typical Portuguese naming
    "Av. Paulista",                       # Famous avenue in São Paulo
    "Rua Augusta",                        # Well-known street in São Paulo
    "Av. Ipiranga",                       # Avenue in São Paulo
    "Rua da Consolação",                  # Street in São Paulo
    "Av. Copacabana",                     # Famous avenue in Rio
    "Rua Visconde de Pirajá",             # Street in Ipanema
    "Av. Atlântica",                      # Beachfront avenue in Rio
    "Rua do Ouvidor",                     # Historic street in Rio Centro
    "Av. Presidente Vargas",              # Avenue named after president
    "Rua Direita",                        # Traditional street name
    "Av. Brasil",                         # Avenue named after country
    "Rua das Flores",                     # "Street of the Flowers"
    "Av. Santos Dumont",                  # Aviation pioneer avenue
    "Rua Barão de Itapetininga",          # Street with noble title
    "Av. São João",                       # Saint John avenue
    "Rua Oscar Freire",                   # Upscale shopping street
    "Av. Brigadeiro Faria Lima",          # Business district avenue
    "Rua 25 de Março",                    # Commercial street
    "Av. Rebouças",                       # Major avenue in São Paulo
    "Rua da Liberdade"                    # Street in Japanese district
]

COUNTRY_DATA['uruguay']['streets'] = [
    # Uruguayan streets from Montevideo and other cities
    "Bvar. Artigas",                      # Boulevard named after national hero
    "Av. 18 de Julio",                    # Main avenue in Montevideo
    "Av. Italia",                         # Important avenue
    "Av. Rivera",                         # Major avenue
    "Av. Gral. Flores",                   # Avenue named after general
    "Calle Colonia",                      # Historic street
    "Calle Soriano",                      # Street in downtown Montevideo
    "Calle Yi",                           # Short downtown street
    "Calle Durazno",                      # Street named after fruit
    "Pasaje Pérez Castellanos",           # Small passage
    "Calle Canelones",                    # Street named after department
    "Calle San José",                     # Street named after saint
    "Calle Río Branco",                   # Street named after river
    "Calle Cerro Largo",                  # Street named after department
    "Calle Andes",                        # Street named after mountain range
    "Av. Millán",                         # Major avenue
    "Calle Mercedes",                     # Street named after department
    "Av. Agraciada",                      # Historic avenue
    "Calle Uruguay",                      # Street named after country
    "Bvar. Batlle y Ordóñez"              # Boulevard named after president
]

COUNTRY_DATA['chile']['cities'] = [
    "Santiago",           # Capital and largest city
    "Valparaíso",        # Main port city
    "Concepción",        # Southern major city
    "La Serena",         # Northern city
    "Antofagasta",       # Northern mining city
    "Temuco",            # Southern city
    "Rancagua",          # Central valley city
    "Talca",             # Central valley city
    "Arica",             # Northernmost city
    "Iquique",           # Northern port city
    "Puerto Montt",      # Southern port city
    "Chillán",           # Bio-Bio region
    "Copiapó",           # Atacama region
    "Osorno",            # Los Lagos region
    "Valdivia"           # Rivers city
]

COUNTRY_DATA['mexico']['cities'] = [
    "Ciudad de México",   # Capital and largest city
    "Guadalajara",        # Second largest city
    "Monterrey",          # Major industrial city
    "Puebla",             # Historic colonial city
    "Tijuana",            # Border city with USA
    "León",               # Leather industry city
    "Ciudad Juárez",      # Border city
    "Torreón",            # Northern industrial city
    "Querétaro",          # Central Mexico city
    "San Luis Potosí",    # Mining city
    "Mérida",             # Yucatan capital
    "Aguascalientes",     # Central city
    "Mexicali",           # Baja California capital
    "Culiacán",           # Sinaloa capital
    "Acapulco"            # Pacific coast resort
]

COUNTRY_DATA['brazil']['cities'] = [
    "São Paulo",          # Largest city in South America
    "Rio de Janeiro",     # Former capital, major tourist destination
    "Belo Horizonte",     # Major southeastern city
    "Brasília",           # Current capital
    "Salvador",           # Historic northeastern city
    "Fortaleza",          # Major northeastern city
    "Manaus",             # Amazon region capital
    "Curitiba",           # Southern city
    "Recife",             # Major northeastern port
    "Goiânia",            # Central Brazil city
    "Belém",              # Amazon river port
    "Guarulhos",          # Greater São Paulo
    "Campinas",           # Technology hub
    "São Luís",           # Northeastern capital
    "Nova Iguaçu"         # Rio de Janeiro metropolitan area
]

COUNTRY_DATA['uruguay']['cities'] = [
    "Montevideo",         # Capital and largest city
    "Salto",              # Second largest city
    "Paysandú",           # Important river port
    "Las Piedras",        # Suburban city near Montevideo
    "Rivera",             # Border city with Brazil
    "Maldonado",          # Coastal city
    "Tacuarembó",         # Northern city
    "Melo",               # Eastern city
    "Mercedes",           # Western city
    "Artigas",            # Northern border city
    "Minas",              # Central city
    "San José de Mayo",   # Central city
    "Durazno",            # Central city
    "Florida",            # Central city
    "Treinta y Tres"      # Eastern city
]

# Backwards compatibility for addresses
chilean_streets = COUNTRY_DATA['chile']['streets']  
chilean_cities = COUNTRY_DATA['chile']['cities']

# Keep backward compatibility
surnames = chilean_surnames
streets = chilean_streets
cities = chilean_cities

# -----------------
# Chilean Organizations Database
# -----------------
# Multi-Country Organizations Database
# -----------------

# Add organizations to country data
COUNTRY_DATA['chile']['organizations'] = [
    # Banks and Financial Institutions
    "Banco de Chile", "Banco Santander Chile", "BancoEstado", "Banco de Crédito e Inversiones",
    "Banco Security", "Banco Falabella", "Banco Ripley", "Banco Itaú Chile",
    
    # Retail and Commerce
    "Falabella", "Ripley", "Paris", "La Polar", "Hites", "Corona", "Easy", "Homecenter Sodimac",
    "Líder", "Jumbo", "Santa Isabel", "Tottus", "Unimarc", "Ekono",
    
    # Telecommunications
    "Entel", "Movistar Chile", "Claro Chile", "WOM", "VTR", "GTD Manquehue",
    
    # Utilities and Services
    "Chilectra", "CGE", "Metrogas", "Aguas Andinas", "ESSAL", "ESSBIO",
    
    # Mining and Industry
    "CODELCO", "Escondida", "Anglo American", "Antofagasta Minerals", "SQM",
    
    # Healthcare
    "Clínica Las Condes", "Clínica Alemana", "Hospital Clínico UC", "FONASA", "Isapre Banmédica",
    
    # Education
    "Universidad de Chile", "Pontificia Universidad Católica", "Universidad de Santiago",
    
    # Government and Public
    "Municipalidad de Santiago", "Servicio de Impuestos Internos", "Registro Civil",
    "Carabineros de Chile", "SEREMI de Salud", "JUNAEB"
]

COUNTRY_DATA['mexico']['organizations'] = [
    # Banks and Financial Institutions
    "BBVA México", "Banorte", "Santander México", "HSBC México", "Citibanamex", "Banco Azteca",
    "Scotiabank México", "Banco Inbursa", "BanCoppel", "Banco del Bajío",
    
    # Retail and Commerce
    "Liverpool", "Palacio de Hierro", "Sears México", "Coppel", "Elektra", "Soriana",
    "Walmart México", "Comercial Mexicana", "OXXO", "7-Eleven México",
    
    # Telecommunications
    "Telcel", "Movistar México", "AT&T México", "Izzi", "Megacable", "Totalplay",
    
    # Government and Public
    "IMSS", "ISSSTE", "INFONAVIT", "SAT", "INE", "SEDENA", "SEMAR", "Guardia Nacional",
    "Secretaría de Salud", "SEP", "CONACYT",
    
    # Energy and Industry
    "PEMEX", "CFE", "Grupo México", "CEMEX", "Bimbo", "América Móvil",
    
    # Education
    "UNAM", "IPN", "ITESM", "UAM", "Universidad de Guadalajara"
]

COUNTRY_DATA['brazil']['organizations'] = [
    # Banks and Financial Institutions
    "Banco do Brasil", "Itaú Unibanco", "Bradesco", "Santander Brasil", "Caixa Econômica Federal",
    "Banco Inter", "Nubank", "BTG Pactual", "Banco Safra", "Banrisul",
    
    # Retail and Commerce
    "Magazine Luiza", "Via Varejo", "Lojas Americanas", "Pão de Açúcar", "Carrefour Brasil",
    "Walmart Brasil", "Extra", "Casas Bahia", "Renner", "C&A Brasil",
    
    # Telecommunications
    "Vivo", "Claro Brasil", "TIM Brasil", "Oi", "Nextel Brasil", "Algar Telecom",
    
    # Government and Public
    "Receita Federal", "INSS", "SUS", "Ministério da Saúde", "IBGE", "Polícia Federal",
    "Correios", "BNDES", "Petrobras", "Eletrobras",
    
    # Industry and Energy
    "Vale", "JBS", "Ambev", "Gerdau", "CSN", "Embraer",
    
    # Education
    "USP", "UNICAMP", "UFRJ", "UFMG", "UnB", "PUC-SP"
]

COUNTRY_DATA['uruguay']['organizations'] = [
    # Banks and Financial Institutions
    "Banco República", "Banco Santander Uruguay", "Banco Itaú Uruguay", "BBVA Uruguay",
    "Banco de la Nación Argentina", "Citibank Uruguay", "Scotiabank Uruguay",
    
    # Retail and Commerce
    "Tienda Inglesa", "Disco", "Devoto", "Ta-Ta", "Géant", "Farmashop", "Red Pagos",
    
    # Telecommunications
    "Antel", "Movistar Uruguay", "Claro Uruguay", "Dedicado",
    
    # Government and Public
    "DGI", "BPS", "ASSE", "UdelaR", "INAU", "MTOP", "Ministerio de Salud",
    "Intendencia de Montevideo", "Policía Nacional", "Bomberos",
    
    # Utilities and Services
    "UTE", "OSE", "ANCAP", "AFE", "Puerto de Montevideo",
    
    # Education
    "Universidad de la República", "Universidad Católica del Uruguay", "Universidad ORT"
]

# Backwards compatibility
chilean_organizations = COUNTRY_DATA['chile']['organizations']

# Keep backward compatibility
organizations = chilean_organizations

# -----------------
# Multi-Country Name Generation Functions
# -----------------

def generate_name_components(country: str = "chile",
                           include_second_name: bool = True, 
                           second_name_probability: float = 0.4,
                           include_second_surname: bool = True, 
                           second_surname_probability: float = 0.8) -> Tuple[str, str, str]:
    """
    Generate realistic name components for the specified country.
    
    Creates a complete name with culturally appropriate variations:
    - First name (always included)
    - Optional second name (e.g., "Juan Carlos")
    - Paternal surname (always included)
    - Optional maternal surname (e.g., "González Rodríguez")
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        include_second_name (bool): Whether to include a second name
        second_name_probability (float): Probability of adding a second name
        include_second_surname (bool): Whether to include a second surname
        second_surname_probability (float): Probability of adding a second surname
        
    Returns:
        Tuple[str, str, str]: A tuple containing:
            - str: First name only
            - str: Full name part (first name + optional second name)
            - str: Complete surname (paternal + optional maternal)
    """
    # Fallback to Chile if country not found
    if country not in COUNTRY_DATA:
        country = "chile"
        
    country_info = COUNTRY_DATA[country]
    
    # 1. Generate first name
    first_name = random.choice(country_info['first_names'])
    full_name_part = first_name
    
    # 2. Generate optional second name
    if include_second_name and random.random() < second_name_probability:
        second_name = random.choice(country_info['first_names'])
        while second_name == first_name:  # Ensure second name is different
            second_name = random.choice(country_info['first_names'])
        full_name_part = f"{first_name} {second_name}"
        
    # 3. Generate paternal surname
    paternal_surname = random.choice(country_info['surnames'])
    complete_surname = paternal_surname
    
    # 4. Generate optional maternal surname
    if include_second_surname and random.random() < second_surname_probability:
        maternal_surname = random.choice(country_info['surnames'])
        while maternal_surname == paternal_surname:  # Ensure surnames are different
            maternal_surname = random.choice(country_info['surnames'])
        complete_surname = f"{paternal_surname} {maternal_surname}"
        
    return first_name, full_name_part, complete_surname

def generate_chilean_name_components(include_second_name: bool = True, 
                                   second_name_probability: float = 0.4,
                                   include_second_surname: bool = True, 
                                   second_surname_probability: float = 0.8) -> Tuple[str, str, str]:
    """
    Generate Chilean name components with enhanced second surname support.
    (Backwards compatibility wrapper)
    """
    return generate_name_components("chile", include_second_name, second_name_probability, 
                                   include_second_surname, second_surname_probability)

def generate_phone(country: str = "chile") -> str:
    """
    Generate a realistic phone number for the specified country WITHOUT international prefixes.
    
    Uses realistic local formats as they appear in documents:
    - Chile: 987654321, 9 8765 4321, 987-654-321 (mobile) or 223456789, 2-2345-6789 (landline)
    - Mexico: 5512345678, 55 1234 5678, 55-1234-5678 (mobile) or 8112345678 (landline)
    - Brazil: 11987654321, 11 98765-4321, (11) 98765-4321 (mobile) or 1123456789 (landline)
    - Uruguay: 91234567, 91 234 567, 91-234-567 (mobile) or 24123456 (landline)
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Realistic local phone number format
    """
    if country == "chile":
        # 80% mobile, 20% landline
        if random.random() < 0.8:
            # Mobile phone (9XXXXXXXX) - various formats
            base_number = f"9{random.randint(10000000,99999999)}"
            format_choice = random.choice([
                lambda n: n,                                    # 987654321 (all together)
                lambda n: f"{n[0]} {n[1:5]} {n[5:]}",          # 9 8765 4321 (spaced)
                lambda n: f"{n[0:3]}-{n[3:6]}-{n[6:]}",        # 987-654-321 (hyphens)
                lambda n: f"{n[0:3]} {n[3:6]} {n[6:]}",        # 987 654 321 (all spaced)
            ])
            return format_choice(base_number)
        else:
            # Santiago landline (2XXXXXXXX)
            base_number = f"2{random.randint(20000000,29999999)}"
            format_choice = random.choice([
                lambda n: n,                                    # 223456789 (all together)
                lambda n: f"{n[0]}-{n[1:5]}-{n[5:]}",          # 2-2345-6789 (hyphens)
                lambda n: f"{n[0:3]} {n[3:7]} {n[7:]}",        # 223 4567 89 (spaced)
            ])
            return format_choice(base_number)
    
    elif country == "mexico":
        # 85% mobile, 15% landline
        if random.random() < 0.85:
            # Mobile phone (city_code + 10 digits total)
            city_code = random.choice([55, 33, 81, 222, 664])  # Mexico City, Guadalajara, Monterrey, Puebla, Tijuana
            remaining_digits = 10 - len(str(city_code))
            phone_number = f"{city_code}{random.randint(10**(remaining_digits-1), 10**remaining_digits-1)}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 5512345678 (all together)
                lambda n: f"{n[:2]} {n[2:6]} {n[6:]}",         # 55 1234 5678 (spaced)
                lambda n: f"{n[:2]}-{n[2:6]}-{n[6:]}",         # 55-1234-5678 (hyphens)
                lambda n: f"({n[:2]}) {n[2:6]}-{n[6:]}",       # (55) 1234-5678 (parentheses)
            ])
            return format_choice(phone_number)
        else:
            # Landline (same format as mobile but different pattern)
            city_code = random.choice([55, 33, 81, 222, 664])
            remaining_digits = 10 - len(str(city_code))
            phone_number = f"{city_code}{random.randint(10**(remaining_digits-1), 10**remaining_digits-1)}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 8112345678 (all together)
                lambda n: f"{n[:2]} {n[2:6]} {n[6:]}",         # 81 1234 5678 (spaced)
                lambda n: f"({n[:2]}) {n[2:6]}-{n[6:]}",       # (81) 1234-5678 (parentheses)
            ])
            return format_choice(phone_number)
    
    elif country == "brazil":
        # 90% mobile, 10% landline
        area_code = random.choice([11, 21, 31, 47, 85])  # São Paulo, Rio, Belo Horizonte, Joinville, Fortaleza
        
        if random.random() < 0.9:
            # Mobile phone (XX 9XXXX-XXXX)
            mobile_number = f"9{random.randint(10000000,99999999)}"
            full_number = f"{area_code}{mobile_number}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 11987654321 (all together)
                lambda n: f"{n[:2]} {n[2:7]}-{n[7:]}",         # 11 98765-4321 (area + hyphen)
                lambda n: f"({n[:2]}) {n[2:7]}-{n[7:]}",       # (11) 98765-4321 (parentheses)
                lambda n: f"{n[:2]} {n[2:7]} {n[7:]}",         # 11 98765 4321 (all spaced)
            ])
            return format_choice(full_number)
        else:
            # Landline (XX XXXX-XXXX)
            landline_number = f"{random.randint(20000000,99999999)}"
            full_number = f"{area_code}{landline_number}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 1123456789 (all together)
                lambda n: f"{n[:2]} {n[2:6]}-{n[6:]}",         # 11 2345-6789 (area + hyphen)
                lambda n: f"({n[:2]}) {n[2:6]}-{n[6:]}",       # (11) 2345-6789 (parentheses)
            ])
            return format_choice(full_number)
    
    elif country == "uruguay":
        # 85% mobile, 15% landline
        if random.random() < 0.85:
            # Mobile phone (9XXXXXXX - 8 digits)
            mobile_number = f"9{random.randint(1000000,9999999)}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 91234567 (all together)
                lambda n: f"{n[:2]} {n[2:5]} {n[5:]}",         # 91 234 567 (spaced)
                lambda n: f"{n[:2]}-{n[2:5]}-{n[5:]}",         # 91-234-567 (hyphens)
                lambda n: f"{n[:3]} {n[3:6]} {n[6:]}",         # 912 345 67 (alternative spacing)
            ])
            return format_choice(mobile_number)
        else:
            # Montevideo landline (2XXXXXXX - 8 digits)
            landline_number = f"2{random.randint(1000000,9999999)}"
            
            format_choice = random.choice([
                lambda n: n,                                    # 24123456 (all together)
                lambda n: f"{n[:2]} {n[2:5]} {n[5:]}",         # 24 123 456 (spaced)
                lambda n: f"{n[:4]}-{n[4:]}",                  # 2412-3456 (hyphen)
            ])
            return format_choice(landline_number)
    
    else:
        # Default to Chilean format
        return generate_phone("chile")

def generate_chilean_phone() -> str:
    """
    Generate a realistic Chilean phone number.
    (Backwards compatibility wrapper)
    """
    return generate_phone("chile")

def generate_email(name: str, surname: str, country: str = "chile") -> str:
    """
    Generate a realistic email address using the person's name and surname for any country.
    
    Creates email in format: firstname.lastname@domain.com
    Uses common email providers in each country.
    For double surnames, uses only the paternal (first) surname.
    
    Args:
        name (str): First name of the person
        surname (str): Complete surname (may include paternal and maternal)
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Email address in lowercase
    """
    # Country-specific email domains
    domains = {
        "chile": ["gmail.com", "hotmail.com", "yahoo.com", "outlook.com", "live.cl", "vtr.net"],
        "mexico": ["gmail.com", "hotmail.com", "yahoo.com.mx", "outlook.com", "live.com.mx", "prodigy.net.mx"],
        "brazil": ["gmail.com", "hotmail.com", "yahoo.com.br", "outlook.com", "uol.com.br", "bol.com.br", "terra.com.br"],
        "uruguay": ["gmail.com", "hotmail.com", "yahoo.com", "outlook.com", "live.com.uy", "adinet.com.uy"]
    }
    
    # Use only the first surname for email (paternal surname)
    first_surname = surname.split()[0] if " " in surname else surname
    
    # Remove accents and special characters for email compatibility
    name_clean = name.lower()
    surname_clean = first_surname.lower()
    
    # Remove common accents for all countries
    accent_map = {
        'á': 'a', 'à': 'a', 'ã': 'a', 'â': 'a',
        'é': 'e', 'è': 'e', 'ê': 'e',
        'í': 'i', 'ì': 'i', 'î': 'i',
        'ó': 'o', 'ò': 'o', 'õ': 'o', 'ô': 'o',
        'ú': 'u', 'ù': 'u', 'û': 'u',
        'ñ': 'n', 'ç': 'c'
    }
    
    for accented, plain in accent_map.items():
        name_clean = name_clean.replace(accented, plain)
        surname_clean = surname_clean.replace(accented, plain)
    
    # Select appropriate domains for country
    country_domains = domains.get(country, domains["chile"])
    
    return f"{name_clean}.{surname_clean}@{random.choice(country_domains)}"

def generate_chilean_email(name: str, surname: str) -> str:
    """
    Generate a realistic Chilean email address using the person's name and surname.
    (Backwards compatibility wrapper)
    """
    return generate_email(name, surname, "chile")

def generate_id(country: str = "chile") -> str:
    """
    Generate a realistic identification number for the specified country.
    
    Each country has its own ID format:
    - Chile: RUT format XX.XXX.XXX-X
    - Mexico: CURP format AAAA######AAAAAA## and RFC format
    - Brazil: CPF format XXX.XXX.XXX-XX
    - Uruguay: Cédula format X.XXX.XXX-X
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Formatted identification number
    """
    if country == "chile":
        # Chilean RUT (Rol Único Tributario) with valid check digit
        rut_base = random.randint(7_000_000, 25_000_000)
        s = str(rut_base)[::-1]
        multipliers = [2, 3, 4, 5, 6, 7] * 2
        checksum = sum(int(digit) * multipliers[i] for i, digit in enumerate(s))
        check_digit_val = 11 - (checksum % 11)
        if check_digit_val == 11:
            check_digit = '0'
        elif check_digit_val == 10:
            check_digit = 'K'
        else:
            check_digit = str(check_digit_val)
        
        # Realistic formatting variations
        format_choice = random.random()
        if format_choice < 0.4:  # 40% - Standard format with dots
            rut_str = f"{rut_base:,}".replace(',', '.')
            return f"{rut_str}-{check_digit}"
        elif format_choice < 0.7:  # 30% - Format with commas
            rut_str = f"{rut_base:,}"
            return f"{rut_str}-{check_digit}"
        else:  # 30% - No separators
            return f"{rut_base}-{check_digit}"
    
    elif country == "mexico":
        # Mexican CURP (Clave Única de Registro de Población) - simplified but valid structure
        if random.random() < 0.7:  # 70% CURP
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            vowels = "AEIOU"
            year = random.randint(60, 99)
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            state_codes = ['AS', 'BC', 'BS', 'CC', 'CS', 'CH', 'DF', 'CL', 'CM', 'DG', 'GT', 'GR', 'HG', 'JC', 'MC', 'MN', 'MS', 'NT', 'NL', 'PL', 'QT', 'QR', 'SP', 'SL', 'SR', 'TC', 'TS', 'TL', 'VZ', 'YN', 'ZS']
            return f"{random.choice(letters)}{random.choice(vowels)}{random.choice(letters)}{random.choice(letters)}{year:02d}{month:02d}{day:02d}H{random.choice(state_codes)}{random.choice(letters)}{random.choice(letters)}{random.choice(letters)}{random.randint(0,9)}{random.randint(0,9)}"
        else:  # 30% RFC (Registro Federal de Contribuyentes)
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            year = random.randint(60, 99)
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            return f"{random.choice(letters)}{random.choice(letters)}{random.choice(letters)}{random.choice(letters)}{year:02d}{month:02d}{day:02d}{random.choice(letters)}{random.choice(letters)}{random.randint(1,9)}"
    
    elif country == "brazil":
        # Brazilian CPF (Cadastro de Pessoas Físicas) with valid check digits
        base = [random.randint(0, 9) for _ in range(9)]
        # Calculate first check digit
        s = sum(base[i] * (10 - i) for i in range(9))
        d1 = 11 - (s % 11)
        if d1 >= 10:
            d1 = 0
        base.append(d1)
        # Calculate second check digit
        s = sum(base[i] * (11 - i) for i in range(10))
        d2 = 11 - (s % 11)
        if d2 >= 10:
            d2 = 0
        base.append(d2)
        cpf = "".join(map(str, base))
        
        # Realistic formatting variations
        format_choice = random.random()
        if format_choice < 0.5:  # 50% - Standard format with dots
            return f"{cpf[:3]}.{cpf[3:6]}.{cpf[6:9]}-{cpf[9:]}"
        elif format_choice < 0.8:  # 30% - Format with commas
            return f"{cpf[:3]},{cpf[3:6]},{cpf[6:9]}-{cpf[9:]}"
        else:  # 20% - No separators
            return f"{cpf[:9]}-{cpf[9:]}"
    
    elif country == "uruguay":
        # Uruguayan Cédula de Identidad with valid check digit
        base = random.randint(1_000_000, 5_999_999)
        s = str(base)
        coeffs = "2987634"
        checksum = sum(int(s[i]) * int(coeffs[i]) for i in range(7))
        check_digit = (10 - (checksum % 10)) % 10
        
        # Realistic formatting variations
        format_choice = random.random()
        if format_choice < 0.5:  # 50% - Standard format with dots
            return f"{base // 1_000_000}.{base % 1_000_000 // 1000:03d}.{base % 1000:03d}-{check_digit}"
        elif format_choice < 0.8:  # 30% - Format with commas  
            return f"{base // 1_000_000},{base % 1_000_000 // 1000:03d},{base % 1000:03d}-{check_digit}"
        else:  # 20% - No separators
            return f"{base}-{check_digit}"
    
    else:
        # Default to Chilean format
        return generate_id("chile")

def generate_chilean_rut() -> str:
    """
    Generate a realistic Chilean RUT (Rol Único Tributario).
    (Backwards compatibility wrapper)
    """
    return generate_id("chile")

def generate_amount(country: str = "chile") -> str:
    """
    Generate a realistic monetary amount for the specified country.
    
    Amounts are generated within typical ranges for each country's currency:
    - Chile: 10,000 - 2,000,000 CLP
    - Mexico: 500 - 100,000 MXN
    - Brazil: 50 - 5,000 BRL
    - Uruguay: 1,000 - 200,000 UYU
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Formatted amount with currency symbol and code
    """
    if country == "chile":
        amount = random.randint(10_000, 2_000_000)
        amount_str = f"{amount:,}".replace(',', '.')
        
        # Realistic formatting variations for Chilean amounts
        format_choice = random.random()
        if format_choice < 0.3:  # 30% - Full format with symbol and currency
            return f"${amount_str} CLP"
        elif format_choice < 0.5:  # 20% - Only symbol
            return f"${amount_str}"
        elif format_choice < 0.7:  # 20% - Only currency code
            return f"{amount_str} CLP"
        else:  # 30% - Just the number (most challenging for model)
            return amount_str
    
    elif country == "mexico":
        amount = random.randint(500, 100_000)
        amount_str = f"{amount:,}"
        
        # Realistic formatting variations for Mexican amounts
        format_choice = random.random()
        if format_choice < 0.3:  # 30% - Full format with symbol and currency
            return f"${amount_str} MXN"
        elif format_choice < 0.5:  # 20% - Only symbol
            return f"${amount_str}"
        elif format_choice < 0.7:  # 20% - Only currency code
            return f"{amount_str} MXN"
        else:  # 30% - Just the number
            return amount_str
    
    elif country == "brazil":
        amount = random.randint(50, 5_000)
        # Brazilian currency format uses comma as decimal separator and dot for thousands
        amount_str = f"{amount:,.2f}".replace(',', 'X').replace('.', ',').replace('X', '.')
        
        # Realistic formatting variations for Brazilian amounts
        format_choice = random.random()
        if format_choice < 0.3:  # 30% - Full format with symbol and currency
            return f"R$ {amount_str} BRL"
        elif format_choice < 0.5:  # 20% - Only symbol
            return f"R$ {amount_str}"
        elif format_choice < 0.7:  # 20% - Only currency code
            return f"{amount_str} BRL"
        else:  # 30% - Just the number
            return amount_str

    elif country == "uruguay":
        amount = random.randint(1_000, 200_000)
        amount_str = f"{amount:,}".replace(',', '.')
        
        # Realistic formatting variations for Uruguayan amounts
        format_choice = random.random()
        if format_choice < 0.3:  # 30% - Full format with symbol and currency
            return f"$U {amount_str} UYU"
        elif format_choice < 0.5:  # 20% - Only symbol
            return f"$U {amount_str}"
        elif format_choice < 0.7:  # 20% - Only currency code
            return f"{amount_str} UYU"
        else:  # 30% - Just the number
            return amount_str
    
    else:
        # Default to Chilean format
        return generate_amount("chile")

def generate_chilean_amount() -> str:
    """
    Generate a realistic Chilean monetary amount.
    (Backwards compatibility wrapper)
    """
    return generate_amount("chile")

def generate_sequence_number(country: str = "chile") -> str:
    """
    Generate a realistic sequential number for different business contexts per country.
    
    Creates varied sequence identifiers used in real business scenarios:
    - Complaint numbers: 7-digit numbers
    - Reference IDs: alphanumeric codes  
    - Transaction IDs: mixed format
    - Country-specific prefixes and formats
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Sequential identifier
    """
    sequence_types = [
        f"{random.randint(1000000, 9999999)}",        f"{random.randint(10000, 99999)}-{random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ')}",        f"{random.choice('ABCDEFGHIJKLMNOPQRSTUVWXYZ')}{random.randint(100000, 999999)}",        f"{get_next_sequence()}",    ]
    
    # Add country-specific prefixes occasionally
    if random.random() < 0.15:
        prefix_map = {
            "chile": "CL",
            "mexico": "MX",
            "brazil": "BR",
            "uruguay": "UY"
        }
        prefix = prefix_map.get(country, "CL")
        # Prepend prefix to a random sequence type
        idx = random.randrange(len(sequence_types))
        sequence_types[idx] = f"{prefix}-{sequence_types[idx]}"
    
    return random.choice(sequence_types)
    
    # Add country-specific prefixes occasionally
    if random.random() < 0.15:  # 15% chance of country prefix
        country_prefixes = {
            "chile": "CL",
            "mexico": "MX", 
            "brazil": "BR",
            "uruguay": "UY"
        }
        prefix = country_prefixes.get(country, "CL")
        return f"{prefix}-{random.randint(10001, 99999)}"
    
    return random.choice(sequence_types)

def generate_chilean_sequence_number() -> str:
    """
    Generate a realistic sequential number for Chilean business contexts.
    (Backwards compatibility wrapper)
    """
    return generate_sequence_number("chile")

def generate_date(country: str = "chile") -> str:
    """
    Generate a realistic date in various formats commonly used in each country.
    
    Creates dates in formats typical for business documents, forms, and records:
    - Multiple date formats per country
    - Recent dates (last 2 years) for relevance
    - Country-specific separators and ordering
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        
    Returns:
        str: Formatted date string
    """
    # Generate a realistic recent date (within last 2 years)
    start_date = datetime.now() - timedelta(days=730)  # 2 years ago
    end_date = datetime.now()
    
    # Random date between start and end
    time_between = end_date - start_date
    days_between = time_between.days
    random_days = random.randrange(days_between)
    random_date = start_date + timedelta(days=random_days)
    
    if country == "chile":
        # Chilean date formats
        formats = [
            random_date.strftime("%d/%m/%Y"),        # 15/08/2024
            random_date.strftime("%d-%m-%Y"),        # 15-08-2024
            random_date.strftime("%d.%m.%Y"),        # 15.08.2024
            random_date.strftime("%d/%m/%y"),        # 15/08/24
            random_date.strftime("%d de %B de %Y"),  # 15 de agosto de 2024
            random_date.strftime("%d-%b-%Y"),        # 15-ago-2024
            random_date.strftime("%d%m%Y"),          # 15082024 (compact)
            random_date.strftime("%Y%m%d"),          # 20240815 (compact)
            random_date.strftime("%m%d%Y"),          # 08152024 (compact)
        ]
    elif country == "mexico":
        # Mexican date formats
        formats = [
            random_date.strftime("%d/%m/%Y"),        # 15/08/2024
            random_date.strftime("%d-%m-%Y"),        # 15-08-2024
            random_date.strftime("%d.%m.%Y"),        # 15.08.2024
            random_date.strftime("%d/%m/%y"),        # 15/08/24
            random_date.strftime("%d de %B de %Y"),  # 15 de agosto de 2024
            random_date.strftime("%m/%d/%Y"),        # 08/15/2024 (some US influence)
            random_date.strftime("%d%m%Y"),          # 15082024 (compact)
            random_date.strftime("%Y%m%d"),          # 20240815 (compact)
            random_date.strftime("%m%d%Y"),          # 08152024 (compact)
        ]
    elif country == "brazil":
        # Brazilian date formats
        formats = [
            random_date.strftime("%d/%m/%Y"),        # 15/08/2024
            random_date.strftime("%d-%m-%Y"),        # 15-08-2024
            random_date.strftime("%d.%m.%Y"),        # 15.08.2024
            random_date.strftime("%d/%m/%y"),        # 15/08/24
            random_date.strftime("%d de %B de %Y"),  # 15 de agosto de 2024
            random_date.strftime("%d/%b/%Y"),        # 15/ago/2024
            random_date.strftime("%d%m%Y"),          # 15082024 (compact)
            random_date.strftime("%Y%m%d"),          # 20240815 (compact)
            random_date.strftime("%m%d%Y"),          # 08152024 (compact)
        ]
    elif country == "uruguay":
        # Uruguayan date formats
        formats = [
            random_date.strftime("%d/%m/%Y"),        # 15/08/2024
            random_date.strftime("%d-%m-%Y"),        # 15-08-2024
            random_date.strftime("%d.%m.%Y"),        # 15.08.2024
            random_date.strftime("%d/%m/%y"),        # 15/08/24
            random_date.strftime("%d de %B de %Y"),  # 15 de agosto de 2024
            random_date.strftime("%d-%b-%Y"),        # 15-ago-2024
            random_date.strftime("%d%m%Y"),          # 15082024 (compact)
            random_date.strftime("%Y%m%d"),          # 20240815 (compact)
            random_date.strftime("%m%d%Y"),          # 08152024 (compact)
        ]
    else:
        # Default to Chilean formats
        formats = [
            random_date.strftime("%d/%m/%Y"),
            random_date.strftime("%d-%m-%Y"),
            random_date.strftime("%d.%m.%Y"),
            random_date.strftime("%d%m%Y"),          # 15082024 (compact)
            random_date.strftime("%Y%m%d"),          # 20240815 (compact)
            random_date.strftime("%m%d%Y"),          # 08152024 (compact)
        ]
    
    return random.choice(formats)

# -----------------
# Advanced Noise Generation Functions
# -----------------

def add_realistic_noise(text: str, noise_probability: float = 0.15) -> str:
    """
    Add realistic noise to text while preserving entity boundaries.
    
    MODIFICATION 4: Entity-boundary-aware noise that drastically reduces failed spans.
    
    Applies controlled noise patterns while protecting critical entity patterns:
    - Soft noise only: spacing, abbreviations, minimal OCR errors
    - Avoids aggressive character substitutions that break entities
    - Protects sequential patterns (numbers/letters) from corruption
    - Maintains entity recognition integrity
    
    Args:
        text (str): Original text
        noise_probability (float): Probability of applying noise (0.0-1.0)
    
    Returns:
        str: Text with controlled noise applied
    """
    if random.random() > noise_probability:
        return text  # No noise applied
    
    # MODIFICATION 4: Safe noise types that preserve entity boundaries
    safe_noise_types = [
        _add_spacing_noise,      # Safe: only affects spaces
        _add_abbreviation_noise, # Safe: preserves meaning
        _add_punctuation_noise,  # Safe: minimal changes
        _add_soft_ocr_noise,     # NEW: Very limited OCR errors
    ]
    
    # Apply only ONE type of soft noise (reduced from multiple)
    noise_function = random.choice(safe_noise_types)
    text = noise_function(text)
    
    return text

def _add_soft_ocr_noise(text: str) -> str:
    """Add very gentle OCR-like noise that preserves entity boundaries."""
    # Only very safe OCR substitutions with low probability
    safe_ocr_substitutions = {
        'ñ': 'n',   # Missing tilde (very safe)
        'ü': 'u',   # Missing umlaut (very safe)
    }
    
    if random.random() < 0.1:  # Only 10% chance of any OCR noise
        for original, replacement in safe_ocr_substitutions.items():
            if original in text and random.random() < 0.3:  # 30% chance per substitution
                text = text.replace(original, replacement, 1)
                break  # Only one substitution
    
    return text

def _add_ocr_scanning_noise(text: str) -> str:
    """Add very minimal OCR scanning artifacts (DISABLED to preserve entities)."""
    # DISABLED: This function was causing entity boundary corruption
    # Only apply very safe accent removal with very low probability
    if random.random() < 0.05:  # Only 5% chance
        safe_replacements = [
            ('á', 'a'), ('é', 'e'), ('í', 'i'), ('ó', 'o'), ('ú', 'u')
        ]
        for original, replacement in safe_replacements:
            if original in text and random.random() < 0.1:  # 10% chance per accent
                text = text.replace(original, replacement, 1)
                break
    
    return text

def _add_ocr_line_break_noise(text: str) -> str:
    """Add very minimal OCR line break errors (MOSTLY DISABLED)."""
    # MOSTLY DISABLED: This was causing entity corruption
    # Only add very safe extra spaces occasionally
    if random.random() < 0.1:  # Only 10% chance
        if random.random() < 0.5:
            text = text.replace(' ', '  ', 1)  # Add one extra space
    
    return text

def _add_spacing_noise(text: str) -> str:
    """Add realistic spacing variations."""
    noise_patterns = [
        lambda t: t.replace(" ", "  "),  # Double spaces occasionally
        lambda t: t.replace(". ", ".  "), # Extra space after period
        lambda t: t.replace(", ", ",  "), # Extra space after comma
        lambda t: t.replace(" .", " ."), # Space before period (rare)
    ]
    pattern = random.choice(noise_patterns)
    return pattern(text)

def _add_abbreviation_noise(text: str) -> str:
    """Add realistic abbreviations that preserve meaning."""
    abbreviations = {
        "Avenida": "Av.",
        "Calle": "C.",
        "Pasaje": "Pje.",
        "Número": "N°",
        "Teléfono": "Tel.",
        "Email": "E-mail",
        "Correo": "Email",
        "Dirección": "Dir.",
        "Registro": "Reg.",
        "Cliente": "Cte.",
        "Usuario": "User",
        "Documento": "Doc.",
        "Identificación": "ID",
    }
    
    for full_word, abbrev in abbreviations.items():
        if full_word in text and random.random() < 0.3:
            text = text.replace(full_word, abbrev)
            break  # Only one abbreviation per text
    
    return text

def _add_punctuation_noise(text: str) -> str:
    """Add realistic punctuation variations."""
    noise_patterns = [
        lambda t: t.replace(".", " ."),  # Space before period
        lambda t: t.replace(":", " :"),  # Space before colon
        lambda t: t.replace(",", " ,"),  # Space before comma (rare)
        lambda t: t.replace(".", ".."),  # Double period occasionally
    ]
    
    if random.random() < 0.2:  # Low probability for punctuation noise
        pattern = random.choice(noise_patterns)
        return pattern(text)
    
    return text

def _add_case_noise(text: str) -> str:
    """Add minimal case variations (REDUCED to preserve entities)."""
    if random.random() < 0.05:  # Reduced from 0.1 to 0.05 (5% chance)
        # Only make very safe words lowercase
        safe_words = ["el", "la", "de", "con", "en", "y", "o"]
        words = text.split()
        for i, word in enumerate(words):
            if word.lower() in safe_words and random.random() < 0.2:  # Reduced from 0.3 to 0.2
                words[i] = word.lower()
        return " ".join(words)
    
    return text

def generate_noisy_sentence_structure() -> str:
    """
    Generate more complex sentence structures with realistic variations.
    
    Creates varied sentence patterns that are more complex than basic templates,
    adding realistic document noise while maintaining entity clarity.
    
    Returns:
        str: Template string with placeholders for Chilean data
    """
    # Complex Chilean sentence structures with noise elements
    noisy_templates = [
        # Standard business communication with variations
        "El cliente {} {} con RUT {} registrado en el sistema. Dirección actual: {}, {}. Teléfono de contacto: {} - Email: {}. Monto pendiente: {}. N° de operación: {}.",
        
        # Informal customer service style
        "Datos del usuario {} {}: documento {} / dirección {} en {} / tel. {} / correo {} / saldo {} / ref. {}.",
        
        # Document-style format with abbreviations
        "Reg. cliente: {} {} (ID: {}) - Dir: {}, {} - Tel: {} - Email: {} - Transacción: {} - Código: {}.",
        
        # Billing/invoice style
        "FACTURA - Cliente: {} {} / RUT: {} / Dirección de facturación: {}, {} / Contacto: {} / {} / Total: {} / N° Factura: {}.",
        
        # Call center script style
        "Buenos días Sr./Sra. {} {}, confirmo sus datos: RUT {}, domicilio en {}, ciudad {}, teléfono {}, email {}, último pago por {}, consulta N° {}.",
        
        # Banking/financial format
        "Estimado/a {} {}: Su cuenta asociada al RUT {} tiene dirección registrada en {}, {}. Para consultas llamar al {} o escribir a {}. Saldo disponible: {}. Código de operación: {}.",
        
        # Government/official style
        "Ciudadano/a {} {} identificado/a con cédula {} domiciliado/a en {}, comuna de {}. Tel. contacto: {}. Correo electrónico: {}. Monto a pagar: ${}. Trámite N°: {}.",
        
        # Insurance/healthcare style
        "Paciente: {} {} - RUT: {} - Domicilio: {}, {} - Fono: {} - Email: {} - Copago: {} - N° Atención: {}.",
        
        # E-commerce/retail style
        "Pedido a nombre de {} {} (RUT {}). Envío a: {}, {}. Teléfono: {}. Email: {}. Total del pedido: {}. N° de seguimiento: {}.",
        
        # Legal/notarial style
        "Comparece don/doña {} {}, RUT {}, domiciliado/a en {}, {}. Teléfono: {}. Correo: {}. Honorarios: {}. Causa N°: {}.",
        
        # NEW: Heavily abbreviated templates (OCR corruption simulation)
        "Clte: {} {} - Doc: {} - Dir: {}/{} - Tel: {} - @ {} - $$ {} - #: {}.",
        "usr {} {} doc {} ubic {}, {} fno {} mail {} val {} cod {}",
        "CLIENTE: {}  {} DOC.{} DIREC: {} {} TEL:{} CORREO:{} TOTAL:{} NUMERO:{}",
        
        # NEW: Industry-specific templates  
        "SUSCRIPTOR {} {} - SERV. {} - INSTALACIÓN: {}, {} - CONTACTO: {} {} - FACT.: {} - ORD: {}",
        "HUÉSPED: {} {} - DOC: {} - SUITE: {} EN {} - TEL: {} - EMAIL: {} - TOTAL: {} - RESERVA: {}",
        "BENEFICIARIO {} {} CI {} DIRECCIÓN {} {} TELÉFONO {} CORREO {} SUBSIDIO {} EXPEDIENTE {}",
        
        # NEW: SMS/Message style templates
        "msg: {} {} id{} vive {} {} tel{} mail{} debe{} ref{}",
        "AVISO: Sr/a {} {} RUT{} dom. {},{} cont.{}/{} pago${} tramite{}",
        
        # NEW: Error-prone templates (missing punctuation, irregular spacing)
        "Cliente {} {}  con documento {}  dirección {}, {} teléfono {} email {} monto {} referencia {}",
        "DATOS {} {} - {} / {} {} / {} / {} / {} / {}",
    ]
    
    return random.choice(noisy_templates)

# -----------------
# Advanced Entity Conflict Resolution (E1010 Fix)
# -----------------

# -----------------
# Multi-Country Text Templates and Noise Functions
# -----------------

def get_sentence_templates(country: str) -> List[str]:
    """
    Get sentence templates appropriate for the specified country.
    
    Args:
        country (str): Country code
        
    Returns:
        List[str]: List of sentence templates with placeholders
    """
    if country == "chile":
        return [
            # Standard business formats (8 templates)
            "El cliente {} con RUT {} reside en {}, {}. Teléfono: {} / Email: {}. Monto: {} - Referencia: {}.",
            "Registro de {} con cédula {}. Ubicado en {}, {}. Contacto: {} / {}. Transacción: {} - ID: {}.",
            "Cliente {} identificado con RUT {}. Dirección: {}, {}. Contacto telefónico: {}. Correo: {}. Valor: {} - Número: {}.",
            "Datos del cliente: {} (RUT: {}). Domicilio: {}, {}. Tel: {} / {}. Monto operación: {} - Folio: {}.",
            "Información de {} con RUT {}. Vive en {}, {}. Fono: {} - Email: {}. Cantidad: {} - Serie: {}.",
            "CLIENTE: {} RUT: {} DIRECCIÓN: {} {} TEL: {} EMAIL: {} MONTO: {} N°: {}",
            "{}|{}|{} {}|{}|{}|{}|{}",
            "Nombre:{} ID:{} Dom:{},{} Cont:{}/{} Val:{} Ref:{}",
            
            # Financial/Banking formats (12 templates)
            "Transferencia bancaria: {} (RUT {}) desde {}, {} al {} via {}. Monto: {} Operación: {}.",
            "BBVA CHILE - Cliente: {} - Doc: {} - Sucursal: {}, {} - Contact: {}/{} - Transfer: {} - Code: {}.",
            "BancoEstado: Titular {} RUT {} domiciliado {}, {} fono {} email {} por ${} boleta {}.",
            "Santander Chile notifica a {} (CI: {}) dirección {}, {} contactos {}/{} débito {} autorización {}.",
            "Itaú informa: Cliente {} documento {} ubicación {} ciudad {} teléfonos {} correos {} cargo {} número {}.",
            "SCOTIABANK: {} con identificación {} residencia {}, {} comunicación {}/{} importe {} serie {}.",
            "Falabella Financial: Socio {} RUT {} dirección {}, {} datos {}/{} compra {} cupón {}.",
            "Cuenta Vista: {} (RUT {}) dirección {}, {} contacto {}/{} saldo {} movimiento {}.",
            "Crédito Hipotecario: Deudor {} cédula {} propiedad {}, {} teléfono {} mail {} cuota {} período {}.",
            "Línea de Crédito: {} con RUT {} ubicado {}, {} contactos {}/{} disponible {} operación {}.",
            "Depósito a Plazo: Inversionista {} documento {} domicilio {}, {} datos {}/{} capital {} certificado {}.",
            "Cuenta Corriente: {} identificado {} residencia {}, {} comunicación {}/{} giro {} cheque {}.",
            
            # Government/Official formats (10 templates)
            "Registro Civil: Ciudadano {} RUT {} domiciliado {}, {} teléfono {} correo {} trámite {} expediente {}.",
            "SII notifica: Contribuyente {} con RUT {} dirección {}, {} contacto {}/{} deuda {} timbre {}.",
            "FONASA: Beneficiario {} cédula {} ubicación {}, {} datos {}/{} copago {} atención {}.",
            "Municipalidad Santiago: Vecino {} documento {} residencia {}, {} comunicación {}/{} multa {} boleta {}.",
            "SEREMI Salud: Persona {} identificación {} domicilio {}, {} teléfono {} email {} sanción {} resolución {}.",
            "Carabineros: Infractor {} RUT {} dirección {}, {} contactos {}/{} multa {} parte {}.",
            "JUNAEB: Estudiante {} cédula {} establecimiento {}, {} apoderado {}/{} beca {} código {}.",
            "Servicio Electoral: Ciudadano {} documento {} inscripción {}, {} datos {}/{} mesa {} local {}.",
            "Registro Social: Familia {} RUT {} vivienda {}, {} contacto {}/{} puntaje {} ficha {}.",
            "SENCE: Capacitando {} identificación {} curso {}, {} comunicación {}/{} subsidio {} código {}.",
            
            # Healthcare formats (8 templates)
            "Clínica Las Condes: Paciente {} RUT {} residencia {}, {} emergencia {}/{} consulta {} hora {}.",
            "Hospital Salvador: {} con cédula {} domicilio {}, {} contactos {}/{} atención {} ficha {}.",
            "ISAPRE Banmédica: Afiliado {} documento {} dirección {}, {} teléfonos {}/{} bonificación {} prestación {}.",
            "Consulta Médica: Sr/a {} identificado {} ubicación {}, {} comunicación {}/{} honorarios {} boleta {}.",
            "Laboratorio: Paciente {} RUT {} dirección {}, {} datos {}/{} examen {} código {}.",
            "Dental: {} con cédula {} residencia {}, {} contacto {}/{} tratamiento {} presupuesto {}.",
            "Farmacia: Cliente {} documento {} ubicación {}, {} teléfono {} email {} medicamento {} receta {}.",
            "Kinesiología: Paciente {} identificación {} domicilio {}, {} comunicación {}/{} sesión {} orden {}.",
            
            # Commercial/Retail formats (12 templates)
            "Falabella: Cliente {} RUT {} envío {}, {} contacto {}/{} compra {} boleta {}.",
            "Ripley: Comprador {} cédula {} despacho {}, {} datos {}/{} total {} transacción {}.",
            "Paris: {} con documento {} dirección {}, {} teléfonos {}/{} adquisición {} cupón {}.",
            "La Polar: Socio {} identificación {} residencia {}, {} comunicación {}/{} financiamiento {} contrato {}.",
            "Líder: Cliente {} RUT {} ubicación {}, {} contactos {}/{} ticket {} caja {}.",
            "Jumbo: Comprador {} cédula {} dirección {}, {} datos {}/{} vale {} número {}.",
            "Easy: {} con documento {} domicilio {}, {} teléfono {} correo {} materiales {} factura {}.",
            "Sodimac: Cliente {} identificación {} obra {}, {} comunicación {}/{} pedido {} orden {}.",
            "Farmacia Ahumada: {} con RUT {} dirección {}, {} contacto {}/{} medicinas {} receta {}.",
            "Copec: Usuario {} cédula {} ubicación {}, {} datos {}/{} combustible {} vale {}.",
            "Shell: Cliente {} documento {} estación {}, {} teléfonos {}/{} carga {} ticket {}.",
            "Petrobras: {} identificado {} dirección {}, {} comunicación {}/{} consumo {} comprobante {}.",
            
            # Insurance formats (8 templates)
            "Mapfre Seguros: Asegurado {} RUT {} bien {}, {} contacto {}/{} prima {} póliza {}.",
            "Liberty Seguros: {} con cédula {} propiedad {}, {} datos {}/{} cobertura {} certificado {}.",
            "HDI Seguros: Titular {} documento {} ubicación {}, {} teléfonos {}/{} siniestro {} reclamo {}.",
            "Zurich: Contratante {} identificación {} dirección {}, {} comunicación {}/{} renovación {} endoso {}.",
            "La Segunda Seguros: {} con RUT {} residencia {}, {} contactos {}/{} indemnización {} liquidación {}.",
            "RSA Seguros: Asegurado {} cédula {} bien {}, {} datos {}/{} deducible {} caso {}.",
            "Consorcio Seguros: {} documento {} propiedad {}, {} teléfono {} email {} cotización {} propuesta {}.",
            "BCI Seguros: Cliente {} identificación {} ubicación {}, {} comunicación {}/{} beneficio {} número {}.",
            
            # Telecommunications (6 templates)
            "Entel: Cliente {} RUT {} instalación {}, {} contacto {}/{} plan {} línea {}.",
            "Movistar: Usuario {} cédula {} dirección {}, {} datos {}/{} servicio {} cuenta {}.",
            "Claro: Suscriptor {} documento {} ubicación {}, {} teléfonos {}/{} factura {} período {}.",
            "WOM: {} identificado {} residencia {}, {} comunicación {}/{} cargo {} mes {}.",
            "VTR: Cliente {} con RUT {} dirección {}, {} contactos {}/{} internet {} contrato {}.",
            "GTD: Usuario {} cédula {} instalación {}, {} datos {}/{} telefonía {} orden {}.",
            
            # Utilities (6 templates)
            "Chilectra: Cliente {} RUT {} suministro {}, {} contacto {}/{} consumo {} medidor {}.",
            "CGE: Usuario {} documento {} dirección {}, {} datos {}/{} factura {} período {}.",
            "Metrogas: Suscriptor {} identificación {} ubicación {}, {} teléfonos {}/{} gas {} instalación {}.",
            "Aguas Andinas: {} con cédula {} predio {}, {} comunicación {}/{} agua {} cuenta {}.",
            "ESSAL: Cliente {} RUT {} dirección {}, {} contactos {}/{} servicio {} medición {}.",
            "ESSBIO: Usuario {} documento {} ubicación {}, {} datos {}/{} alcantarillado {} código {}.",
            
            # Abbreviated/Corrupted OCR formats (10 templates)
            "CLT:{} DOC:{} DIR:{} {} TEL:{} @ {} $:{} #:{}",
            "{}  {}  {}  {}    {}  {}   {}   {}",
            "{} ({}) {} {} {} {} {} {}",
            "Sr(a). {} C.I. {} domiciliado en {} {} teléfono {} e-mail {} por {} folio {}",
            "USR {} ID {} LOC {},{} CONT {}/{} VAL {} REF {}",
            "REG: {} | {} | {} {} | {} | {} | {} | {}",
            "{} - {} - {} - {} - {} - {} - {} - {}",
            "NAM:{} DOC:{} ADD:{} {} PHN:{} EML:{} AMT:{} NUM:{}",
            "[{}] [{}] [{} {}] [{}] [{}] [{}] [{}]",
            "CLI {} RUT {} DOM {},{} FON {} COR {} MON {} COD {}"
        ]
    elif country == "mexico":
        return [
            # Standard business formats (8 templates)
            "El cliente {} con CURP {} reside en {}, {}. Teléfono: {} / Email: {}. Monto: {} - Referencia: {}.",
            "Registro de {} con RFC {}. Ubicado en {}, {}. Contacto: {} / {}. Transacción: {} - ID: {}.",
            "Cliente {} identificado con CURP {}. Dirección: {}, {}. Contacto telefónico: {}. Correo: {}. Valor: {} - Número: {}.",
            "Datos del cliente: {} (CURP: {}). Domicilio: {}, {}. Tel: {} / {}. Monto operación: {} - Folio: {}.",
            "Información de {} con RFC {}. Vive en {}, {}. Fono: {} - Email: {}. Cantidad: {} - Serie: {}.",
            "CLIENTE: {} CURP: {} DIRECCIÓN: {} {} TEL: {} EMAIL: {} MONTO: {} N°: {}",
            "{}|{}|{} {}|{}|{}|{}|{}",
            "Nombre:{} RFC:{} Dom:{},{} Cont:{}/{} Val:{} Ref:{}",
            
            # Banking formats (12 templates)
            "BBVA México: Cliente {} CURP {} sucursal {}, {} contacto {}/{} transferencia {} código {}.",
            "Banorte informa: {} con RFC {} domicilio {}, {} datos {}/{} operación {} número {}.",
            "Santander México: Titular {} documento {} ubicación {} ciudad {} teléfonos {} correos {} cargo {} folio {}.",
            "HSBC México: Cliente {} identificación {} residencia {}, {} comunicación {}/{} importe {} serie {}.",
            "Citibanamex: {} con CURP {} dirección {}, {} contactos {}/{} débito {} autorización {}.",
            "Banco Azteca: Usuario {} RFC {} ubicación {}, {} datos {}/{} crédito {} expediente {}.",
            "Scotiabank México: Cuenta {} documento {} domicilio {}, {} teléfono {} email {} saldo {} movimiento {}.",
            "Inbursa: Cliente {} identificación {} residencia {}, {} comunicación {}/{} disponible {} operación {}.",
            "BanCoppel: {} con CURP {} dirección {}, {} contactos {}/{} financiamiento {} contrato {}.",
            "Banco del Bajío: Titular {} RFC {} ubicación {}, {} datos {}/{} inversión {} certificado {}.",
            "Cuenta de Cheques: {} documento {} domicilio {}, {} teléfonos {}/{} giro {} cheque {}.",
            "Tarjeta de Crédito: {} identificación {} residencia {}, {} comunicación {}/{} límite {} corte {}.",
            
            # Government/Official formats (10 templates)
            "SAT notifica: Contribuyente {} con RFC {} dirección {}, {} contacto {}/{} adeudo {} requerimiento {}.",
            "IMSS: Derechohabiente {} CURP {} ubicación {}, {} datos {}/{} servicio {} número {}.",
            "ISSSTE: Trabajador {} documento {} domicilio {}, {} teléfono {} email {} prestación {} folio {}.",
            "INFONAVIT: Acreditado {} identificación {} vivienda {}, {} comunicación {}/{} crédito {} contrato {}.",
            "INE: Ciudadano {} con CURP {} residencia {}, {} contactos {}/{} credencial {} vigencia {}.",
            "SEDENA: Personal {} RFC {} ubicación {}, {} datos {}/{} trámite {} expediente {}.",
            "SEMAR: Elemento {} documento {} domicilio {}, {} teléfonos {}/{} gestión {} número {}.",
            "Secretaría de Salud: Paciente {} identificación {} residencia {}, {} comunicación {}/{} atención {} folio {}.",
            "SEP: Estudiante {} con CURP {} institución {}, {} contactos {}/{} beca {} código {}.",
            "CONACYT: Beneficiario {} RFC {} proyecto {}, {} datos {}/{} apoyo {} convocatoria {}.",
            
            # Healthcare formats (8 templates)
            "Hospital General: Paciente {} CURP {} residencia {}, {} emergencia {}/{} consulta {} expediente {}.",
            "IMSS Clínica: {} con RFC {} domicilio {}, {} contactos {}/{} atención {} número {}.",
            "ISSSTE Hospital: Derechohabiente {} documento {} ubicación {}, {} teléfonos {}/{} servicio {} folio {}.",
            "Clínica Privada: Sr/a {} identificación {} dirección {}, {} comunicación {}/{} honorarios {} recibo {}.",
            "Laboratorios: Paciente {} con CURP {} residencia {}, {} datos {}/{} estudio {} código {}.",
            "Farmacia: Cliente {} RFC {} ubicación {}, {} contacto {}/{} medicamento {} receta {}.",
            "Consultorio Dental: {} documento {} domicilio {}, {} teléfono {} email {} tratamiento {} presupuesto {}.",
            "Centro de Rehabilitación: Paciente {} identificación {} dirección {}, {} comunicación {}/{} terapia {} orden {}.",
            
            # Commercial/Retail formats (12 templates)
            "Liverpool: Cliente {} CURP {} envío {}, {} contacto {}/{} compra {} ticket {}.",
            "Palacio de Hierro: {} con RFC {} despacho {}, {} datos {}/{} total {} factura {}.",
            "Sears México: Comprador {} documento {} dirección {}, {} teléfonos {}/{} adquisición {} número {}.",
            "Coppel: Cliente {} identificación {} residencia {}, {} comunicación {}/{} crédito {} contrato {}.",
            "Elektra: {} con CURP {} ubicación {}, {} contactos {}/{} financiamiento {} folio {}.",
            "Soriana: Comprador {} RFC {} dirección {}, {} datos {}/{} vale {} código {}.",
            "Walmart México: Cliente {} documento {} ubicación {}, {} teléfono {} correo {} pedido {} orden {}.",
            "OXXO: Usuario {} identificación {} dirección {}, {} comunicación {}/{} recarga {} comprobante {}.",
            "7-Eleven: {} con CURP {} ubicación {}, {} contactos {}/{} producto {} ticket {}.",
            "Pemex: Cliente {} RFC {} estación {}, {} datos {}/{} combustible {} vale {}.",
            "CFE: Usuario {} documento {} suministro {}, {} teléfonos {}/{} factura {} período {}.",
            "Telmex: Suscriptor {} identificación {} instalación {}, {} comunicación {}/{} servicio {} contrato {}.",
            
            # Insurance formats (8 templates)
            "GNP Seguros: Asegurado {} CURP {} bien {}, {} contacto {}/{} prima {} póliza {}.",
            "Quálitas: {} con RFC {} vehículo {}, {} datos {}/{} cobertura {} certificado {}.",
            "MAPFRE México: Titular {} documento {} propiedad {}, {} teléfonos {}/{} siniestro {} reclamo {}.",
            "AXA Seguros: Contratante {} identificación {} dirección {}, {} comunicación {}/{} renovación {} endoso {}.",
            "Zurich México: {} con CURP {} residencia {}, {} contactos {}/{} indemnización {} liquidación {}.",
            "HDI Seguros: Asegurado {} RFC {} bien {}, {} datos {}/{} deducible {} expediente {}.",
            "Seguros Monterrey: {} documento {} propiedad {}, {} teléfono {} email {} cotización {} propuesta {}.",
            "BBVA Seguros: Cliente {} identificación {} ubicación {}, {} comunicación {}/{} beneficio {} número {}.",
            
            # Telecommunications (6 templates)
            "Telcel: Cliente {} CURP {} servicio {}, {} contacto {}/{} plan {} línea {}.",
            "Movistar México: Usuario {} RFC {} dirección {}, {} datos {}/{} factura {} cuenta {}.",
            "AT&T México: Suscriptor {} documento {} ubicación {}, {} teléfonos {}/{} cargo {} período {}.",
            "Izzi: {} identificado {} instalación {}, {} comunicación {}/{} internet {} contrato {}.",
            "Megacable: Cliente {} con CURP {} dirección {}, {} contactos {}/{} televisión {} orden {}.",
            "Totalplay: Usuario {} RFC {} ubicación {}, {} datos {}/{} paquete {} código {}.",
            
            # Abbreviated/Corrupted OCR formats (10 templates)
            "CLT:{} CURP:{} DIR:{} {} TEL:{} @ {} $:{} #:{}",
            "{}  {}  {}  {}    {}  {}   {}   {}",
            "{} ({}) {} {} {} {} {} {}",
            "Sr(a). {} RFC {} domiciliado en {} {} teléfono {} e-mail {} por {} folio {}",
            "USR {} CURP {} LOC {},{} CONT {}/{} VAL {} REF {}",
            "REG: {} | {} | {} {} | {} | {} | {} | {}",
            "{} - {} - {} - {} - {} - {} - {} - {}",
            "NAM:{} RFC:{} ADD:{} {} PHN:{} EML:{} AMT:{} NUM:{}",
            "[{}] [{}] [{} {}] [{}] [{}] [{}] [{}]",
            "CLI {} CURP {} DOM {},{} FON {} COR {} MON {} COD {}"
        ]
    elif country == "brazil":
        return [
            # Standard business formats (8 templates)
            "O cliente {} com CPF {} reside em {}, {}. Telefone: {} / Email: {}. Valor: {} - Referência: {}.",
            "Registro de {} com CPF {}. Localizado em {}, {}. Contato: {} / {}. Transação: {} - ID: {}.",
            "Cliente {} identificado com CPF {}. Endereço: {}, {}. Telefone: {}. Email: {}. Valor: {} - Número: {}.",
            "Dados do cliente: {} (CPF: {}). Domicílio: {}, {}. Tel: {} / {}. Valor da operação: {} - Protocolo: {}.",
            "Informação de {} com CPF {}. Mora em {}, {}. Fone: {} - Email: {}. Quantia: {} - Série: {}.",
            "CLIENTE: {} CPF: {} ENDEREÇO: {} {} TEL: {} EMAIL: {} VALOR: {} N°: {}",
            "{}|{}|{} {}|{}|{}|{}|{}",
            "Nome:{} CPF:{} End:{},{} Cont:{}/{} Val:{} Ref:{}",
            
            # Banking formats (12 templates)
            "Banco do Brasil: Cliente {} CPF {} agência {}, {} contato {}/{} transferência {} código {}.",
            "Itaú Unibanco: {} com CPF {} endereço {}, {} dados {}/{} operação {} número {}.",
            "Bradesco informa: Titular {} documento {} localização {} cidade {} telefones {} emails {} débito {} protocolo {}.",
            "Santander Brasil: Cliente {} identificação {} residência {}, {} comunicação {}/{} valor {} série {}.",
            "Caixa Econômica: {} com CPF {} endereço {}, {} contatos {}/{} crédito {} autorização {}.",
            "Banco Inter: Usuário {} documento {} localização {}, {} dados {}/{} PIX {} transação {}.",
            "Nubank: Conta {} identificação {} residência {}, {} telefone {} email {} cartão {} fatura {}.",
            "BTG Pactual: Cliente {} com CPF {} endereço {}, {} comunicação {}/{} investimento {} operação {}.",
            "Banco Safra: {} documento {} domicílio {}, {} contatos {}/{} aplicação {} certificado {}.",
            "Banrisul: Titular {} identificação {} localização {}, {} dados {}/{} conta {} movimentação {}.",
            "Conta Corrente: {} com CPF {} endereço {}, {} telefones {}/{} saque {} comprovante {}.",
            "Poupança: Cliente {} documento {} residência {}, {} comunicação {}/{} rendimento {} extrato {}.",
            
            # Government/Official formats (10 templates)
            "Receita Federal: Contribuinte {} com CPF {} endereço {}, {} contato {}/{} imposto {} intimação {}.",
            "INSS: Segurado {} CPF {} localização {}, {} dados {}/{} benefício {} número {}.",
            "SUS: Paciente {} documento {} residência {}, {} telefone {} email {} atendimento {} protocolo {}.",
            "Ministério da Saúde: Cidadão {} identificação {} endereço {}, {} comunicação {}/{} vacinação {} cartão {}.",
            "IBGE: Entrevistado {} com CPF {} domicílio {}, {} contatos {}/{} censo {} questionário {}.",
            "Polícia Federal: Requerente {} documento {} localização {}, {} dados {}/{} passaporte {} processo {}.",
            "Correios: Destinatário {} identificação {} endereço {}, {} telefones {}/{} encomenda {} código {}.",
            "BNDES: Empresa {} com CPF {} projeto {}, {} comunicação {}/{} financiamento {} contrato {}.",
            "Tribunal Eleitoral: Eleitor {} documento {} zona {}, {} contatos {}/{} título {} seção {}.",
            "Detran: Condutor {} identificação {} residência {}, {} dados {}/{} multa {} auto {}.",
            
            # Healthcare formats (8 templates)
            "Hospital Público: Paciente {} CPF {} residência {}, {} emergência {}/{} consulta {} prontuário {}.",
            "SUS Unidade: {} com documento {} endereço {}, {} contatos {}/{} atendimento {} número {}.",
            "Clínica Privada: Sr/a {} identificação {} localização {}, {} telefones {}/{} particular {} recibo {}.",
            "Laboratório: Paciente {} com CPF {} residência {}, {} comunicação {}/{} exame {} laudo {}.",
            "Farmácia: Cliente {} documento {} endereço {}, {} contato {}/{} medicamento {} receita {}.",
            "Consultório: {} identificação {} localização {}, {} telefone {} email {} consulta {} agendamento {}.",
            "Fisioterapia: Paciente {} com CPF {} residência {}, {} dados {}/{} sessão {} prescrição {}.",
            "Psicologia: {} documento {} endereço {}, {} comunicação {}/{} terapia {} encaminhamento {}.",
            
            # Commercial/Retail formats (12 templates)
            "Magazine Luiza: Cliente {} CPF {} entrega {}, {} contato {}/{} compra {} nota {}.",
            "Via Varejo: {} com documento {} endereço {}, {} dados {}/{} total {} cupom {}.",
            "Lojas Americanas: Comprador {} identificação {} localização {}, {} telefones {}/{} aquisição {} código {}.",
            "Pão de Açúcar: Cliente {} com CPF {} residência {}, {} comunicação {}/{} vale {} número {}.",
            "Carrefour Brasil: {} documento {} endereço {}, {} contatos {}/{} cartão {} fidelidade {}.",
            "Extra: Comprador {} identificação {} localização {}, {} dados {}/{} desconto {} promoção {}.",
            "Casas Bahia: Cliente {} com CPF {} entrega {}, {} telefone {} email {} crediário {} contrato {}.",
            "Renner: {} documento {} endereço {}, {} comunicação {}/{} produto {} etiqueta {}.",
            "C&A Brasil: Comprador {} identificação {} localização {}, {} contatos {}/{} vale {} código {}.",
            "Posto de Gasolina: Cliente {} com CPF {} localização {}, {} dados {}/{} combustível {} cupom {}.",
            "Padaria: {} documento {} endereço {}, {} telefones {}/{} fiado {} caderno {}.",
            "Mercado: Cliente {} identificação {} residência {}, {} comunicação {}/{} compra {} ticket {}.",
            
            # Insurance formats (8 templates)
            "Porto Seguro: Segurado {} CPF {} bem {}, {} contato {}/{} prêmio {} apólice {}.",
            "Bradesco Seguros: {} com documento {} propriedade {}, {} dados {}/{} cobertura {} certificado {}.",
            "SulAmérica: Titular {} identificação {} endereço {}, {} telefones {}/{} sinistro {} processo {}.",
            "Mapfre Brasil: Contratante {} com CPF {} localização {}, {} comunicação {}/{} renovação {} endosso {}.",
            "Allianz: {} documento {} residência {}, {} contatos {}/{} indenização {} liquidação {}.",
            "Tokio Marine: Segurado {} identificação {} bem {}, {} dados {}/{} franquia {} evento {}.",
            "Liberty Seguros: {} com CPF {} propriedade {}, {} telefone {} email {} cotação {} proposta {}.",
            "HDI Brasil: Cliente {} documento {} endereço {}, {} comunicação {}/{} benefício {} número {}.",
            
            # Telecommunications (6 templates)
            "Vivo: Cliente {} CPF {} instalação {}, {} contato {}/{} plano {} linha {}.",
            "Claro Brasil: Usuário {} documento {} endereço {}, {} dados {}/{} fatura {} conta {}.",
            "TIM Brasil: Assinante {} identificação {} localização {}, {} telefones {}/{} cobrança {} período {}.",
            "Oi: {} com CPF {} residência {}, {} comunicação {}/{} serviço {} contrato {}.",
            "Nextel Brasil: Cliente {} documento {} endereço {}, {} contatos {}/{} rádio {} código {}.",
            "Algar Telecom: Usuário {} identificação {} instalação {}, {} dados {}/{} internet {} ordem {}.",
            
            # Abbreviated/Corrupted OCR formats (10 templates)
            "CLT:{} CPF:{} END:{} {} TEL:{} @ {} R$:{} #:{}",
            "{}  {}  {}  {}    {}  {}   {}   {}",
            "{} ({}) {} {} {} {} {} {}",
            "Sr(a). {} CPF {} domiciliado em {} {} telefone {} e-mail {} por {} protocolo {}",
            "USR {} CPF {} LOC {},{} CONT {}/{} VAL {} REF {}",
            "REG: {} | {} | {} {} | {} | {} | {} | {}",
            "{} - {} - {} - {} - {} - {} - {} - {}",
            "NOME:{} CPF:{} END:{} {} TEL:{} EMAIL:{} VALOR:{} NUM:{}",
            "[{}] [{}] [{} {}] [{}] [{}] [{}] [{}]",
            "CLI {} CPF {} DOM {},{} FON {} COR {} VAL {} COD {}"
        ]
    elif country == "uruguay":
        return [
            # Standard business formats (8 templates)
            "El cliente {} con cédula {} reside en {}, {}. Teléfono: {} / Email: {}. Monto: {} - Referencia: {}.",
            "Registro de {} con CI {}. Ubicado en {}, {}. Contacto: {} / {}. Transacción: {} - ID: {}.",
            "Cliente {} identificado con cédula {}. Dirección: {}, {}. Contacto telefónico: {}. Correo: {}. Valor: {} - Número: {}.",
            "Datos del cliente: {} (CI: {}). Domicilio: {}, {}. Tel: {} / {}. Monto operación: {} - Folio: {}.",
            "Información de {} con cédula {}. Vive en {}, {}. Fono: {} - Email: {}. Cantidad: {} - Serie: {}.",
            "CLIENTE: {} CI: {} DIRECCIÓN: {} {} TEL: {} EMAIL: {} MONTO: {} N°: {}",
            "{}|{}|{} {}|{}|{}|{}|{}",
            "Nombre:{} CI:{} Dom:{},{} Cont:{}/{} Val:{} Ref:{}",
            
            # Banking formats (12 templates)
            "Banco República: Cliente {} CI {} sucursal {}, {} contacto {}/{} transferencia {} código {}.",
            "Santander Uruguay: {} con cédula {} dirección {}, {} datos {}/{} operación {} número {}.",
            "Itaú Uruguay: Titular {} documento {} ubicación {} ciudad {} teléfonos {} correos {} cargo {} folio {}.",
            "BBVA Uruguay: Cliente {} identificación {} residencia {}, {} comunicación {}/{} importe {} serie {}.",
            "Banco de la Nación: {} con CI {} dirección {}, {} contactos {}/{} débito {} autorización {}.",
            "Citibank Uruguay: Usuario {} cédula {} ubicación {}, {} datos {}/{} crédito {} expediente {}.",
            "Scotiabank Uruguay: Cuenta {} documento {} domicilio {}, {} teléfono {} email {} saldo {} movimiento {}.",
            "Cuenta Vista: Cliente {} identificación {} residencia {}, {} comunicación {}/{} disponible {} operación {}.",
            "Cuenta Corriente: {} con cédula {} dirección {}, {} contactos {}/{} giro {} cheque {}.",
            "Caja de Ahorros: Titular {} CI {} ubicación {}, {} datos {}/{} depósito {} libreta {}.",
            "Préstamo: {} documento {} domicilio {}, {} teléfonos {}/{} cuota {} período {}.",
            "Tarjeta de Crédito: Cliente {} identificación {} residencia {}, {} comunicación {}/{} límite {} corte {}.",
            
            # Government/Official formats (10 templates)
            "DGI notifica: Contribuyente {} con CI {} dirección {}, {} contacto {}/{} deuda {} intimación {}.",
            "BPS: Trabajador {} cédula {} ubicación {}, {} datos {}/{} jubilación {} expediente {}.",
            "ASSE: Paciente {} documento {} domicilio {}, {} teléfono {} email {} atención {} número {}.",
            "Intendencia de Montevideo: Vecino {} identificación {} residencia {}, {} comunicación {}/{} multa {} boleta {}.",
            "UdelaR: Estudiante {} con CI {} facultad {}, {} contactos {}/{} beca {} código {}.",
            "INAU: Menor {} cédula {} ubicación {}, {} datos {}/{} programa {} expediente {}.",
            "MTOP: Conductor {} documento {} dirección {}, {} teléfonos {}/{} licencia {} trámite {}.",
            "Ministerio de Salud: Ciudadano {} identificación {} residencia {}, {} comunicación {}/{} vacunación {} carnet {}.",
            "Policía Nacional: Persona {} con CI {} ubicación {}, {} contactos {}/{} denuncia {} número {}.",
            "Bomberos: Propietario {} cédula {} dirección {}, {} datos {}/{} habilitación {} expediente {}.",
            
            # Healthcare formats (8 templates)
            "Hospital de Clínicas: Paciente {} CI {} residencia {}, {} emergencia {}/{} consulta {} historia {}.",
            "ASSE Policlínica: {} con cédula {} domicilio {}, {} contactos {}/{} atención {} número {}.",
            "Sanatorio Privado: Sr/a {} documento {} ubicación {}, {} teléfonos {}/{} honorarios {} recibo {}.",
            "Mutualista: Socio {} identificación {} residencia {}, {} comunicación {}/{} orden {} código {}.",
            "Laboratorio: Paciente {} con CI {} dirección {}, {} datos {}/{} análisis {} resultado {}.",
            "Consultorio: {} cédula {} ubicación {}, {} contacto {}/{} consulta {} agenda {}.",
            "Farmacia: Cliente {} documento {} domicilio {}, {} teléfono {} email {} medicamento {} receta {}.",
            "Fisioterapia: Paciente {} identificación {} residencia {}, {} comunicación {}/{} sesión {} orden {}.",
            
            # Commercial/Retail formats (12 templates)
            "Tienda Inglesa: Cliente {} CI {} entrega {}, {} contacto {}/{} compra {} ticket {}.",
            "Disco: {} con cédula {} dirección {}, {} datos {}/{} total {} vale {}.",
            "Devoto: Comprador {} documento {} ubicación {}, {} teléfonos {}/{} adquisición {} número {}.",
            "Ta-Ta: Cliente {} identificación {} residencia {}, {} comunicación {}/{} descuento {} código {}.",
            "Géant: {} con CI {} dirección {}, {} contactos {}/{} producto {} etiqueta {}.",
            "Farmashop: Comprador {} cédula {} ubicación {}, {} datos {}/{} medicamento {} receta {}.",
            "Red Pagos: Usuario {} documento {} dirección {}, {} teléfono {} email {} pago {} comprobante {}.",
            "Ancap: Cliente {} identificación {} estación {}, {} comunicación {}/{} combustible {} vale {}.",
            "UTE: Suscriptor {} con CI {} suministro {}, {} contactos {}/{} factura {} período {}.",
            "OSE: Usuario {} cédula {} conexión {}, {} datos {}/{} agua {} medidor {}.",
            "Conaprole: Cliente {} documento {} ubicación {}, {} teléfonos {}/{} pedido {} orden {}.",
            "Mercado: Comprador {} identificación {} dirección {}, {} comunicación {}/{} fiado {} cuenta {}.",
            
            # Insurance formats (8 templates)
            "BSE Seguros: Asegurado {} CI {} bien {}, {} contacto {}/{} prima {} póliza {}.",
            "Sura Uruguay: {} con cédula {} propiedad {}, {} datos {}/{} cobertura {} certificado {}.",
            "Mapfre Uruguay: Titular {} documento {} ubicación {}, {} teléfonos {}/{} siniestro {} reclamo {}.",
            "La Caja Notarial: Contratante {} identificación {} dirección {}, {} comunicación {}/{} renovación {} endoso {}.",
            "Surco Seguros: {} con CI {} residencia {}, {} contactos {}/{} indemnización {} liquidación {}.",
            "Berkley Uruguay: Asegurado {} cédula {} bien {}, {} datos {}/{} deducible {} caso {}.",
            "Zurich Uruguay: {} documento {} propiedad {}, {} teléfono {} email {} cotización {} propuesta {}.",
            "La República Seguros: Cliente {} identificación {} ubicación {}, {} comunicación {}/{} beneficio {} número {}.",
            
            # Telecommunications (6 templates)
            "Antel: Cliente {} CI {} instalación {}, {} contacto {}/{} plan {} línea {}.",
            "Movistar Uruguay: Usuario {} cédula {} dirección {}, {} datos {}/{} servicio {} cuenta {}.",
            "Claro Uruguay: Suscriptor {} documento {} ubicación {}, {} teléfonos {}/{} factura {} período {}.",
            "Dedicado: {} identificado {} residencia {}, {} comunicación {}/{} internet {} contrato {}.",
            "Cable: Cliente {} con CI {} dirección {}, {} contactos {}/{} televisión {} decodificador {}.",
            "Fibra Óptica: Usuario {} cédula {} instalación {}, {} datos {}/{} velocidad {} orden {}.",
            
            # Utilities (6 templates)
            "UTE: Cliente {} CI {} suministro {}, {} contacto {}/{} consumo {} medidor {}.",
            "OSE: Usuario {} documento {} conexión {}, {} datos {}/{} agua {} cuenta {}.",
            "Gas: Suscriptor {} identificación {} ubicación {}, {} teléfonos {}/{} instalación {} código {}.",
            "AFE: Pasajero {} con cédula {} estación {}, {} comunicación {}/{} boleto {} viaje {}.",
            "Puerto de Montevideo: Importador {} CI {} trámite {}, {} contactos {}/{} mercadería {} manifiesto {}.",
            "Correo Uruguayo: Destinatario {} documento {} dirección {}, {} datos {}/{} envío {} seguimiento {}.",
            
            # Abbreviated/Corrupted OCR formats (10 templates)
            "CLT:{} CI:{} DIR:{} {} TEL:{} @ {} $:{} #:{}",
            "{}  {}  {}  {}    {}  {}   {}   {}",
            "{} ({}) {} {} {} {} {} {}",
            "Sr(a). {} CI {} domiciliado en {} {} teléfono {} e-mail {} por {} folio {}",
            "USR {} CI {} LOC {},{} CONT {}/{} VAL {} REF {}",
            "REG: {} | {} | {} {} | {} | {} | {} | {}",
            "{} - {} - {} - {} - {} - {} - {} - {}",
            "NOM:{} CI:{} DIR:{} {} TEL:{} EMAIL:{} MON:{} NUM:{}",
            "[{}] [{}] [{} {}] [{}] [{}] [{}] [{}]",
            "CLI {} CI {} DOM {},{} FON {} COR {} MON {} COD {}"
        ]
    else:
        return get_sentence_templates("chile")  # Default to Chilean templates

def safe_format_template(template: str, entity_data: List[str]) -> str:
    """
    Safely format a template with entity data, handling placeholder mismatches.
    
    Args:
        template (str): Template string with {} placeholders
        entity_data (List[str]): List of entity values
        
    Returns:
        str: Formatted string
    """
    try:
        return template.format(*entity_data)
    except IndexError:
        # Fallback: truncate entity data to match placeholder count
        placeholder_count = template.count('{}')
        return template.format(*entity_data[:placeholder_count])

def shuffle_template_entities(template: str, entity_data: List[str], 
                            shuffle_probability: float = 0.3) -> Tuple[str, List[str]]:
    """
    Shuffle entity positions in templates to reduce overfitting and improve model generalization.
    
    This function helps prevent the model from learning fixed entity positions by randomly
    rearranging entities while maintaining template coherence. Only applies shuffling
    based on probability to maintain some consistency.
    
    Args:
        template (str): Template string with {} placeholders
        entity_data (List[str]): List of entity values to insert
        shuffle_probability (float): Probability of applying shuffling (0.0-1.0)
        
    Returns:
        Tuple[str, List[str]]: Shuffled template and corresponding entity list
    """
    if random.random() > shuffle_probability or len(entity_data) < 3:
        return template, entity_data
    
    # Count placeholders in template
    placeholder_count = template.count('{}')
    if placeholder_count != len(entity_data):
        return template, entity_data  # Mismatch, return original
    
    # Create shuffling strategies based on template structure
    shuffle_strategies = [
        # Strategy 1: Swap adjacent pairs (gentle shuffling)
        lambda data: _swap_adjacent_pairs(data),
        
        # Strategy 2: Rotate positions (maintains some order)
        lambda data: _rotate_entities(data),
        
        # Strategy 3: Swap first and last (common pattern break)
        lambda data: _swap_first_last(data),
        
        # Strategy 4: Random shuffle of middle entities (keep name/ID stable)
        lambda data: _shuffle_middle_entities(data),
        
        # Strategy 5: Swap entity categories (name<->id, contact<->address)
        lambda data: _swap_entity_categories(data)
    ]
    
    # Apply random shuffling strategy
    strategy = random.choice(shuffle_strategies)
    shuffled_data = strategy(entity_data.copy())
    
    return template, shuffled_data

def _swap_adjacent_pairs(data: List[str]) -> List[str]:
    """Swap adjacent entity pairs to break position patterns."""
    result = data.copy()
    for i in range(0, len(result) - 1, 2):
        if i + 1 < len(result):
            result[i], result[i + 1] = result[i + 1], result[i]
    return result

def _rotate_entities(data: List[str]) -> List[str]:
    """Rotate entity positions by 1-2 positions."""
    if len(data) < 3:
        return data
    
    rotation = random.randint(1, min(2, len(data) - 1))
    return data[rotation:] + data[:rotation]

def _swap_first_last(data: List[str]) -> List[str]:
    """Swap first and last entities."""
    if len(data) < 2:
        return data
    
    result = data.copy()
    result[0], result[-1] = result[-1], result[0]
    return result

def _shuffle_middle_entities(data: List[str]) -> List[str]:
    """Shuffle middle entities while keeping first and last stable."""
    if len(data) < 4:
        return data
    
    result = data.copy()
    middle_section = result[1:-1]
    random.shuffle(middle_section)
    result[1:-1] = middle_section
    return result

def _swap_entity_categories(data: List[str]) -> List[str]:
    """Intelligent entity category swapping based on common patterns."""
    if len(data) < 4:
        return data
    
    result = data.copy()
    
    # Common entity position patterns (name, id, address, city, phone, email, amount, ref)
    # Swap some logical pairs while maintaining template coherence
    
    # Swap contact information (phone <-> email) if both present
    if len(result) >= 6:
        # Positions 4 and 5 are typically phone and email
        if random.random() < 0.5:
            result[4], result[5] = result[5], result[4]
    
    # Swap financial information (amount <-> reference) if both present  
    if len(result) >= 8:
        # Positions 6 and 7 are typically amount and reference
        if random.random() < 0.5:
            result[6], result[7] = result[7], result[6]
    
    return result

def apply_country_noise(sentence: str, country: str, noise_level: float) -> str:
    """
    Apply country-specific noise patterns to a sentence.
    
    Args:
        sentence (str): Original sentence
        country (str): Country code  
        noise_level (float): Noise intensity (0.0-1.0)
        
    Returns:
        str: Sentence with applied noise
    """
    if noise_level <= 0.0:
        return sentence
    
    # For now, apply simple noise patterns
    # This can be enhanced with country-specific abbreviations and noise patterns
    if random.random() < noise_level * 0.3:
        # Simple noise patterns that don't affect entity boundaries
        noise_replacements = {
            "Teléfono": "Tel",
            "Telefone": "Tel", 
            "Email": "E-mail",
            "Monto": "Mto",
            "Valor": "Vlr",
            "Referencia": "Ref",
            "Referência": "Ref",
            "Número": "Núm",
            "Número": "Nro"
        }
        
        for original, replacement in noise_replacements.items():
            if original in sentence:
                sentence = sentence.replace(original, replacement)
                break  # Apply only one noise change
    
    return sentence

def generate_example_with_noise(country: str = "chile", include_noise: bool = True, noise_level: float = 0.2) -> Tuple[str, Dict[str, List[Tuple[int, int, str]]]]:
    """
    Generate a complete customer data example for any supported country with controlled noise and guaranteed zero E1010 errors.
    
    Creates realistic customer information including:
    - Full name with country-specific conventions
    - Country-specific ID number format
    - Complete address with country-specific streets and cities
    - Country-specific phone number format
    - Email address with country-appropriate domains
    - Monetary amount in country currency
    - Sequential reference number
    - Controlled noise that preserves entity boundaries
    
    CRITICAL: Implements advanced entity conflict resolution to guarantee zero E1010 overlapping span errors.
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
    
    Returns:
        Tuple[str, Dict]: A tuple containing:
            - str: Generated sentence with customer data and optional noise
            - Dict: NER annotations with entity positions and labels
                   Format: {"entities": [(start, end, label), ...]}
    
    Example:
        >>> sentence, annotations = generate_example_with_noise("mexico")
        >>> print(sentence)
        "El cliente MARÍA GUADALUPE HERNÁNDEZ GARCÍA con CURP MAGR850315MDFNRL09..."
        >>> print(annotations)
        {"entities": [(11, 38, "CUSTOMER_NAME"), (49, 68, "ID_NUMBER"), ...]}
    """
    # Validate country
    if country not in COUNTRY_DATA:
        country = "chile"  # fallback to Chile
    
    country_info = COUNTRY_DATA[country]
    
    # Generate name components with enhanced second surname support
    first_name, full_name_part, complete_surname = generate_name_components(
        country=country,
        include_second_name=True, second_name_probability=0.4,
        include_second_surname=True, second_surname_probability=0.8
    )
    complete_full_name = f"{full_name_part} {complete_surname}"    # Complete name for entity recognition
    
    # Generate country-specific data
    id_number = generate_id(country)                              # Country-specific ID format
    street = random.choice(country_info['streets'])               # Country-specific street
    street_number = random.randint(10, 999)                      # Street number
    address = f"{street} {street_number}"                         # Complete address
    city = random.choice(country_info['cities'])                  # Country-specific city
    phone = generate_phone(country)                               # Country-specific phone format
    email = generate_email(first_name, complete_surname, country) # Country-specific email
    amount = generate_amount(country)                             # Country-specific currency
    sequence = generate_sequence_number(country)                  # Country-specific sequence
    
    # Country-specific sentence templates with cultural appropriateness
    templates = get_sentence_templates(country)
    
    # Select random template
    template = random.choice(templates)
    
    # Count placeholders to determine entity format needed
    placeholder_count = template.count('{}')
    
    # Prepare entity data based on template placeholder count
    if placeholder_count == 9:
        # Template expects split name (first_name, last_name, id, address, city, phone, email, amount, sequence)
        entity_data = [
            full_name_part, complete_surname, id_number, address, city, 
            phone, email, amount, sequence
        ]
    else:
        # Template expects complete name (8 placeholders: full_name, id, address, city, phone, email, amount, sequence)
        entity_data = [
            complete_full_name, id_number, address, city, 
            phone, email, amount, sequence
        ]
    
    # Apply entity shuffling to reduce overfitting (30% probability)
    template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
    
    # Format template with shuffled entity data - Add error handling
    try:
        sentence = safe_format_template(template, shuffled_entities)
    except IndexError as e:
        # Fallback: if still mismatch, use complete name format
        entity_data = [
            complete_full_name, id_number, address, city, 
            phone, email, amount, sequence
        ]
        template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
        sentence = safe_format_template(template, shuffled_entities)
    
    # Apply country-specific noise if requested
    if include_noise:
        sentence = apply_country_noise(sentence, country, noise_level)
    
    # CRITICAL: Advanced entity detection with E1010 conflict resolution
    # NOTE: Use original entity values for detection - always use complete names for consistency
    if placeholder_count == 9:
        # For split name templates, we still want to identify the complete name as one entity
        entity_mappings = [
            (complete_full_name, "CUSTOMER_NAME"),        # Full customer name
            (id_number, "ID_NUMBER"),                     # Country-specific ID
            (address, "ADDRESS"),                         # Street address
            (city, "ADDRESS"),                            # City
            (phone, "PHONE_NUMBER"),                      # Phone number
            (email, "EMAIL"),                             # Email address
            (amount, "AMOUNT"),                           # Amount with currency
            (sequence, "SEQ_NUMBER")                      # Sequential number
        ]
    else:
        # Standard entity mappings for complete name templates
        entity_mappings = [
            (complete_full_name, "CUSTOMER_NAME"),        # Full customer name
            (id_number, "ID_NUMBER"),                     # Country-specific ID
            (address, "ADDRESS"),                         # Street address
            (city, "ADDRESS"),                            # City
            (phone, "PHONE_NUMBER"),                      # Phone number
            (email, "EMAIL"),                             # Email address
            (amount, "AMOUNT"),                           # Amount with currency
            (sequence, "SEQ_NUMBER")                      # Sequential number
        ]
    
    # IMPROVED ENTITY DETECTION WITH CONFLICT RESOLUTION (E1010 FIX)
    used_positions = set()
    entities = []
    
    # Sort entities by length (longest first) to prioritize longer matches
    # This prevents shorter entities from blocking longer, more important ones
    sorted_mappings = sorted(entity_mappings, key=lambda x: len(x[0]), reverse=True)
    
    for entity_text, label in sorted_mappings:
        if not entity_text.strip():  # Skip empty entities
            continue
            
        # IMPROVEMENT: Try multiple search strategies for better entity detection
        start_pos = -1
        
        # Strategy 1: Exact match
        start_pos = sentence.find(entity_text)
        
        # Strategy 2: Try with normalized spaces if exact match fails
        if start_pos == -1:
            normalized_entity = ' '.join(entity_text.split())
            start_pos = sentence.find(normalized_entity)
            if start_pos != -1:
                entity_text = normalized_entity  # Use the normalized version
        
        # Strategy 3: For SEQ_NUMBER, try pattern matching if still not found
        if start_pos == -1 and label == "SEQ_NUMBER":
            import re
            # Create a pattern that allows for minor spacing variations
            pattern = re.escape(entity_text).replace(r'\ ', r'\s*')
            match = re.search(pattern, sentence)
            if match:
                start_pos = match.start()
                entity_text = match.group()  # Use the actual matched text
        
        # Strategy 4: For SEQ_NUMBER, try comprehensive pattern matching with character variations
        if start_pos == -1 and label == "SEQ_NUMBER":
            import re
            # Create robust pattern for mixed alphanumeric sequences
            # Handle patterns like: 55454-A, 46544564A, 079279-A, CL-12345, etc.
            
            # Escape the original entity text
            pattern = re.escape(entity_text)
            
            # Apply common OCR/noise corruptions:
            # 1. O/0 confusion (both directions)
            pattern = pattern.replace('0', '[0OoΘ]').replace('O', '[0OoΘ]').replace('o', '[0OoΘ]')
            
            # 2. I/1/l confusion (all variations)  
            pattern = pattern.replace('1', '[1IlĪį]').replace('I', '[1IlĪį]').replace('l', '[1IlĪį]')
            
            # 3. 5/S confusion
            pattern = pattern.replace('5', '[5SŞ]').replace('S', '[5SŞ]')
            
            # 4. 6/G confusion
            pattern = pattern.replace('6', '[6GĞ]').replace('G', '[6GĞ]')
            
            # 5. 8/B confusion
            pattern = pattern.replace('8', '[8BĞ]').replace('B', '[8BĞ]')
            
            # 6. 2/Z confusion
            pattern = pattern.replace('2', '[2ZĞ]').replace('Z', '[2ZĞ]')
            
            # 7. Handle spacing variations around separators
            pattern = pattern.replace(r'\-', r'\s*\-\s*')  # Allow spaces around hyphens
            pattern = pattern.replace(r'\ ', r'\s*')        # Allow flexible spacing
            
            # 8. Case insensitive matching for letters
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                start_pos = match.start()
                entity_text = match.group()  # Use the actual matched text
                
        # Strategy 5: Last resort - conservative fuzzy pattern matching for SEQ_NUMBER
        if start_pos == -1 and label == "SEQ_NUMBER":
            import re
            # Only try fuzzy matching if we haven't found the entity yet
            # Look for sequence-like patterns that match the original entity structure
            original_entity = entity_text.strip()
            
            # Determine the pattern type of the original entity
            if re.match(r'^\d+$', original_entity):
                # Pure number pattern
                pattern = r'\b\d{' + str(len(original_entity)-1) + ',' + str(len(original_entity)+1) + r'}\b'
                matches = re.finditer(pattern, sentence)
            elif re.match(r'^\d+-[A-Za-z]$', original_entity):
                # Number-hyphen-letter pattern
                pattern = r'\b\d{4,8}\s*\-\s*[A-Za-z]\b'
                matches = re.finditer(pattern, sentence, re.IGNORECASE)
            elif re.match(r'^[A-Za-z]{2}-\d+$', original_entity):
                # Country prefix pattern
                pattern = r'\b[A-Za-z]{2}\s*\-\s*\d{4,6}\b'
                matches = re.finditer(pattern, sentence, re.IGNORECASE)
            elif re.match(r'^[A-Za-z]\d+$', original_entity):
                # Letter-number pattern
                pattern = r'\b[A-Za-z]\d{5,8}\b'
                matches = re.finditer(pattern, sentence, re.IGNORECASE)
            else:
                # Generic alphanumeric pattern
                pattern = r'\b[A-Za-z0-9]{' + str(max(4, len(original_entity)-2)) + ',' + str(len(original_entity)+2) + r'}\b'
                matches = re.finditer(pattern, sentence, re.IGNORECASE)
            
            # Find the first available match
            for match in matches:
                test_start, test_end = match.span()
                test_range = set(range(test_start, test_end))
                if not test_range.intersection(used_positions):
                    # Additional validation: make sure it's not part of another entity type
                    matched_text = match.group()
                    # Skip if it looks like a phone number, ID, or amount
                    if not (re.match(r'^\+?[\d\s\-()]{8,}$', matched_text) or  # Phone pattern
                            '.' in matched_text or '$' in matched_text or       # Amount indicators  
                            '@' in matched_text):                               # Email indicator
                        start_pos = test_start
                        entity_text = matched_text
                        break
        
        if start_pos != -1:
            end_pos = start_pos + len(entity_text)
            
            # CRITICAL: Check if this position overlaps with already used positions
            position_range = set(range(start_pos, end_pos))
            if not position_range.intersection(used_positions):
                entities.append((start_pos, end_pos, label))
                used_positions.update(position_range)
                # Entity successfully added without conflicts
    
    # Sort entities by start position for consistent output
    entities.sort(key=lambda x: x[0])
    
    # MODIFICATION 2: Merge consecutive ADDRESS entities
    entities = merge_consecutive_address_entities(sentence, entities)
    
    return (sentence, {"entities": entities})

def generate_example_with_custom_mode(country: str = "chile", 
                                    mode: str = "full",
                                    include_noise: bool = True, 
                                    noise_level: float = 0.2) -> Tuple[str, Dict[str, List[Tuple[int, int, str]]]]:
    """
    Generate a complete customer data example with specific mode control for entity balance.
    
    This function addresses the entity imbalance problem by providing targeted generation modes
    that boost underrepresented entities (ADDRESS, CUSTOMER_NAME, ID_NUMBER) while reducing
    overrepresented ones (SEQ_NUMBER, AMOUNT, DATE).
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        mode (str): Generation mode:
                   - 'full': All entities (NAME, ID, ADDRESS, PHONE, EMAIL, AMOUNT, SEQ)
                   - 'personal_id': NAME + ID_NUMBER (boost ID_NUMBER frequency)
                   - 'address_focused': NAME + ADDRESS (boost ADDRESS frequency)
                   - 'contact_only': NAME + PHONE + EMAIL
                   - 'financial_heavy': AMOUNT + SEQ_NUMBER + DATE (enhanced for OCR tables with multiple sequences)
                   - 'table_heavy': Multiple AMOUNTS + SEQ_NUMBERS + DATES (simulates OCR table rows)
                   - 'minimal_entities': NAME + ID + PHONE (basic)
        include_noise (bool): Whether to add noise
        noise_level (float): Noise intensity (0.0-1.0)
        
    Returns:
        Tuple[str, Dict]: Generated sentence and NER annotations
    """
    
    if country not in COUNTRY_DATA:
        country = "chile"
        
    country_info = COUNTRY_DATA[country]
    
    # Generate name components
    first_name = random.choice(country_info['first_names'])
    last_name = random.choice(country_info['surnames'])
    second_last_name = random.choice(country_info['surnames'])
    
    # Create full name variants
    if country == "brazil":
        # Brazilian names can have middle names
        if random.choice([True, False]):
            middle_name = random.choice(country_info['first_names'])
            full_name_part = f"{first_name} {middle_name}"
        else:
            full_name_part = first_name
        complete_surname = f"{last_name} {second_last_name}"
    else:
        # Spanish-speaking countries format
        full_name_part = first_name
        complete_surname = f"{last_name} {second_last_name}"
    
    complete_full_name = f"{full_name_part} {complete_surname}"
    
    # Initialize entity mappings with required name
    entity_mappings = [(complete_full_name, "CUSTOMER_NAME")]
    
    if mode == "full":
        # All entities - use existing generate_example_with_noise
        return generate_example_with_noise(country, include_noise, noise_level)
        
    elif mode == "personal_id":
        # Focus on NAME + ID_NUMBER to boost ID_NUMBER frequency
        id_number = generate_id(country)
        templates = [
            "Cliente {} identificado con documento {}.",
            "El documento de identidad de {} es {}.",
            "Registro de {} con ID {}.",
            "{} presenta documento número {}.",
            "Identificación de {}: {}.",
            "Verificación de identidad: {} documento {}.",
            "Cliente {} con identificación {}.",
            "Número de identificación de {}: {}."
        ]
        template = random.choice(templates)
        
        # Apply entity shuffling to reduce overfitting
        entity_data = [complete_full_name, id_number]
        template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
        sentence = safe_format_template(template, shuffled_entities)
        
        entity_mappings.append((id_number, "ID_NUMBER"))
        
    elif mode == "address_focused":
        # Focus on NAME + ADDRESS to boost ADDRESS frequency
        street = random.choice(country_info['streets'])
        street_number = random.randint(10, 999)
        address = f"{street} {street_number}"
        city = random.choice(country_info['cities'])
        
        templates = [
            "El domicilio de {} está en {}, {}.",
            "Dirección registrada de {}: {}, {}.",
            "{} vive en {}, ciudad de {}.",
            "Ubicación de {}: {}, {}.",
            "Domicilio actual de {} es {}, {}.",
            "Residencia de {}: {}, {}.",
            "Dirección de correspondencia de {}: {}, {}.",
            "Cliente {} con domicilio en {}, {}."
        ]
        template = random.choice(templates)
        
        # Apply entity shuffling to reduce overfitting
        entity_data = [complete_full_name, address, city]
        template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
        sentence = safe_format_template(template, shuffled_entities)
        
        entity_mappings.extend([(address, "ADDRESS"), (city, "ADDRESS")])
        
    elif mode == "contact_only":
        # NAME + PHONE + EMAIL
        phone = generate_phone(country)
        email = generate_email(first_name.split()[0], last_name, country)
        
        templates = [
            "Contacto de {}: teléfono {} y email {}.",
            "{} puede ser contactado al {} o {}.",
            "Datos de contacto de {}: {} / {}.",
            "Para comunicarse con {}: {} o {}.",
            "Tel: {} Email: {} de {}.",
            "Información de contacto - Cliente {}: {} - {}.",
            "Comunicación con {}: teléfono {} correo {}.",
            "Datos de {} para contacto: {} y {}."
        ]
        template = random.choice(templates)
        
        # Apply entity shuffling to reduce overfitting
        entity_data = [complete_full_name, phone, email]
        template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
        sentence = safe_format_template(template, shuffled_entities)
        
        entity_mappings.extend([(phone, "PHONE_NUMBER"), (email, "EMAIL")])
        
    elif mode == "financial_heavy":
        # Enhanced AMOUNT + SEQ_NUMBER + DATE mode with OCR table patterns
        
        # 30% chance for multi-sequence table patterns (common in OCR tables)
        if random.random() < 0.3:
            # Generate multiple sequences for table-like patterns
            amount1 = generate_amount(country)
            amount2 = generate_amount(country)
            sequence1 = generate_sequence_number(country)
            sequence2 = generate_sequence_number(country)
            date1 = generate_date(country)
            date2 = generate_date(country)
            
            table_templates = [
                # OCR table row patterns (multiple entries)
                "{} {} {} {} {} {}",
                "{} | {} | {} | {} | {} | {}",
                "{}    {}    {}    {}    {}    {}",
                "{} - {} - {} - {} - {} - {}",
                "{}  {}  {}  {}  {}  {}",
                "{}/{}/{} {}/{}/{}",
                "{}||{}||{} {}||{}||{}",
                "{} #{} {} {} #{} {}",
                "PAGO {} N°{} {} PAGO {} N°{} {}",
                "TXN:{} REF:{} DT:{} TXN:{} REF:{} DT:{}",
                
                # Table column headers corrupted by OCR
                "MONTO CODIGO FECHA {} {} {} {} {} {}",
                "AMT REF DATE {} {} {} {} {} {}",
                "VALOR NUM DIA {} {} {} {} {} {}",
                "$ N° FECHA {} {} {} {} {} {}",
                
                # Corrupted table separators
                "{} .{} .{} .{} .{} .{}",
                "{}   {}   {}   {}   {}   {}",
                "{} I {} I {} I {} I {} I {}",
                "{} l {} l {} l {} l {} l {}",  # OCR confuses | with l
                
                # Financial statement patterns
                "DEBE {} HAB {} REF {} DEBE {} HAB {} REF {}",
                "DEB:{} CRED:{} N°:{} DEB:{} CRED:{} N°:{}",
                "CARGO {} ABONO {} ID {} CARGO {} ABONO {} ID {}",
                
                # Multi-row invoice patterns
                "ITEM {} QTY {} DATE {} ITEM {} QTY {} DATE {}",
                "PROD:{} COD:{} FEC:{} PROD:{} COD:{} FEC:{}",
                "LIN {} REF {} {} LIN {} REF {} {}"
            ]
            
            template = random.choice(table_templates)
            sentence = template.format(amount1, sequence1, date1, amount2, sequence2, date2)
            entity_mappings = [
                (amount1, "AMOUNT"), (sequence1, "SEQ_NUMBER"), (date1, "DATE"),
                (amount2, "AMOUNT"), (sequence2, "SEQ_NUMBER"), (date2, "DATE")
            ]
            
        else:
            # Standard single-entry patterns (70% of cases)
            amount = generate_amount(country)
            sequence = generate_sequence_number(country)
            date = generate_date(country)
            
            templates = [
                # Standard formal templates
                "Transacción por {} - N° {} - Fecha: {}.",
                "Pago de {} folio {} realizado el {}.",
                "Monto {} referencia {} procesado {}.",
                "Operación {} código {} fecha {}.",
                "Valor {} N° {} del {}.",
                "Importe {} ref. {} - {}.",
                "Total {} serie {} en fecha {}.",
                "Cantidad {} número {} con fecha {}.",
                "Suma {} código {} registrada el {}.",
                "Factura {} N° {} emitida {}.",
                
                # Banking/Financial institution templates
                "Transferencia {} REF {} FECHA {}",
                "DÉBITO {} N°{} {}",
                "CRÉDITO {} TXN:{} {}",
                "RETIRO {} DOC:{} {}",
                "DEPÓSITO {} #{}  {}",
                
                # Invoice/Receipt styles
                "TOTAL $ {} COMPROBANTE {} {}",
                "SUBTOTAL {} NUM {} DIA {}",
                "IMPORTE {} TICKET {} {}",
                "PRECIO {} BOLETA {} {}",
                
                # Transaction log styles
                "{} | {} | {}",
                "AMT:{} REF:{} DATE:{}",
                "$ {} #{} {}",
                "VALOR:{} COD:{} {}",
                
                # Error/corrupted document styles (OCR simulation)
                "{}  N.{}  {}",
                "{}    {} {}",
                "$ {} - {}  {}",
                "TOT {} N{}{}",
                
                # System/database output styles
                "Amount: {} | Ref: {} | Date: {}",
                "[{}] [{}] [{}]",
                "{}||{}||{}",
                "Val={} Id={} Dt={}",
                
                # Mixed language patterns (common in Latin America)
                "AMOUNT {} NUM {} {}",
                "QTY {} REF {} {}",
                "SUM {} ID {} {}",
                
                # NEW: Sequential number heavy patterns (for tables with lots of seq_numbers)
                "N°{} {} {} N°{} {} {}",
                "REF{} AMT:{} DT:{} REF{} AMT:{} DT:{}",
                "#{} ${} {} #{} ${} {}",
                "COD{} VAL{} FEC{} COD{} VAL{} FEC{}",
                "ID:{} TOTAL:{} WHEN:{}",
                "SEQ:{} MONTO:{} DIA:{}",
                "NUM:{} VALOR:{} FECHA:{}",
                
                # OCR corruption patterns with heavy sequences
                "{}N{}{} {}N{}{} {}N{}{}",
                "{} .{} .{} .{} .{} .{} .{} .{}",
                "{}|{}|{} {}|{}|{} {}|{}|{}",
                
                # Table fragment patterns (single row from table)
                "FILA {} COL {} FECHA {}",
                "ROW {} VAL {} DATE {}",
                "LIN:{} AMT:{} DT:{}",
                
                # NEW: Enhanced complex financial patterns
                "BANCO: Operación {} por valor {} ejecutada {} - Estado: PROCESADA",
                "E-COMMERCE: Compra #{} monto {} procesada {} - Merchant: APPROVED",
                "TARJETA: Cargo {} autorización {} realizado {} - Terminal: ACTIVO",
                "TRANSFERENCIA: Envío {} código {} completado {} - Beneficiario: CONFIRMADO",
                "PRÉSTAMO: Cuota {} número {} vencimiento {} - Mora: NINGUNA",
                "INVERSIÓN: Aporte {} contrato {} constituido {} - Rendimiento: CALCULADO",
                "SEGURO: Prima {} póliza {} cobrada {} - Cobertura: VIGENTE",
                "IMPUESTO: Tributo {} boleta {} cancelado {} - Estado: PAGADO",
                "PENSIÓN: Cotización {} período {} depositada {} - AFP: REGISTRADA",
                "DIVIDENDO: Utilidad {} acción {} distribuida {} - Sociedad: APROBADA",
                
                # Complex OCR table patterns
                "│ {} │ {} │ {} │ PROCESADO │",
                "║ {} ║ {} ║ {} ║ COMPLETADO ║",
                "┌─ {} ─┬─ {} ─┬─ {} ─┐",
                "├ {} ┤ {} ┤ {} ┤",
                "▌{} ▐ {} ▐ {} ▐",
                "◆ {} ◇ {} ◆ {} ◇",
                "● {} ○ {} ● {} ○",
                "★ {} ☆ {} ★ {} ☆",
                
                # Financial document headers with data
                "ESTADO DE CUENTA - Saldo {} Mov {} Fecha {} - BANCO CENTRAL",
                "COMPROBANTE FISCAL - Total {} Ref {} Emisión {} - HACIENDA",
                "EXTRACTO BANCARIO - Cargo {} Num {} Período {} - ENTIDAD FINANCIERA",
                "LIQUIDACIÓN SALARIAL - Haberes {} Código {} Mes {} - RECURSOS HUMANOS",
                "BOLETA TRIBUTARIA - Impuesto {} Timbre {} Vence {} - SERVICIO IMPUESTOS",
                "FACTURA COMERCIAL - Neto {} Serie {} Fecha {} - PROVEEDOR AUTORIZADO",
                "ORDEN DE PAGO - Importe {} Cheque {} Librado {} - TESORERÍA GENERAL",
                "NOTA DE CRÉDITO - Descuento {} Documento {} Aplicado {} - CONTABILIDAD"
            ]
            
            # Sometimes add extra sequence numbers for sequence-heavy patterns
            if random.random() < 0.4:
                extra_seq = generate_sequence_number(country)
                enhanced_templates = [
                    "{} REF:{} ALT:{} {}",
                    "{} N°{} SUBN°{} {}",
                    "MONTO:{} ID:{} SUB:{} FECHA:{}",
                    "VAL:{} COD:{} REF:{} DT:{}",
                    "{} #{} @{} {}",
                    "AMT:{} MAIN:{} AUX:{} DATE:{}"
                ]
                if random.random() < 0.5:
                    template = random.choice(enhanced_templates)
                    
                    # Apply entity shuffling for enhanced templates
                    entity_data = [amount, sequence, extra_seq, date]
                    template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
                    sentence = safe_format_template(template, shuffled_entities)
                    
                    entity_mappings = [(amount, "AMOUNT"), (sequence, "SEQ_NUMBER"), (extra_seq, "SEQ_NUMBER"), (date, "DATE")]
                else:
                    template = random.choice(templates)
                    
                    # Apply entity shuffling for standard templates
                    entity_data = [amount, sequence, date]
                    template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
                    sentence = safe_format_template(template, shuffled_entities)
                    
                    entity_mappings = [(amount, "AMOUNT"), (sequence, "SEQ_NUMBER"), (date, "DATE")]
            else:
                template = random.choice(templates)
                
                # Apply entity shuffling for standard templates
                entity_data = [amount, sequence, date]
                template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
                sentence = safe_format_template(template, shuffled_entities)
                
                entity_mappings = [(amount, "AMOUNT"), (sequence, "SEQ_NUMBER"), (date, "DATE")]
        
    elif mode == "table_heavy":
        # Table-optimized mode: Multiple AMOUNTS + SEQ_NUMBERS + DATES (OCR table simulation)
        # Generates 3-6 entities per line to simulate table rows
        num_entries = random.randint(3, 6)
        amounts = [generate_amount(country) for _ in range(num_entries)]
        sequences = [generate_sequence_number(country) for _ in range(num_entries)]
        dates = [generate_date(country) for _ in range(num_entries)]
        
        # OCR table row patterns with many sequences (enhanced with more complex patterns)
        table_patterns = [
            # Pipe-separated table (most common OCR pattern)
            " | ".join([f"{a} | {s} | {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Space-separated table (OCR loses formatting)
            "  ".join([f"{a}  {s}  {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Tab-like spacing (irregular OCR)
            "    ".join([f"{a}    {s}    {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Comma-separated (CSV-like)
            ", ".join([f"{a}, {s}, {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Mixed separators (corrupted OCR)
            " | ".join([f"{a} - {s} - {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Column headers + data (OCR includes headers)
            f"MONTO CODIGO FECHA " + "  ".join([f"{a} {s} {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Numbered rows (table with row numbers)
            " | ".join([f"{i+1}. {a} #{s} {d}" for i, (a, s, d) in enumerate(zip(amounts, sequences, dates))]),
            
            # Database-style output
            " || ".join([f"{a}||{s}||{d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Financial statement style
            " ".join([f"DEB:{a} REF:{s} DT:{d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Invoice line items
            " | ".join([f"ITEM:{s} VAL:{a} FEC:{d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # NEW: Enhanced complex table patterns
            
            # Excel-style table with borders
            "┌" + "─".join([f"─{a}─┬─{s}─┬─{d}─" for a, s, d in zip(amounts, sequences, dates)]) + "┐",
            
            # Complex financial table patterns
            " ║ ".join([f"DEBE {a} ║ DOC {s} ║ FECHA {d}" for a, s, d in zip(amounts, sequences, dates)]),
            " ▌ ".join([f"${a} ▌ #{s} ▌ {d}" for a, s, d in zip(amounts, sequences, dates)]),
            " ● ".join([f"TOT:{a} ● REF:{s} ● DT:{d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Complex OCR corrupted patterns
            "".join([f"{a}|{s}|{d}||" for a, s, d in zip(amounts, sequences, dates)]),
            "".join([f"{a}.{s}.{d}.." for a, s, d in zip(amounts, sequences, dates)]),
            "".join([f"{a}#{s}#{d}##" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Table with status indicators
            " │ ".join([f"{a} │ {s} │ {d} │ ✓" for a, s, d in zip(amounts, sequences, dates)]),
            " ┃ ".join([f"{a} ┃ {s} ┃ {d} ┃ OK" for a, s, d in zip(amounts, sequences, dates)]),
            " ▐ ".join([f"{a} ▐ {s} ▐ {d} ▐ PROC" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Complex financial report patterns
            "BALANCE: " + " / ".join([f"ACTIVO {a} CÓDIGO {s} PERÍODO {d}" for a, s, d in zip(amounts, sequences, dates)]),
            "FLUJO: " + " → ".join([f"INGRESO {a} REF {s} FECHA {d}" for a, s, d in zip(amounts, sequences, dates)]),
            "ESTADO: " + " ◊ ".join([f"SALDO {a} CTA {s} CORTE {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Advanced OCR table corruption
            " ̃ ".join([f"{a} ̃ {s} ̃ {d}" for a, s, d in zip(amounts, sequences, dates)]),  # OCR tilde confusion
            " ̄ ".join([f"{a} ̄ {s} ̄ {d}" for a, s, d in zip(amounts, sequences, dates)]),  # OCR line confusion
            " ` ".join([f"{a} ` {s} ` {d}" for a, s, d in zip(amounts, sequences, dates)]),  # OCR apostrophe confusion
            
            # Table with merged cells simulation
            f"RESUMEN FINANCIERO: {amounts[0]} | {sequences[0]} | {dates[0]} " + 
            " + ".join([f"{a} | {s} | {d}" for a, s, d in zip(amounts[1:], sequences[1:], dates[1:])]),
            
            # Multi-level table patterns
            "NIVEL 1: " + " >> ".join([f"VAL {a} ID {s} DT {d}" for a, s, d in zip(amounts[:2], sequences[:2], dates[:2])]) +
            " NIVEL 2: " + " >> ".join([f"VAL {a} ID {s} DT {d}" for a, s, d in zip(amounts[2:], sequences[2:], dates[2:])]),
            
            # Financial reconciliation patterns
            "CONCILIACIÓN: " + " vs ".join([f"BANCO {a} REF {s} {d}" for a, s, d in zip(amounts, sequences, dates)]),
            
            # Complex spacing patterns (OCR artifacts)
            "   ".join([f"{a}   {s}   {d}   " for a, s, d in zip(amounts, sequences, dates)]),
            "     ".join([f"{a}     {s}     {d}     " for a, s, d in zip(amounts, sequences, dates)]),
            
            # Table with calculation patterns
            " = ".join([f"LÍNEA {i+1}: {a} × {s} = {d}" for i, (a, s, d) in enumerate(zip(amounts, sequences, dates))])
        ]
        
        sentence = random.choice(table_patterns)
        
        # Create entity mappings for all amounts, sequences, and dates
        entity_mappings = []
        for a, s, d in zip(amounts, sequences, dates):
            entity_mappings.extend([(a, "AMOUNT"), (s, "SEQ_NUMBER"), (d, "DATE")])
        
    elif mode == "minimal_entities":
        # NAME + ID + PHONE (basic coverage)
        id_number = generate_id(country)
        phone = generate_phone(country)
        
        templates = [
            "{} (ID: {}) contacto: {}.",
            "Cliente {} documento {} tel {}.",
            "{} - {} - {}.",
            "Reg: {} ID {} fono {}.",
            "{} con {} llamar {}.",
            "Usuario {} identificación {} teléfono {}.",
            "Registro {} documento {} contacto {}."
        ]
        template = random.choice(templates)
        
        # Apply entity shuffling to reduce overfitting
        entity_data = [complete_full_name, id_number, phone]
        template, shuffled_entities = shuffle_template_entities(template, entity_data, 0.3)
        sentence = safe_format_template(template, shuffled_entities)
        
        entity_mappings.extend([(id_number, "ID_NUMBER"), (phone, "PHONE_NUMBER")])
        
    else:
        # Fallback to full mode
        return generate_example_with_noise(country, include_noise, noise_level)
    
    # Apply country-specific noise if requested
    if include_noise:
        sentence = apply_country_noise(sentence, country, noise_level)
    
    # Entity detection with conflict resolution
    used_positions = set()
    entities = []
    sorted_mappings = sorted(entity_mappings, key=lambda x: len(x[0]), reverse=True)
    
    for entity_text, label in sorted_mappings:
        if not entity_text.strip():
            continue
            
        start_pos = sentence.find(entity_text)
        if start_pos != -1:
            end_pos = start_pos + len(entity_text)
            position_range = set(range(start_pos, end_pos))
            if not position_range.intersection(used_positions):
                entities.append((start_pos, end_pos, label))
                used_positions.update(position_range)
    
    entities.sort(key=lambda x: x[0])
    entities = merge_consecutive_address_entities(sentence, entities)
    
    return (sentence, {"entities": entities})

def merge_consecutive_address_entities(text: str, entities: List[Tuple[int, int, str]]) -> List[Tuple[int, int, str]]:
    """
    Merge consecutive ADDRESS entities that are separated by short gaps (commas, spaces).
    
    Example:
        INPUT:  ADDRESS: "Av. Américo Vespucio 263" + ADDRESS: "Valdivia"
        OUTPUT: ADDRESS: "Av. Américo Vespucio 263, Valdivia"
    
    Args:
        text (str): The full text
        entities (List[Tuple[int, int, str]]): List of (start, end, label) tuples
    
    Returns:
        List[Tuple[int, int, str]]: Merged entities list
    """
    if not entities:
        return entities
    
    merged_entities = []
    address_entities = []
    
    # Separate ADDRESS entities from others
    for start, end, label in entities:
        if label == "ADDRESS":
            address_entities.append((start, end, label))
        else:
            merged_entities.append((start, end, label))
    
    if len(address_entities) <= 1:
        # No merging needed
        return entities
    
    # Sort address entities by position
    address_entities.sort(key=lambda x: x[0])
    
    # Merge consecutive address entities
    i = 0
    while i < len(address_entities):
        current_start, current_end, label = address_entities[i]
        
        # Look for consecutive address entities
        consecutive_addresses = [(current_start, current_end, label)]
        j = i + 1
        
        while j < len(address_entities):
            next_start, next_end, next_label = address_entities[j]
            
            # Check if next address is close enough to merge (within 10 characters)
            gap_text = text[current_end:next_start].strip()
            if len(gap_text) <= 10 and all(c in ", \n\t-" for c in gap_text):
                # Merge these addresses
                consecutive_addresses.append((next_start, next_end, next_label))
                current_end = next_end
                j += 1
            else:
                break
        
        # Create merged address entity
        if len(consecutive_addresses) > 1:
            # Multiple addresses to merge
            merged_start = consecutive_addresses[0][0]
            merged_end = consecutive_addresses[-1][1]
            merged_entities.append((merged_start, merged_end, "ADDRESS"))
        else:
            # Single address, keep as is
            merged_entities.append(consecutive_addresses[0])
        
        i = j if j > i + 1 else i + 1
    
    # Sort all entities by start position
    merged_entities.sort(key=lambda x: x[0])
    return merged_entities

def generate_chilean_example_with_noise(include_noise: bool = True, noise_level: float = 0.2) -> Tuple[str, Dict[str, List[Tuple[int, int, str]]]]:
    """
    Generate a complete Chilean customer data example with controlled noise and guaranteed zero E1010 errors.
    (Backwards compatibility wrapper)
    """
    return generate_example_with_noise("chile", include_noise, noise_level)

def generate_multiple_chilean_examples_with_noise(count: int = 5, include_noise: bool = True, noise_level: float = 0.2) -> List[Tuple[str, Dict[str, List[Tuple[int, int, str]]]]]:
    """
    Generate multiple Chilean customer data examples with controlled noise for training or testing.
    
    Useful for creating datasets for machine learning models, particularly
    for Named Entity Recognition (NER) training in Chilean customer service or
    financial applications.
    
    Args:
        count (int): Number of examples to generate
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
    
    Returns:
        List[Tuple]: List of (sentence, annotations) tuples
    """
    examples = []
    for _ in range(count):
        examples.append(generate_chilean_example_with_noise(include_noise, noise_level))
    return examples

def generate_chilean_example_with_mode(mode: str = "full", include_noise: bool = True) -> Tuple[str, Dict[str, List[Tuple[int, int, str]]]]:
    """
    Generate Chilean customer data example with specific complexity mode and optional noise.
    
    Supports different complexity modes for varied NER training scenarios:
    - full: All entity types (names, IDs, addresses, contact, financial)
    - addr_only: Names and addresses only
    - id_only: Names and ID numbers only  
    - contact_only: Names and contact information only
    - financial_only: Names and financial information only
    
    Args:
        mode (str): Complexity mode
        include_noise (bool): Whether to add realistic noise patterns
    
    Returns:
        Tuple[str, Dict]: Generated sentence and NER annotations
    """
    # Generate Chilean name components
    first_name, full_name_part, complete_surname = generate_chilean_name_components()
    complete_full_name = f"{full_name_part} {complete_surname}"
    
    sentence = ""
    entity_mappings = [(complete_full_name, "CUSTOMER_NAME")]

    if mode == "full":
        return generate_chilean_example_with_noise(include_noise=include_noise)
        
    elif mode == "addr_only":
        address = f"{random.choice(chilean_streets)} {random.randint(10, 999)}"
        city = random.choice(chilean_cities)
        sentence = f"El domicilio de {complete_full_name} es {address}, {city}."
        entity_mappings.extend([(address, "ADDRESS"), (city, "ADDRESS")])
        
    elif mode == "id_only":
        rut_number = generate_chilean_rut()
        sentence = f"El RUT de {complete_full_name} es {rut_number}."
        entity_mappings.append((rut_number, "ID_NUMBER"))
        
    elif mode == "contact_only":
        phone = generate_chilean_phone()
        email = generate_chilean_email(first_name, complete_surname)
        sentence = f"Contactar a {complete_full_name} al {phone} o via email a {email}."
        entity_mappings.extend([(phone, "PHONE_NUMBER"), (email, "EMAIL")])
        
    elif mode == "financial_only":
        amount = generate_chilean_amount()
        sequence = generate_chilean_sequence_number()
        sentence = f"El cliente {complete_full_name} tiene un monto de {amount} (ref: {sequence})."
        entity_mappings.extend([(amount, "AMOUNT"), (sequence, "SEQ_NUMBER")])
        
    else:
        raise ValueError(f"Unknown mode: {mode}")

    if include_noise:
        sentence = add_realistic_noise(sentence)

    # Simplified entity detection for these specific modes
    entities = []
    used_positions = set()
    sorted_mappings = sorted(entity_mappings, key=lambda x: len(x[0]), reverse=True)
    
    for entity_text, label in sorted_mappings:
        if not entity_text.strip():
            continue
            
        start_pos = sentence.find(entity_text)
        if start_pos != -1:
            end_pos = start_pos + len(entity_text)
            position_range = set(range(start_pos, end_pos))
            if not position_range.intersection(used_positions):
                entities.append((start_pos, end_pos, label))
                used_positions.update(position_range)
    
    entities.sort(key=lambda x: x[0])
    return (sentence, {"entities": entities})

# -----------------
# Large-Scale spaCy Training Dataset Creation
# -----------------

def make_chilean_docbin_with_noise(n_total: int = 100000, 
                                 balance: bool = True, 
                                 include_noise: bool = True,
                                 noise_level: float = 0.2,
                                 output_dir: str = ".") -> Tuple[DocBin, Dict[str, int]]:
    """
    Create a spaCy DocBin for Chilean NER training with controlled noise and guaranteed zero E1010 errors.
    
    This function generates a comprehensive Chilean dataset with varied complexity modes
    and realistic noise patterns to ensure robust NER model training for Chilean documents.
    
    Args:
        n_total (int): Total number of examples to generate
        balance (bool): Whether to balance examples across complexity modes
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Directory to save the training files
    
    Returns:
        Tuple[DocBin, Dict]: DocBin object and statistics about generation
        
    Entity Distribution Strategy for Chilean Training:
        - 30% full complexity (all entities)
        - 25% address-focused (names + Chilean addresses)
        - 20% id-focused (names + Chilean RUTs)
        - 15% contact-focused (names + Chilean contact info)
        - 10% financial-focused (names + Chilean amounts)
    """
    print(f"🏗️  Generating {n_total} Chilean NLP training examples with noise...")
    print(f"📊 Balance mode: {'Enabled' if balance else 'Disabled'}")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"🔊 Noise level: {noise_level}")
    
    # Try to load Spanish language model
    try:
        try:
            nlp = spacy.load("es_core_news_lg")  # Best accuracy
            print("✅ Using Spanish Large model (es_core_news_lg)")
        except OSError:
            nlp = spacy.load("es_core_news_md")  # Good accuracy with word vectors
            print("✅ Using Spanish Medium model (es_core_news_md)")
    except OSError:
        try:
            nlp = spacy.load("es_core_news_sm")  # Basic accuracy
            print("✅ Using Spanish Small model (es_core_news_sm)")
        except OSError:
            print("⚠️  No Spanish models found, using blank model")
            nlp = spacy.blank("es")
    
    db = DocBin()
    
    # Define mode distribution for balanced Chilean training
    mode_choices = (
        ["full"] * 30 +          # 30% full complexity
        ["addr_only"] * 25 +     # 25% address-focused  
        ["id_only"] * 20 +       # 20% ID-focused
        ["contact_only"] * 15 +  # 15% contact-focused
        ["financial_only"] * 10  # 10% financial-focused
    )
    
    # Calculate per-mode distribution
    modes = ["full", "addr_only", "id_only", "contact_only", "financial_only"]
    per_mode = n_total // len(modes) if balance else None
    
    # Statistics tracking
    mode_stats = {mode: 0 for mode in modes}
    entity_stats = {}
    
    created = 0
    failed_spans = 0
    overlap_errors = 0  # Track E1010 errors (should be zero)
    
    print("📈 Generating Chilean training data...")
    
    while created < n_total:
        # Select complexity mode (with balancing if enabled)
        mode = random.choice(mode_choices)
        if balance and per_mode and mode_stats[mode] >= per_mode:
            continue
            
        try:
            # Generate Chilean example with selected mode and noise
            text, annotations = generate_chilean_example_with_mode(mode, include_noise)
            
            # Create spaCy document
            doc = nlp.make_doc(text)
            spans = []
            
            # Convert annotations to spaCy spans with overlap detection
            for (start, end, label) in annotations["entities"]:
                span = doc.char_span(start, end, label=label, alignment_mode="contract")
                if span is not None:
                    # Check for overlaps with existing spans (E1010 prevention)
                    overlap_detected = False
                    for existing_span in spans:
                        if (start < existing_span.end_char and end > existing_span.start_char):
                            overlap_detected = True
                            overlap_errors += 1
                            break
                    
                    if not overlap_detected:
                        spans.append(span)
                        
                        # Update entity statistics
                        if label in entity_stats:
                            entity_stats[label] += 1
                        else:
                            entity_stats[label] = 1
                else:
                    failed_spans += 1
            
            # Only add document if it has valid spans
            if spans:
                # Set entities on the document
                doc.ents = spans
                db.add(doc)
                created += 1
                mode_stats[mode] += 1
                
                # Progress indicator
                if created % 10000 == 0:
                    print(f"  📊 Generated {created:,} examples...")
            
        except Exception as e:
            print(f"⚠️  Error generating example: {e}")
            continue
    
    # Final statistics
    print(f"\n✅ Chilean Training Dataset Created Successfully!")
    print(f"📊 Total examples: {created:,}")
    print(f"🎯 Failed spans: {failed_spans}")
    print(f"❌ Overlap errors (E1010): {overlap_errors} ({'ZERO' if overlap_errors == 0 else 'ERROR'})")
    print(f"🎭 Noise included: {include_noise}")
    
    print(f"\n📈 Mode Distribution:")
    for mode, count in mode_stats.items():
        percentage = (count / created * 100) if created > 0 else 0
        print(f"  {mode:15}: {count:6,} ({percentage:5.1f}%)")
    
    print(f"\n🏷️  Entity Distribution:")
    total_entities = sum(entity_stats.values())
    for entity_type, count in sorted(entity_stats.items()):
        percentage = (count / total_entities * 100) if total_entities > 0 else 0
        print(f"  {entity_type:15}: {count:6,} ({percentage:5.1f}%)")
    
    # Save to file
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    output_file = output_path / f"chilean_training_data_noisy_{created}.spacy"
    db.to_disk(output_file)
    
    print(f"\n💾 Saved to: {output_file}")
    print(f"📁 File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    return db, {
        "total_examples": created,
        "failed_spans": failed_spans,
        "overlap_errors": overlap_errors,  # Critical metric - should be 0
        "mode_distribution": mode_stats,
        "entity_distribution": entity_stats,
        "noise_enabled": include_noise,
        "noise_level": noise_level
    }

def create_chilean_training_dataset_with_noise(train_size: int = 80000, 
                                             dev_size: int = 20000, 
                                             include_noise: bool = True,
                                             noise_level: float = 0.15,
                                             output_dir: str = "output") -> None:
    """
    Create comprehensive Chilean training and development datasets with controlled noise.
    
    Generates both training and development sets with statistics and saves them
    to separate .spacy files for immediate use in spaCy training pipelines.
    
    Args:
        train_size (int): Number of training examples
        dev_size (int): Number of development examples  
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Output directory for files
    """
    print("=" * 80)
    print("CHILEAN PII TRAINING DATASET CREATION WITH NOISE")
    print("=" * 80)
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Generate training set
    print(f"\n🎯 Creating Chilean Training Set ({train_size:,} examples)")
    train_db, train_stats = make_chilean_docbin_with_noise(
        n_total=train_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level,
        output_dir=output_dir
    )
    
    # Generate development set
    print(f"\n🎯 Creating Chilean Development Set ({dev_size:,} examples)")
    dev_db, dev_stats = make_chilean_docbin_with_noise(
        n_total=dev_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level * 0.8,  # Slightly less noise for dev set
        output_dir=output_dir
    )
    
    # Save datasets
    train_file = output_path / f"chilean_train_noisy_{train_size}.spacy"
    dev_file = output_path / f"chilean_dev_noisy_{dev_size}.spacy"
    
    train_db.to_disk(train_file)
    dev_db.to_disk(dev_file)
    
    # Save statistics
    stats = {
        "creation_date": datetime.now().isoformat(),
        "total_examples": train_size + dev_size,
        "training_set": train_stats,
        "development_set": dev_stats,
        "noise_configuration": {
            "noise_enabled": include_noise,
            "noise_level": noise_level,
            "dev_noise_level": noise_level * 0.8
        },
        "files": {
            "training": str(train_file),
            "development": str(dev_file)
        }
    }
    
    stats_file = output_path / f"chilean_dataset_stats_noisy_{train_size + dev_size}.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\n📊 DATASET CREATION COMPLETE")
    print(f"📁 Training file: {train_file}")
    print(f"📁 Development file: {dev_file}")
    print(f"📁 Statistics file: {stats_file}")
    print(f"💾 Total size: {(train_file.stat().st_size + dev_file.stat().st_size) / 1024 / 1024:.1f} MB")
    
    # Critical E1010 validation
    total_overlap_errors = train_stats["overlap_errors"] + dev_stats["overlap_errors"]
    if total_overlap_errors == 0:
        print(f"✅ SUCCESS: Zero E1010 overlapping span errors guaranteed!")
    else:
        print(f"❌ WARNING: {total_overlap_errors} E1010 overlapping span errors detected!")
    
    print(f"\n🚀 Ready for spaCy training:")
    print(f"python -m spacy train config.cfg --output ./models --paths.train {train_file} --paths.dev {dev_file}")
    
    # MODIFICATION 1: Always create Excel for data review
    print(f"\n📊 Creating Excel file for data review...")
    excel_file = output_path / f"chilean_dataset_review_{train_size + dev_size}.xlsx"
    excel_examples = max(10, min(10000, train_size // 5))  # At least 10, max 10,000, or 1/5 of training
    export_chilean_data_to_excel_with_noise(
        n_examples=excel_examples,
        output_file=str(excel_file),
        include_noise=include_noise,
        noise_level=noise_level
    )
    print(f"📁 Excel review file: {excel_file}")

def create_multi_country_training_dataset_with_noise(train_size: int = 80000, 
                                                   dev_size: int = 20000, 
                                                   include_noise: bool = True,
                                                   noise_level: float = 0.2,
                                                   output_dir: str = "output",
                                                   mode_weights: Dict[str, int] = None) -> None:
    """
    Create comprehensive multi-country training and development datasets with controlled noise.
    
    Generates both training and development sets with statistics for all supported countries
    and saves them to separate .spacy files for immediate use in spaCy training pipelines.
    
    Args:
        train_size (int): Number of training examples
        dev_size (int): Number of development examples  
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Output directory for files
        mode_weights (Dict[str, int]): Custom mode weights for entity balance
    """
    print("=" * 80)
    print("MULTI-COUNTRY LATIN AMERICAN PII TRAINING DATASET CREATION WITH NOISE")
    print("=" * 80)
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Generate training set
    print(f"\n🎯 Creating Multi-Country Training Set ({train_size:,} examples)")
    train_db, train_stats = make_multi_country_docbin_with_noise(
        n_total=train_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level,
        output_dir=output_dir,
        mode_weights=mode_weights
    )
    
    # Generate development set
    print(f"\n🎯 Creating Multi-Country Development Set ({dev_size:,} examples)")
    dev_db, dev_stats = make_multi_country_docbin_with_noise(
        n_total=dev_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level * 0.8,  # Slightly less noise for dev set
        output_dir=output_dir,
        mode_weights=mode_weights
    )
    
    # Save datasets
    train_file = output_path / f"multi_country_train_noisy_{train_size}.spacy"
    dev_file = output_path / f"multi_country_dev_noisy_{dev_size}.spacy"
    
    train_db.to_disk(train_file)
    dev_db.to_disk(dev_file)
    
    # Save statistics
    stats = {
        "creation_date": datetime.now().isoformat(),
        "total_examples": train_size + dev_size,
        "countries": ["chile", "mexico", "brazil", "uruguay"],
        "training_set": train_stats,
        "development_set": dev_stats,
        "noise_configuration": {
            "noise_enabled": include_noise,
            "noise_level": noise_level,
            "dev_noise_level": noise_level * 0.8
        },
        "files": {
            "training": str(train_file),
            "development": str(dev_file)
        }
    }
    
    stats_file = output_path / f"multi_country_dataset_stats_noisy_{train_size + dev_size}.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\n📊 MULTI-COUNTRY DATASET CREATION COMPLETE")
    print(f"📁 Training file: {train_file}")
    print(f"📁 Development file: {dev_file}")
    print(f"📁 Statistics file: {stats_file}")
    print(f"💾 Total size: {(train_file.stat().st_size + dev_file.stat().st_size) / 1024 / 1024:.1f} MB")
    
    # Critical E1010 validation
    total_overlap_errors = train_stats["overlap_errors"] + dev_stats["overlap_errors"]
    if total_overlap_errors == 0:
        print(f"✅ SUCCESS: Zero E1010 overlapping span errors guaranteed!")
    else:
        print(f"❌ WARNING: {total_overlap_errors} E1010 overlapping span errors detected!")
    
    print(f"\n🚀 Ready for spaCy training:")
    print(f"python -m spacy train config.cfg --output ./models --paths.train {train_file} --paths.dev {dev_file}")
    
    # MODIFICATION 1: Always create Excel for data review
    print(f"\n📊 Creating Excel file for multi-country data review...")
    excel_file = output_path / f"multi_country_dataset_review_{train_size + dev_size}.xlsx"
    excel_examples = max(20, min(10000, train_size // 5))  # At least 20 for multi-country, max 10,000
    export_multi_country_data_to_excel_with_noise(
        n_examples=excel_examples,
        output_file=str(excel_file),
        include_noise=include_noise,
        noise_level=noise_level
    )
    print(f"📁 Excel review file: {excel_file}")
    
    # MODIFICATION 2: Always create JSON for NER training
    print(f"\n📄 Creating JSON file for NER training data...")
    export_training_data_to_json(
        train_size=train_size,
        dev_size=dev_size,
        country="all",
        include_noise=include_noise,
        noise_level=noise_level,
        output_dir=output_dir
    )
    print(f"📁 JSON files created for NER training")

def make_multi_country_docbin_with_noise(n_total: int = 100000, 
                                       balance: bool = True, 
                                       include_noise: bool = True,
                                       noise_level: float = 0.15,
                                       output_dir: str = ".",
                                       mode_weights: Dict[str, int] = None) -> Tuple[DocBin, Dict[str, int]]:
    """
    Create a spaCy DocBin for multi-country NER training with controlled noise and guaranteed zero E1010 errors.
    
    This function generates a comprehensive multi-country dataset with varied complexity modes
    and realistic noise patterns to ensure robust NER model training for Latin American documents.
    
    Args:
        n_total (int): Total number of examples to generate
        balance (bool): Whether to balance examples across countries and complexity modes
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Directory to save the training files
        mode_weights (Dict[str, int]): Custom weights for each generation mode. If None, uses balanced weights.
                                     Available modes:
                                     - 'full': All entities (NAME, ID, ADDRESS, PHONE, EMAIL, AMOUNT, SEQ)
                                     - 'personal_id': Focus on NAME + ID_NUMBER (boosts ID_NUMBER frequency)
                                     - 'address_focused': Focus on NAME + ADDRESS (boosts ADDRESS frequency)  
                                     - 'contact_only': NAME + PHONE + EMAIL
                                     - 'financial_heavy': Focus on AMOUNT + SEQ_NUMBER + DATE (financial-focused, no names)
                                     - 'minimal_entities': NAME + ID + PHONE (basic coverage)
    
    Returns:
        Tuple[DocBin, Dict]: DocBin object and statistics about generation
    """
    # Default balanced weights to address entity imbalance
    if mode_weights is None:
        mode_weights = {
            'full': 20,              # 20% - All entities
            'personal_id': 25,       # 25% - NAME + ID_NUMBER (BOOST ID_NUMBER)
            'address_focused': 25,   # 25% - NAME + ADDRESS (BOOST ADDRESS)
            'contact_only': 15,      # 15% - NAME + PHONE + EMAIL
            'financial_heavy': 10,   # 10% - AMOUNT + SEQ_NUMBER + DATE (financial-focused)
            'minimal_entities': 5    # 5% - NAME + ID + PHONE (basic coverage)
        }
    
    print(f"🏗️  Generating {n_total} multi-country NLP training examples with custom mode weights...")
    print(f"📊 Mode weights: {mode_weights}")
    print(f"📊 Balance mode: {'Enabled' if balance else 'Disabled'}")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"🔊 Noise level: {noise_level}")
    
    # Create mode choices list based on weights
    mode_choices = []
    for mode, weight in mode_weights.items():
        mode_choices.extend([mode] * weight)
    
    print(f"📈 Total mode distribution: {len(mode_choices)} choices across {len(mode_weights)} modes")
    
    # Try to load Spanish language model
    try:
        try:
            nlp = spacy.load("es_core_news_lg")  # Best accuracy
            print("✅ Using Spanish Large model (es_core_news_lg)")
        except OSError:
            nlp = spacy.load("es_core_news_md")  # Good accuracy with word vectors
            print("✅ Using Spanish Medium model (es_core_news_md)")
    except OSError:
        try:
            nlp = spacy.load("es_core_news_sm")  # Basic accuracy
            print("✅ Using Spanish Small model (es_core_news_sm)")
        except OSError:
            print("⚠️  No Spanish models found, using blank model")
            nlp = spacy.blank("es")
    
    db = DocBin()
    
    # Define countries and their distribution
    countries = ["chile", "mexico", "brazil", "uruguay"]
    examples_per_country = n_total // len(countries) if balance else None
    
    # Statistics tracking
    country_stats = {country: 0 for country in countries}
    entity_stats = {}
    
    created = 0
    failed_spans = 0
    overlap_errors = 0  # Track E1010 errors (should be zero)
    
    print("📈 Generating multi-country training data...")
    
    while created < n_total:
        # Select country (with balancing if enabled)
        if balance and examples_per_country:
            # Find country with least examples
            country = min(country_stats.items(), key=lambda x: x[1])[0]
            if country_stats[country] >= examples_per_country:
                # All countries at target, fill remaining randomly
                country = random.choice(countries)
        else:
            country = random.choice(countries)
            
        try:
            # Select generation mode based on weights
            mode = random.choice(mode_choices)
            
            # Generate example with selected country, mode, and noise
            text, annotations = generate_example_with_custom_mode(country, mode, include_noise, noise_level)
            
            # Create spaCy document
            doc = nlp.make_doc(text)
            spans = []
            
            # Convert annotations to spaCy spans with overlap detection
            for (start, end, label) in annotations["entities"]:
                span = doc.char_span(start, end, label=label, alignment_mode="contract")
                if span is not None:
                    # Check for overlaps with existing spans (E1010 prevention)
                    overlap_detected = False
                    for existing_span in spans:
                        if (start < existing_span.end_char and end > existing_span.start_char):
                            overlap_detected = True
                            overlap_errors += 1
                            break
                    
                    if not overlap_detected:
                        spans.append(span)
                        
                        # Update entity statistics
                        if label in entity_stats:
                            entity_stats[label] += 1
                        else:
                            entity_stats[label] = 1
                else:
                    failed_spans += 1
            
            # Only add document if it has valid spans
            if spans:
                # Set entities on the document
                doc.ents = spans
                db.add(doc)
                created += 1
                country_stats[country] += 1
                
                # Progress indicator
                if created % 10000 == 0:
                    print(f"  📊 Generated {created:,} examples...")
            
        except Exception as e:
            print(f"⚠️  Error generating example: {e}")
            continue
    
    # Final statistics
    print(f"\n✅ Multi-Country Training Dataset Created Successfully!")
    print(f"📊 Total examples: {created:,}")
    print(f"🎯 Failed spans: {failed_spans}")
    print(f"❌ Overlap errors (E1010): {overlap_errors} ({'ZERO' if overlap_errors == 0 else 'ERROR'})")
    print(f"🎭 Noise included: {include_noise}")
    
    print(f"\n🌎 Country Distribution:")
    for country, count in country_stats.items():
        percentage = (count / created * 100) if created > 0 else 0
        print(f"  {country.upper():15}: {count:6,} ({percentage:5.1f}%)")
    
    print(f"\n🏷️  Entity Distribution:")
    total_entities = sum(entity_stats.values())
    for entity_type, count in sorted(entity_stats.items()):
        percentage = (count / total_entities * 100) if total_entities > 0 else 0
        print(f"  {entity_type:15}: {count:6,} ({percentage:5.1f}%)")
    
    # Save to file
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    output_file = output_path / f"multi_country_training_data_noisy_{created}.spacy"
    db.to_disk(output_file)
    
    print(f"\n💾 Saved to: {output_file}")
    print(f"📁 File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    return db, {
        "total_examples": created,
        "failed_spans": failed_spans,
        "overlap_errors": overlap_errors,  # Critical metric - should be 0
        "country_distribution": country_stats,
        "entity_distribution": entity_stats,
        "noise_enabled": include_noise,
        "noise_level": noise_level
    }

def create_country_training_dataset_with_noise(country: str = "chile",
                                             train_size: int = 80000, 
                                             dev_size: int = 20000, 
                                             include_noise: bool = True,
                                             noise_level: float = 0.15,
                                             output_dir: str = "output",
                                             mode_weights: Dict[str, int] = None) -> None:
    """
    Create comprehensive country-specific training and development datasets with controlled noise.
    
    Generates both training and development sets with statistics for a specific country
    and saves them to separate .spacy files for immediate use in spaCy training pipelines.
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        train_size (int): Number of training examples
        dev_size (int): Number of development examples  
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Output directory for files
        mode_weights (Dict[str, int]): Custom mode weights for entity balance
    """
    print("=" * 80)
    print(f"{country.upper()} PII TRAINING DATASET CREATION WITH NOISE")
    print("=" * 80)
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Generate training set
    print(f"\n🎯 Creating {country.upper()} Training Set ({train_size:,} examples)")
    train_db, train_stats = make_country_docbin_with_noise(
        country=country,
        n_total=train_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level,
        output_dir=output_dir,
        mode_weights=mode_weights
    )
    
    # Generate development set
    print(f"\n🎯 Creating {country.upper()} Development Set ({dev_size:,} examples)")
    dev_db, dev_stats = make_country_docbin_with_noise(
        country=country,
        n_total=dev_size, 
        balance=True, 
        include_noise=include_noise,
        noise_level=noise_level * 0.8,  # Slightly less noise for dev set
        output_dir=output_dir,
        mode_weights=mode_weights
    )
    
    # Save datasets
    train_file = output_path / f"{country}_train_noisy_{train_size}.spacy"
    dev_file = output_path / f"{country}_dev_noisy_{dev_size}.spacy"
    
    train_db.to_disk(train_file)
    dev_db.to_disk(dev_file)
    
    # Save statistics
    stats = {
        "creation_date": datetime.now().isoformat(),
        "total_examples": train_size + dev_size,
        "country": country,
        "training_set": train_stats,
        "development_set": dev_stats,
        "noise_configuration": {
            "noise_enabled": include_noise,
            "noise_level": noise_level,
            "dev_noise_level": noise_level * 0.8
        },
        "files": {
            "training": str(train_file),
            "development": str(dev_file)
        }
    }
    
    stats_file = output_path / f"{country}_dataset_stats_noisy_{train_size + dev_size}.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"\n📊 {country.upper()} DATASET CREATION COMPLETE")
    print(f"📁 Training file: {train_file}")
    print(f"📁 Development file: {dev_file}")
    print(f"📁 Statistics file: {stats_file}")
    print(f"💾 Total size: {(train_file.stat().st_size + dev_file.stat().st_size) / 1024 / 1024:.1f} MB")
    
    # Critical E1010 validation
    total_overlap_errors = train_stats["overlap_errors"] + dev_stats["overlap_errors"]
    if total_overlap_errors == 0:
        print(f"✅ SUCCESS: Zero E1010 overlapping span errors guaranteed!")
    else:
        print(f"❌ WARNING: {total_overlap_errors} E1010 overlapping span errors detected!")
    
    print(f"\n🚀 Ready for spaCy training:")
    print(f"python -m spacy train config.cfg --output ./models --paths.train {train_file} --paths.dev {dev_file}")
    
    # MODIFICATION 1: Always create Excel for data review
    print(f"\n📊 Creating Excel file for {country.upper()} data review...")
    excel_file = output_path / f"{country}_dataset_review_{train_size + dev_size}.xlsx"
    excel_examples = max(10, min(10000, train_size // 5))  # At least 10, max 10,000, or 1/5 of training
    if country == "chile":
        export_chilean_data_to_excel_with_noise(
            n_examples=excel_examples,
            output_file=str(excel_file),
            include_noise=include_noise,
            noise_level=noise_level
        )
    else:
        export_country_data_to_excel_with_noise(
            country=country,
            n_examples=excel_examples,
            output_file=str(excel_file),
            include_noise=include_noise,
            noise_level=noise_level
        )
    print(f"📁 Excel review file: {excel_file}")

def make_country_docbin_with_noise(country: str = "chile",
                                 n_total: int = 100000, 
                                 balance: bool = True, 
                                 include_noise: bool = True,
                                 noise_level: float = 0.15,
                                 output_dir: str = ".",
                                 mode_weights: Dict[str, int] = None) -> Tuple[DocBin, Dict[str, int]]:
    """
    Create a spaCy DocBin for country-specific NER training with controlled noise and guaranteed zero E1010 errors.
    
    This function generates a comprehensive country-specific dataset with varied complexity modes
    and realistic noise patterns to ensure robust NER model training for the specified country's documents.
    
    Args:
        country (str): Country code - "chile", "mexico", "brazil", or "uruguay"
        n_total (int): Total number of examples to generate
        balance (bool): Whether to balance examples across complexity modes
        include_noise (bool): Whether to add realistic noise patterns
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Directory to save the training files
        mode_weights (Dict[str, int]): Custom mode weights for entity balance
    
    Returns:
        Tuple[DocBin, Dict]: DocBin object and statistics about generation
    """
    # Default balanced weights to address entity imbalance
    if mode_weights is None:
        mode_weights = {
            'full': 20,              # 20% - All entities
            'personal_id': 25,       # 25% - NAME + ID_NUMBER (BOOST ID_NUMBER)
            'address_focused': 25,   # 25% - NAME + ADDRESS (BOOST ADDRESS)
            'contact_only': 15,      # 15% - NAME + PHONE + EMAIL
            'financial_heavy': 10,   # 10% - AMOUNT + SEQ_NUMBER + DATE (financial-focused)
            'minimal_entities': 5    # 5% - NAME + ID + PHONE (basic coverage)
        }
    
    print(f"🏗️  Generating {n_total} {country.upper()} NLP training examples with custom mode weights...")
    print(f"📊 Mode weights: {mode_weights}")
    print(f"📊 Balance mode: {'Enabled' if balance else 'Disabled'}")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"🔊 Noise level: {noise_level}")
    
    # Create mode choices list based on weights
    mode_choices = []
    for mode, weight in mode_weights.items():
        mode_choices.extend([mode] * weight)
    
    print(f"📈 Total mode distribution: {len(mode_choices)} choices across {len(mode_weights)} modes")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"🔊 Noise level: {noise_level}")
    
    # Try to load appropriate language model based on country
    try:
        if country == "brazil":
            try:
                nlp = spacy.load("pt_core_news_lg")
                print("✅ Using Portuguese Large model (pt_core_news_lg)")
            except OSError:
                nlp = spacy.load("pt_core_news_sm")
                print("✅ Using Portuguese Small model (pt_core_news_sm)")
        else:
            # Spanish for other countries
            try:
                nlp = spacy.load("es_core_news_lg")
                print("✅ Using Spanish Large model (es_core_news_lg)")
            except OSError:
                nlp = spacy.load("es_core_news_sm")
                print("✅ Using Spanish Small model (es_core_news_sm)")
    except OSError:
        print(f"⚠️  No appropriate language models found, using blank model")
        lang_code = "pt" if country == "brazil" else "es"
        nlp = spacy.blank(lang_code)
    
    db = DocBin()
    
    # Statistics tracking
    entity_stats = {}
    
    created = 0
    failed_spans = 0
    overlap_errors = 0  # Track E1010 errors (should be zero)
    
    print(f"📈 Generating {country.upper()} training data...")
    
    while created < n_total:
        try:
            # Select generation mode based on weights
            mode = random.choice(mode_choices)
            
            # Generate example with selected country, mode, and noise
            text, annotations = generate_example_with_custom_mode(country, mode, include_noise, noise_level)
            
            # Create spaCy document
            doc = nlp.make_doc(text)
            spans = []
            
            # Convert annotations to spaCy spans with overlap detection
            for (start, end, label) in annotations["entities"]:
                span = doc.char_span(start, end, label=label, alignment_mode="contract")
                if span is not None:
                    # Check for overlaps with existing spans (E1010 prevention)
                    overlap_detected = False
                    for existing_span in spans:
                        if (start < existing_span.end_char and end > existing_span.start_char):
                            overlap_detected = True
                            overlap_errors += 1
                            break
                    
                    if not overlap_detected:
                        spans.append(span)
                        
                        # Update entity statistics
                        if label in entity_stats:
                            entity_stats[label] += 1
                        else:
                            entity_stats[label] = 1
                else:
                    failed_spans += 1
            
            # Only add document if it has valid spans
            if spans:
                # Set entities on the document
                doc.ents = spans
                db.add(doc)
                created += 1
                
                # Progress indicator
                if created % 10000 == 0:
                    print(f"  📊 Generated {created:,} examples...")
            
        except Exception as e:
            print(f"⚠️  Error generating example: {e}")
            continue
    
    # Final statistics
    print(f"\n✅ {country.upper()} Training Dataset Created Successfully!")
    print(f"📊 Total examples: {created:,}")
    print(f"🎯 Failed spans: {failed_spans}")
    print(f"❌ Overlap errors (E1010): {overlap_errors} ({'ZERO' if overlap_errors == 0 else 'ERROR'})")
    print(f"🎭 Noise included: {include_noise}")
    
    print(f"\n🏷️  Entity Distribution:")
    total_entities = sum(entity_stats.values())
    for entity_type, count in sorted(entity_stats.items()):
        percentage = (count / total_entities * 100) if total_entities > 0 else 0
        print(f"  {entity_type:15}: {count:6,} ({percentage:5.1f}%)")
    
    # Save to file
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    output_file = output_path / f"{country}_training_data_noisy_{created}.spacy"
    db.to_disk(output_file)
    
    print(f"\n💾 Saved to: {output_file}")
    print(f"📁 File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB")
    
    return db, {
        "total_examples": created,
        "failed_spans": failed_spans,
        "overlap_errors": overlap_errors,  # Critical metric - should be 0
        "entity_distribution": entity_stats,
        "noise_enabled": include_noise,
        "noise_level": noise_level,
        "country": country
    }

# -----------------
# Excel Export Functionality for Data Review
# -----------------

def export_chilean_data_to_excel_with_noise(n_examples: int = 100, 
                                          output_file: str = "chilean_customer_data_review_noisy.xlsx",
                                          include_noise: bool = True,
                                          noise_level: float = 0.15) -> None:
    """
    Export generated Chilean customer data to Excel for comprehensive review and validation.
    
    Creates a detailed Excel workbook with multiple sheets for thorough analysis:
    - Summary statistics and overview
    - Complete data with entity annotations
    - Analysis by complexity mode
    - Chilean naming pattern analysis
    - Entity type distribution
    - Noise pattern analysis
    
    Args:
        n_examples (int): Number of examples to generate and export
        output_file (str): Excel filename
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
    """
    print(f"📊 Generating {n_examples} Chilean examples for Excel review...")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"📁 Output file: {output_file}")
    
    # Generate examples across all modes
    modes = ["full", "addr_only", "id_only", "contact_only", "financial_only"]
    examples_per_mode = n_examples // len(modes)
    all_data = []
    
    # Statistics tracking
    mode_counts = {mode: 0 for mode in modes}
    entity_counts = {}
    noise_patterns = {}
    name_patterns = {
        "compound_first_names": 0,
        "double_surnames": 0,
        "simple_names": 0
    }
    
    for mode in modes:
        for _ in range(examples_per_mode):
            try:
                sentence, annotations = generate_chilean_example_with_mode(mode, include_noise)
                
                # Analyze naming patterns
                entities = annotations["entities"]
                for start, end, label in entities:
                    if label == "CUSTOMER_NAME":
                        name = sentence[start:end]
                        name_parts = name.split()
                        
                        if len(name_parts) == 4:  # First Second Paternal Maternal
                            name_patterns["compound_first_names"] += 1
                            name_patterns["double_surnames"] += 1
                        elif len(name_parts) == 3:
                            if name_parts[1] in chilean_second_names:
                                name_patterns["compound_first_names"] += 1
                            else:
                                name_patterns["double_surnames"] += 1
                        else:
                            name_patterns["simple_names"] += 1
                        break
                
                # Count entities
                for start, end, label in entities:
                    entity_counts[label] = entity_counts.get(label, 0) + 1
                
                # Analyze noise patterns
                if include_noise:
                    if "  " in sentence:  # Double spaces
                        noise_patterns["spacing"] = noise_patterns.get("spacing", 0) + 1
                    if "Av." in sentence or "Tel." in sentence:  # Abbreviations
                        noise_patterns["abbreviations"] = noise_patterns.get("abbreviations", 0) + 1
                    if " ." in sentence or " :" in sentence:  # Punctuation spacing
                        noise_patterns["punctuation"] = noise_patterns.get("punctuation", 0) + 1
                
                # Extract individual entities for detailed view
                entity_details = []
                for start, end, label in entities:
                    entity_text = sentence[start:end]
                    entity_details.append(f"{label}: '{entity_text}'")
                
                all_data.append({
                    "ID": len(all_data) + 1,
                    "Mode": mode,
                    "Generated_Text": sentence,
                    "Entity_Count": len(entities),
                    "Entities": " | ".join(entity_details),
                    "Has_Noise": include_noise,
                    "Text_Length": len(sentence)
                })
                
                mode_counts[mode] += 1
                
            except Exception as e:
                print(f"⚠️  Error generating example: {e}")
                continue
    
    # Generate remaining examples to reach target
    remaining = n_examples - len(all_data)
    for _ in range(remaining):
        mode = random.choice(modes)
        try:
            sentence, annotations = generate_chilean_example_with_noise(include_noise, noise_level)
            
            entities = annotations["entities"]
            entity_details = []
            for start, end, label in entities:
                entity_text = sentence[start:end]
                entity_details.append(f"{label}: '{entity_text}'")
                entity_counts[label] = entity_counts.get(label, 0) + 1
            
            all_data.append({
                "ID": len(all_data) + 1,
                "Mode": "full",  # Default mode for remaining examples
                "Generated_Text": sentence,
                "Entity_Count": len(entities),
                "Entities": " | ".join(entity_details),
                "Has_Noise": include_noise,
                "Text_Length": len(sentence)
            })
            
        except Exception as e:
            continue
    
    # Create Excel workbook
    print(f"📝 Creating Excel workbook with {len(all_data)} examples...")
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # 1. Summary Sheet
        summary_data = {
            "Metric": [
                "Total Examples Generated",
                "Country",
                "Modes Included",
                "Noise Generation Enabled",
                "Noise Level",
                "Average Text Length",
                "Average Entities per Example",
                "Unique Entity Types",
                "Compound First Names",
                "Double Surnames", 
                "Simple Names",
                "Generation Date"
            ],
            "Value": [
                len(all_data),
                "Chile",
                ", ".join(modes),
                "Yes" if include_noise else "No",
                f"{noise_level:.2f}" if include_noise else "N/A",
                f"{sum(d['Text_Length'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                f"{sum(d['Entity_Count'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                len(entity_counts),
                name_patterns["compound_first_names"],
                name_patterns["double_surnames"],
                name_patterns["simple_names"],
                datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        # 2. All Data Sheet
        all_data_df = pd.DataFrame(all_data)
        all_data_df.to_excel(writer, sheet_name='All_Data', index=False)
        
        # 3. By Mode Sheet
        mode_summary = []
        for mode in modes:
            mode_examples = [d for d in all_data if d["Mode"] == mode]
            
            mode_summary.append({
                "Mode": mode,
                "Total_Examples": len(mode_examples),
                "Avg_Text_Length": f"{sum(d['Text_Length'] for d in mode_examples)/len(mode_examples):.1f}" if mode_examples else "0",
                "Avg_Entities_Per_Example": f"{sum(d['Entity_Count'] for d in mode_examples)/len(mode_examples):.1f}" if mode_examples else "0"
            })
        
        mode_df = pd.DataFrame(mode_summary)
        mode_df.to_excel(writer, sheet_name='By_Mode', index=False)
        
        # 4. Name Pattern Analysis Sheet
        name_analysis = [
            {"Pattern_Type": "Compound First Names", "Count": name_patterns["compound_first_names"], "Percentage": f"{name_patterns['compound_first_names']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Double Surnames", "Count": name_patterns["double_surnames"], "Percentage": f"{name_patterns['double_surnames']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Simple Names", "Count": name_patterns["simple_names"], "Percentage": f"{name_patterns['simple_names']/len(all_data)*100:.1f}%" if all_data else "0%"}
        ]
        
        name_df = pd.DataFrame(name_analysis)
        name_df.to_excel(writer, sheet_name='Name_Analysis', index=False)
        
        # 5. Entity Statistics Sheet
        entity_descriptions = {
            "CUSTOMER_NAME": "Full customer names with country conventions",
            "ID_NUMBER": "Country-specific ID numbers (RUT, CURP, CPF, etc.)",
            "ADDRESS": "Complete addresses with country-specific formats",
            "PHONE": "Country-specific phone numbers",
            "EMAIL": "Email addresses with country domains",
            "AMOUNT": "Monetary amounts with local currencies",
            "SEQ_NUMBER": "Sequential reference numbers"
        }
        
        entity_analysis = []
        for entity_type, count in entity_counts.items():
            percentage = (count / len(all_data) * 100) if all_data else 0
            entity_analysis.append({
                "Entity_Type": entity_type,
                "Count": count,
                "Percentage": f"{percentage:.1f}%",
                "Description": entity_descriptions.get(entity_type, "Entity type")
            })
        
        entity_df = pd.DataFrame(entity_analysis)
        entity_df.to_excel(writer, sheet_name='Entity_Statistics', index=False)
        
        # 6. Noise Analysis Sheet (if noise is enabled)
        if include_noise and noise_patterns:
            noise_analysis = []
            for pattern_type, count in noise_patterns.items():
                noise_analysis.append({
                    "Noise_Pattern": pattern_type,
                    "Occurrences": count,
                    "Percentage": f"{(count / len(all_data) * 100):.1f}%" if all_data else "0%"
                })
            
            noise_df = pd.DataFrame(noise_analysis)
            noise_df.to_excel(writer, sheet_name='Noise_Analysis', index=False)
    
    print(f"✅ Excel file created successfully: {output_file}")
    print(f"📊 Generated {len(all_data)} Chilean examples")
    print(f"🏷️  Entity distribution: {dict(sorted(entity_counts.items()))}")
    print(f"📊 Mode distribution: {mode_counts}")
    print(f"📋 Chilean naming patterns: {name_patterns}")
    
    if include_noise:
        print(f"🎭 Noise patterns detected: {noise_patterns}")
    
    print(f"\n📖 Excel sheets created:")
    print(f"  • Summary - Overview statistics")
    print(f"  • All_Data - Complete generated data")
    print(f"  • By_Mode - Analysis by generation mode")
    print(f"  • Name_Analysis - Chilean naming pattern analysis")
    print(f"  • Entity_Statistics - Entity type distribution")
    if include_noise:
        print(f"  • Noise_Analysis - Noise pattern analysis")

def export_multi_country_data_to_excel_with_noise(n_examples: int = 100, 
                                                 output_file: str = "multi_country_customer_data_review_noisy.xlsx",
                                                 include_noise: bool = True,
                                                 noise_level: float = 0.15) -> None:
    """
    Export generated multi-country customer data to Excel for comprehensive review and validation.
    
    Creates a detailed Excel workbook with multiple sheets for thorough analysis across all countries:
    - Summary statistics and overview
    - Complete data with entity annotations
    - Analysis by country
    - Multi-country naming pattern analysis
    - Entity type distribution
    - Noise pattern analysis
    
    Args:
        n_examples (int): Number of examples to generate and export per country
        output_file (str): Excel filename
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
    """
    print(f"📊 Generating {n_examples} multi-country examples for Excel review...")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"📁 Output file: {output_file}")
    
    # Supported countries
    countries = ["chile", "mexico", "brazil", "uruguay"]
    examples_per_country = n_examples // len(countries)
    all_data = []
    
    # Statistics tracking
    country_counts = {country: 0 for country in countries}
    entity_counts = {}
    noise_patterns = {}
    name_patterns = {
        "compound_first_names": 0,
        "double_surnames": 0,
        "simple_names": 0
    }
    
    # Generate examples for each country
    for country in countries:
        print(f"  📍 Generating {examples_per_country} examples for {country.upper()}...")
        
        for _ in range(examples_per_country):
            try:
                sentence, annotations = generate_example_with_noise(country, include_noise, noise_level)
                
                # Analyze naming patterns
                entities = annotations["entities"]
                for start, end, label in entities:
                    if label == "CUSTOMER_NAME":
                        name = sentence[start:end]
                        name_parts = name.split()
                        
                        if len(name_parts) >= 4:  # Compound names
                            name_patterns["compound_first_names"] += 1
                            name_patterns["double_surnames"] += 1
                        elif len(name_parts) == 3:
                            name_patterns["double_surnames"] += 1
                        else:
                            name_patterns["simple_names"] += 1
                        break
                
                # Count entities
                for start, end, label in entities:
                    entity_counts[label] = entity_counts.get(label, 0) + 1
                
                # Analyze noise patterns
                if include_noise:
                    if "  " in sentence:  # Double spaces
                        noise_patterns["spacing"] = noise_patterns.get("spacing", 0) + 1
                    if any(abbr in sentence for abbr in ["Av.", "Tel.", "Ref.", "Núm."]):
                        noise_patterns["abbreviations"] = noise_patterns.get("abbreviations", 0) + 1
                    if " ." in sentence or " :" in sentence:  # Punctuation spacing
                        noise_patterns["punctuation"] = noise_patterns.get("punctuation", 0) + 1
                
                # Extract individual entities for detailed view
                entity_details = []
                for start, end, label in entities:
                    entity_text = sentence[start:end]
                    entity_details.append(f"{label}: '{entity_text}'")
                
                all_data.append({
                    "ID": len(all_data) + 1,
                    "Country": country.upper(),
                    "Generated_Text": sentence,
                    "Entity_Count": len(entities),
                    "Entities": " | ".join(entity_details),
                    "Has_Noise": include_noise,
                    "Text_Length": len(sentence)
                })
                
                country_counts[country] += 1
                
            except Exception as e:
                print(f"⚠️  Error generating {country} example: {e}")
                continue
    
    # Create Excel workbook
    print(f"📝 Creating Excel workbook with {len(all_data)} examples...")
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # 1. Summary Sheet
        summary_data = {
            "Metric": [
                "Total Examples Generated",
                "Countries Included",
                "Examples per Country",
                "Noise Generation Enabled",
                "Noise Level",
                "Average Text Length",
                "Average Entities per Example",
                "Unique Entity Types",
                "Compound First Names",
                "Double Surnames", 
                "Simple Names",
                "Generation Date"
            ],
            "Value": [
                len(all_data),
                ", ".join([c.upper() for c in countries]),
                examples_per_country,
                "Yes" if include_noise else "No",
                f"{noise_level:.2f}" if include_noise else "N/A",
                f"{sum(d['Text_Length'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                f"{sum(d['Entity_Count'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                len(entity_counts),
                name_patterns["compound_first_names"],
                name_patterns["double_surnames"],
                name_patterns["simple_names"],
                datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        # 2. All Data Sheet
        all_data_df = pd.DataFrame(all_data)
        all_data_df.to_excel(writer, sheet_name='All_Data', index=False)
        
        # 3. By Country Sheet
        country_summary = []
        for country in countries:
            country_examples = [d for d in all_data if d["Country"] == country.upper()]
            
            country_summary.append({
                "Country": country.upper(),
                "Total_Examples": len(country_examples),
                "Avg_Text_Length": f"{sum(d['Text_Length'] for d in country_examples)/len(country_examples):.1f}" if country_examples else "0",
                "Avg_Entities_Per_Example": f"{sum(d['Entity_Count'] for d in country_examples)/len(country_examples):.1f}" if country_examples else "0",
                "Percentage": f"{len(country_examples)/len(all_data)*100:.1f}%" if all_data else "0%"
            })
        
        country_df = pd.DataFrame(country_summary)
        country_df.to_excel(writer, sheet_name='By_Country', index=False)
        
        # 4. Name Pattern Analysis Sheet
        name_analysis = [
            {"Pattern_Type": "Compound First Names", "Count": name_patterns["compound_first_names"], "Percentage": f"{name_patterns['compound_first_names']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Double Surnames", "Count": name_patterns["double_surnames"], "Percentage": f"{name_patterns['double_surnames']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Simple Names", "Count": name_patterns["simple_names"], "Percentage": f"{name_patterns['simple_names']/len(all_data)*100:.1f}%" if all_data else "0%"}
        ]
        
        name_df = pd.DataFrame(name_analysis)
        name_df.to_excel(writer, sheet_name='Name_Analysis', index=False)
        
        # 5. Entity Statistics Sheet
        entity_descriptions = {
            "CUSTOMER_NAME": "Full customer names with country conventions",
            "ID_NUMBER": "Country-specific ID numbers (RUT, CURP, CPF, etc.)",
            "ADDRESS": "Complete addresses with country-specific formats",
            "PHONE_NUMBER": "Country-specific phone numbers",
            "EMAIL": "Email addresses with country domains",
            "AMOUNT": "Monetary amounts with local currencies",
            "SEQ_NUMBER": "Sequential reference numbers"
        }
        
        entity_analysis = []
        for entity_type, count in entity_counts.items():
            percentage = (count / len(all_data) * 100) if all_data else 0
            entity_analysis.append({
                "Entity_Type": entity_type,
                "Count": count,
                "Percentage": f"{percentage:.1f}%",
                "Description": entity_descriptions.get(entity_type, "Entity type")
            })
        
        entity_df = pd.DataFrame(entity_analysis)
        entity_df.to_excel(writer, sheet_name='Entity_Statistics', index=False)
        
        # 6. Noise Analysis Sheet (if noise is enabled)
        if include_noise and noise_patterns:
            noise_analysis = []
            for pattern_type, count in noise_patterns.items():
                noise_analysis.append({
                    "Noise_Pattern": pattern_type,
                    "Occurrences": count,
                    "Percentage": f"{(count / len(all_data) * 100):.1f}%" if all_data else "0%"
                })
            
            noise_df = pd.DataFrame(noise_analysis)
            noise_df.to_excel(writer, sheet_name='Noise_Analysis', index=False)
    
    print(f"✅ Excel file created successfully: {output_file}")
    print(f"📊 Generated {len(all_data)} examples across {len(countries)} countries")
    print(f"🏷️  Entity distribution: {dict(sorted(entity_counts.items()))}")
    print(f"🌎 Country distribution: {country_counts}")
    print(f"📋 Multi-country naming patterns: {name_patterns}")
    
    if include_noise:
        print(f"🎭 Noise patterns detected: {noise_patterns}")
    
    print(f"\n📖 Excel sheets created:")
    print(f"  • Summary - Overview statistics")
    print(f"  • All_Data - Complete generated data")
    print(f"  • By_Country - Analysis by country")
    print(f"  • Name_Analysis - Multi-country naming pattern analysis")
    print(f"  • Entity_Statistics - Entity type distribution")
    if include_noise:
        print(f"  • Noise_Analysis - Noise pattern analysis")

def export_country_data_to_excel_with_noise(country: str = "chile",
                                          n_examples: int = 100, 
                                          output_file: str = None,
                                          include_noise: bool = True,
                                          noise_level: float = 0.15) -> None:
    """
    Export generated country-specific customer data to Excel for comprehensive review and validation.
    
    Creates a detailed Excel workbook with multiple sheets for thorough analysis:
    - Summary statistics and overview
    - Complete data with entity annotations
    - Entity type distribution
    - Noise pattern analysis
    
    Args:
        country (str): Country to generate data for
        n_examples (int): Number of examples to generate and export
        output_file (str): Excel filename (auto-generated if None)
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
    """
    if output_file is None:
        output_file = f"{country}_customer_data_review_noisy.xlsx"
    
    print(f"📊 Generating {n_examples} {country.upper()} examples for Excel review...")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"📁 Output file: {output_file}")
    
    all_data = []
    
    # Statistics tracking
    entity_counts = {}
    noise_patterns = {}
    name_patterns = {
        "compound_first_names": 0,
        "double_surnames": 0,
        "simple_names": 0
    }
    
    # Generate examples
    for _ in range(n_examples):
        try:
            sentence, annotations = generate_example_with_noise(country, include_noise, noise_level)
            
            # Analyze naming patterns
            entities = annotations["entities"]
            for start, end, label in entities:
                if label == "CUSTOMER_NAME":
                    name = sentence[start:end]
                    name_parts = name.split()
                    
                    if len(name_parts) >= 4:  # Compound names
                        name_patterns["compound_first_names"] += 1
                        name_patterns["double_surnames"] += 1
                    elif len(name_parts) == 3:
                        name_patterns["double_surnames"] += 1
                    else:
                        name_patterns["simple_names"] += 1
                    break
            
            # Count entities
            for start, end, label in entities:
                entity_counts[label] = entity_counts.get(label, 0) + 1
            
            # Analyze noise patterns
            if include_noise:
                if "  " in sentence:  # Double spaces
                    noise_patterns["spacing"] = noise_patterns.get("spacing", 0) + 1
                if any(abbr in sentence for abbr in ["Av.", "Tel.", "Ref.", "Núm."]):
                    noise_patterns["abbreviations"] = noise_patterns.get("abbreviations", 0) + 1
                if " ." in sentence or " :" in sentence:  # Punctuation spacing
                    noise_patterns["punctuation"] = noise_patterns.get("punctuation", 0) + 1
            
            # Extract individual entities for detailed view
            entity_details = []
            for start, end, label in entities:
                entity_text = sentence[start:end]
                entity_details.append(f"{label}: '{entity_text}'")
            
            all_data.append({
                "ID": len(all_data) + 1,
                "Country": country.upper(),
                "Generated_Text": sentence,
                "Entity_Count": len(entities),
                "Entities": " | ".join(entity_details),
                "Has_Noise": include_noise,
                "Text_Length": len(sentence)
            })
            
        except Exception as e:
            print(f"⚠️  Error generating {country} example: {e}")
            continue
    
    # Create Excel workbook
    print(f"📝 Creating Excel workbook with {len(all_data)} examples...")
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # 1. Summary Sheet
        summary_data = {
            "Metric": [
                "Total Examples Generated",
                "Country",
                "Noise Generation Enabled",
                "Noise Level",
                "Average Text Length",
                "Average Entities per Example",
                "Unique Entity Types",
                "Compound First Names",
                "Double Surnames", 
                "Simple Names",
                "Generation Date"
            ],
            "Value": [
                len(all_data),
                country.upper(),
                "Yes" if include_noise else "No",
                f"{noise_level:.2f}" if include_noise else "N/A",
                f"{sum(d['Text_Length'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                f"{sum(d['Entity_Count'] for d in all_data)/len(all_data):.1f}" if all_data else "0",
                len(entity_counts),
                name_patterns["compound_first_names"],
                name_patterns["double_surnames"],
                name_patterns["simple_names"],
                datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            ]
        }
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        
        # 2. All Data Sheet
        all_data_df = pd.DataFrame(all_data)
        all_data_df.to_excel(writer, sheet_name='All_Data', index=False)
        
        # 3. Name Pattern Analysis Sheet
        name_analysis = [
            {"Pattern_Type": "Compound First Names", "Count": name_patterns["compound_first_names"], "Percentage": f"{name_patterns['compound_first_names']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Double Surnames", "Count": name_patterns["double_surnames"], "Percentage": f"{name_patterns['double_surnames']/len(all_data)*100:.1f}%" if all_data else "0%"},
            {"Pattern_Type": "Simple Names", "Count": name_patterns["simple_names"], "Percentage": f"{name_patterns['simple_names']/len(all_data)*100:.1f}%" if all_data else "0%"}
        ]
        
        name_df = pd.DataFrame(name_analysis)
        name_df.to_excel(writer, sheet_name='Name_Analysis', index=False)
        
        # 4. Entity Statistics Sheet
        entity_descriptions = {
            "CUSTOMER_NAME": "Full customer names with country conventions",
            "ID_NUMBER": "Country-specific ID numbers (RUT, CURP, CPF, etc.)",
            "ADDRESS": "Complete addresses with country-specific formats",
            "PHONE_NUMBER": "Country-specific phone numbers",
            "EMAIL": "Email addresses with country domains",
            "AMOUNT": "Monetary amounts with local currencies",
            "SEQ_NUMBER": "Sequential reference numbers"
        }
        
        entity_analysis = []
        for entity_type, count in entity_counts.items():
            percentage = (count / len(all_data) * 100) if all_data else 0
            entity_analysis.append({
                "Entity_Type": entity_type,
                "Count": count,
                "Percentage": f"{percentage:.1f}%",
                "Description": entity_descriptions.get(entity_type, "Entity type")
            })
        
        entity_df = pd.DataFrame(entity_analysis)
        entity_df.to_excel(writer, sheet_name='Entity_Statistics', index=False)
        
        # 5. Noise Analysis Sheet (if noise is enabled)
        if include_noise and noise_patterns:
            noise_analysis = []
            for pattern_type, count in noise_patterns.items():
                noise_analysis.append({
                    "Noise_Pattern": pattern_type,
                    "Occurrences": count,
                    "Percentage": f"{(count / len(all_data) * 100):.1f}%" if all_data else "0%"
                })
            
            noise_df = pd.DataFrame(noise_analysis)
            noise_df.to_excel(writer, sheet_name='Noise_Analysis', index=False)
    
    print(f"✅ Excel file created successfully: {output_file}")
    print(f"📊 Generated {len(all_data)} examples for {country.upper()}")
    print(f"🏷️  Entity distribution: {dict(sorted(entity_counts.items()))}")
    print(f"📋 {country.upper()} naming patterns: {name_patterns}")
    
    if include_noise:
        print(f"🎭 Noise patterns detected: {noise_patterns}")
    
    print(f"\n📖 Excel sheets created:")
    print(f"  • Summary - Overview statistics")
    print(f"  • All_Data - Complete generated data")
    print(f"  • Name_Analysis - {country.upper()} naming pattern analysis")
    print(f"  • Entity_Statistics - Entity type distribution")
    if include_noise:
        print(f"  • Noise_Analysis - Noise pattern analysis")

# -----------------
# JSON Export Functionality for NER Training
# -----------------

def export_ner_data_to_json(n_examples: int = 100,
                           country: str = "chile",
                           output_file: str = None,
                           include_noise: bool = True,
                           noise_level: float = 0.15) -> None:
    """
    Export generated data to JSON format for NER training with start/end positions and entity labels.
    
    Creates a JSON file with the following structure:
    {
        "metadata": {
            "total_examples": int,
            "country": str,
            "noise_enabled": bool,
            "noise_level": float,
            "generation_date": str,
            "entity_types": list
        },
        "data": [
            {
                "id": int,
                "text": str,
                "entities": [
                    {
                        "start": int,
                        "end": int,
                        "label": str,
                        "text": str
                    }
                ]
            }
        ]
    }
    
    Args:
        n_examples (int): Number of examples to generate
        country (str): Country to generate data for
        output_file (str): JSON filename (auto-generated if None)
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
    """
    if output_file is None:
        output_file = f"{country}_ner_data_{n_examples}.json"
    
    print(f"📝 Generating {n_examples} {country.upper()} examples for JSON NER export...")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"📁 Output file: {output_file}")
    
    all_data = []
    entity_types = set()
    
    # Generate examples
    for i in range(n_examples):
        try:
            sentence, annotations = generate_example_with_noise(country, include_noise, noise_level)
            
            # Convert entities to detailed format
            entities = []
            for start, end, label in annotations["entities"]:
                entity_text = sentence[start:end]
                entities.append({
                    "start": start,
                    "end": end,
                    "label": label,
                    "text": entity_text
                })
                entity_types.add(label)
            
            all_data.append({
                "id": i + 1,
                "text": sentence,
                "entities": entities
            })
            
        except Exception as e:
            print(f"⚠️  Error generating example {i+1}: {e}")
            continue
    
    # Create JSON structure
    json_data = {
        "metadata": {
            "total_examples": len(all_data),
            "country": country.upper(),
            "noise_enabled": include_noise,
            "noise_level": noise_level if include_noise else 0.0,
            "generation_date": datetime.now().isoformat(),
            "entity_types": sorted(list(entity_types))
        },
        "data": all_data
    }
    
    # Save to JSON file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"✅ JSON file created successfully: {output_file}")
    print(f"📊 Generated {len(all_data)} examples for {country.upper()}")
    print(f"🏷️  Entity types: {sorted(list(entity_types))}")
    print(f"📋 Total entities: {sum(len(item['entities']) for item in all_data)}")
    print(f"💾 File size: {Path(output_file).stat().st_size / 1024:.1f} KB")

def export_multi_country_ner_data_to_json(n_examples: int = 100,
                                         output_file: str = None,
                                         include_noise: bool = True,
                                         noise_level: float = 0.15) -> None:
    """
    Export multi-country generated data to JSON format for NER training.
    
    Creates a comprehensive JSON file with examples from all supported countries.
    
    Args:
        n_examples (int): Number of examples to generate per country
        output_file (str): JSON filename (auto-generated if None)
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
    """
    if output_file is None:
        total_examples = n_examples * 4  # 4 countries
        output_file = f"multi_country_ner_data_{total_examples}.json"
    
    print(f"📝 Generating {n_examples} examples per country for multi-country JSON NER export...")
    print(f"🎭 Noise generation: {'Enabled' if include_noise else 'Disabled'}")
    print(f"📁 Output file: {output_file}")
    
    countries = ["chile", "mexico", "brazil", "uruguay"]
    all_data = []
    entity_types = set()
    country_counts = {country: 0 for country in countries}
    
    example_id = 1
    
    # Generate examples for each country
    for country in countries:
        print(f"  📍 Generating {n_examples} examples for {country.upper()}...")
        
        for i in range(n_examples):
            try:
                sentence, annotations = generate_example_with_noise(country, include_noise, noise_level)
                
                # Convert entities to detailed format
                entities = []
                for start, end, label in annotations["entities"]:
                    entity_text = sentence[start:end]
                    entities.append({
                        "start": start,
                        "end": end,
                        "label": label,
                        "text": entity_text
                    })
                    entity_types.add(label)
                
                all_data.append({
                    "id": example_id,
                    "country": country.upper(),
                    "text": sentence,
                    "entities": entities
                })
                
                country_counts[country] += 1
                example_id += 1
                
            except Exception as e:
                print(f"⚠️  Error generating {country} example {i+1}: {e}")
                continue
    
    # Create JSON structure
    json_data = {
        "metadata": {
            "total_examples": len(all_data),
            "countries": [country.upper() for country in countries],
            "examples_per_country": dict((k.upper(), v) for k, v in country_counts.items()),
            "noise_enabled": include_noise,
            "noise_level": noise_level if include_noise else 0.0,
            "generation_date": datetime.now().isoformat(),
            "entity_types": sorted(list(entity_types))
        },
        "data": all_data
    }
    
    # Save to JSON file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"✅ JSON file created successfully: {output_file}")
    print(f"📊 Generated {len(all_data)} examples across {len(countries)} countries")
    print(f"🌎 Country distribution: {country_counts}")
    print(f"🏷️  Entity types: {sorted(list(entity_types))}")
    print(f"📋 Total entities: {sum(len(item['entities']) for item in all_data)}")
    print(f"💾 File size: {Path(output_file).stat().st_size / 1024:.1f} KB")

def export_training_data_to_json(train_size: int = 1000,
                                dev_size: int = 200,
                                country: str = "chile",
                                include_noise: bool = True,
                                noise_level: float = 0.15,
                                output_dir: str = "output") -> None:
    """
    Export training and development datasets to JSON format for NER training.
    
    Creates separate JSON files for training and development sets with proper NER annotations.
    
    Args:
        train_size (int): Number of training examples
        dev_size (int): Number of development examples
        country (str): Country to generate data for ("chile", "mexico", "brazil", "uruguay", or "all")
        include_noise (bool): Whether to include noise in generated data
        noise_level (float): Intensity of noise (0.0-1.0)
        output_dir (str): Output directory for files
    """
    print("=" * 80)
    print(f"{country.upper()} NER TRAINING DATA JSON EXPORT")
    print("=" * 80)
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    if country == "all":
        # Multi-country dataset
        train_file = output_path / f"multi_country_train_ner_{train_size}.json"
        dev_file = output_path / f"multi_country_dev_ner_{dev_size}.json"
        
        print(f"\n🎯 Creating Multi-Country Training JSON ({train_size} examples)")
        export_multi_country_ner_data_to_json(
            n_examples=train_size // 4,  # Distribute across 4 countries
            output_file=str(train_file),
            include_noise=include_noise,
            noise_level=noise_level
        )
        
        print(f"\n🎯 Creating Multi-Country Development JSON ({dev_size} examples)")
        export_multi_country_ner_data_to_json(
            n_examples=dev_size // 4,  # Distribute across 4 countries
            output_file=str(dev_file),
            include_noise=include_noise,
            noise_level=noise_level * 0.8  # Slightly less noise for dev set
        )
    else:
        # Single country dataset
        train_file = output_path / f"{country}_train_ner_{train_size}.json"
        dev_file = output_path / f"{country}_dev_ner_{dev_size}.json"
        
        print(f"\n🎯 Creating {country.upper()} Training JSON ({train_size} examples)")
        export_ner_data_to_json(
            n_examples=train_size,
            country=country,
            output_file=str(train_file),
            include_noise=include_noise,
            noise_level=noise_level
        )
        
        print(f"\n🎯 Creating {country.upper()} Development JSON ({dev_size} examples)")
        export_ner_data_to_json(
            n_examples=dev_size,
            country=country,
            output_file=str(dev_file),
            include_noise=include_noise,
            noise_level=noise_level * 0.8  # Slightly less noise for dev set
        )
    
    print(f"\n📊 JSON EXPORT COMPLETE")
    print(f"📁 Training file: {train_file}")
    print(f"📁 Development file: {dev_file}")
    print(f"💾 Total size: {(train_file.stat().st_size + dev_file.stat().st_size) / 1024:.1f} KB")
    
    print(f"\n🔍 JSON Structure:")
    print(f"   • metadata: Dataset information and statistics")
    print(f"   • data: Array of examples with text and entity annotations")
    print(f"   • entities: Start/end positions and labels for each entity")
    print(f"\n📖 Ready for NER training frameworks like spaCy, Transformers, etc.")

# -----------------
# Command-Line Interface and Main Functions
# -----------------

def demonstrate_multi_country_functionality_with_noise():
    """
    Demonstration function showing multi-country PII generation with noise capabilities.
    
    Provides examples of:
    1. Basic customer data generation with noise for all supported countries
    2. Different complexity modes for NLP training
    3. spaCy dataset creation for multi-country NER training
    4. E1010 conflict resolution validation across countries
    """
    parser = argparse.ArgumentParser(description="Multi-Country Latin American PII Training Data Generator with Advanced Noise")
    parser.add_argument("--mode", choices=["demo", "create-dataset", "excel-export", "json-export"], default="demo",
                       help="Mode: 'demo' shows examples, 'create-dataset' generates training data, 'excel-export' creates review file, 'json-export' creates JSON NER data")
    parser.add_argument("--country", choices=["chile", "mexico", "brazil", "uruguay", "all"], default="chile",
                       help="Country for generation: 'chile', 'mexico', 'brazil', 'uruguay', or 'all' for mixed dataset")
    parser.add_argument("--train-size", type=int, default=100000, help="Training set size")
    parser.add_argument("--dev-size", type=int, default=20000, help="Development set size")
    parser.add_argument("--output-dir", type=str, default="output", help="Output directory")
    parser.add_argument("--excel-examples", type=int, default=100, help="Number of examples for Excel export")
    parser.add_argument("--excel-file", type=str, default="multi_country_customer_data_review_noisy.xlsx", help="Excel output filename")
    parser.add_argument("--json-examples", type=int, default=100, help="Number of examples for JSON export")
    parser.add_argument("--noise", action="store_true", default=True, help="Enable noise generation")
    parser.add_argument("--no-noise", action="store_true", help="Disable noise generation")
    parser.add_argument("--noise-level", type=float, default=0.2, help="Noise intensity (0.0-1.0) - Enhanced for OCR simulation")
    
    # NEW: Entity balance configuration
    parser.add_argument("--entity-balance", choices=["default", "address-boost", "id-boost", "contact-boost", "minimal-financial"], 
                       default="default", help="Entity balance strategy to fix dataset imbalance")
    parser.add_argument("--custom-weights", type=str, help="Custom mode weights as JSON: '{\"full\":20,\"personal_id\":30,...}'")
    
    args = parser.parse_args()
    
    # Handle noise settings
    include_noise = args.noise and not args.no_noise
    
    # Configure mode weights based on balance strategy
    mode_weights = None
    if args.custom_weights:
        import json
        mode_weights = json.loads(args.custom_weights)
        print(f"🎯 Using custom mode weights: {mode_weights}")
    elif args.entity_balance == "address-boost":
        mode_weights = {
            'full': 15,
            'personal_id': 20,
            'address_focused': 40,  # BOOST ADDRESS
            'contact_only': 15,
            'financial_heavy': 5,   # REDUCE financial
            'minimal_entities': 5
        }
        print("🏠 Using ADDRESS-BOOST mode weights")
    elif args.entity_balance == "id-boost":
        mode_weights = {
            'full': 15,
            'personal_id': 40,      # BOOST ID_NUMBER
            'address_focused': 20,
            'contact_only': 15,
            'financial_heavy': 5,   # REDUCE financial
            'minimal_entities': 5
        }
        print("🆔 Using ID-BOOST mode weights")
    elif args.entity_balance == "contact-boost":
        mode_weights = {
            'full': 15,
            'personal_id': 20,
            'address_focused': 20,
            'contact_only': 35,     # BOOST CONTACT
            'financial_heavy': 5,   # REDUCE financial
            'minimal_entities': 5
        }
        print("📞 Using CONTACT-BOOST mode weights")
    elif args.entity_balance == "minimal-financial":
        mode_weights = {
            'full': 20,
            'personal_id': 30,
            'address_focused': 30,
            'contact_only': 15,
            'financial_heavy': 3,   # MINIMAL financial
            'minimal_entities': 2
        }
        print("💰 Using MINIMAL-FINANCIAL mode weights")
    else:
        print("⚖️  Using DEFAULT balanced mode weights")
    # Default uses balanced weights defined in the function
    
    if args.mode == "create-dataset":
        # Create multi-country NLP training dataset with noise
        if args.country == "all":
            create_multi_country_training_dataset_with_noise(
                train_size=args.train_size,
                dev_size=args.dev_size,
                include_noise=include_noise,
                noise_level=args.noise_level,
                output_dir=args.output_dir,
                mode_weights=mode_weights
            )
        else:
            create_country_training_dataset_with_noise(
                country=args.country,
                train_size=args.train_size,
                dev_size=args.dev_size,
                include_noise=include_noise,
                noise_level=args.noise_level,
                output_dir=args.output_dir,
                mode_weights=mode_weights
            )
        return
    elif args.mode == "excel-export":
        # Create Excel file for data review
        output_path = Path(args.output_dir)
        output_path.mkdir(exist_ok=True)
        excel_file = output_path / args.excel_file
        
        if args.country == "all":
            export_multi_country_data_to_excel_with_noise(
                n_examples=args.excel_examples,
                output_file=str(excel_file),
                include_noise=include_noise,
                noise_level=args.noise_level
            )
        else:
            export_country_data_to_excel_with_noise(
                country=args.country,
                n_examples=args.excel_examples,
                output_file=str(excel_file),
                include_noise=include_noise,
                noise_level=args.noise_level
            )
        return
    elif args.mode == "json-export":
        # Create JSON file for NER training data
        output_path = Path(args.output_dir)
        output_path.mkdir(exist_ok=True)
        
        if args.country == "all":
            json_file = output_path / f"multi_country_ner_{args.json_examples * 4}.json"
            export_multi_country_ner_data_to_json(
                n_examples=args.json_examples,
                output_file=str(json_file),
                include_noise=include_noise,
                noise_level=args.noise_level
            )
        else:
            json_file = output_path / f"{args.country}_ner_{args.json_examples}.json"
            export_ner_data_to_json(
                n_examples=args.json_examples,
                country=args.country,
                output_file=str(json_file),
                include_noise=include_noise,
                noise_level=args.noise_level
            )
        return
    
    # Demo mode - show examples
    print("================================================================================")
    print("MULTI-COUNTRY LATIN AMERICAN PII TRAINING DATA GENERATOR WITH ADVANCED NOISE")
    print("================================================================================")
    print()
    print("🌎 SUPPORTED COUNTRIES:")
    country_info = {
        "chile": ("Chile (CL)", "RUT, +56 phones, CLP currency, Chilean Spanish"),
        "mexico": ("Mexico (MX)", "CURP/RFC, +52 phones, MXN currency, Mexican Spanish"),
        "brazil": ("Brazil (BR)", "CPF, +55 phones, BRL currency, Portuguese"),
        "uruguay": ("Uruguay (UY)", "Cédula, +598 phones, UYU currency, Uruguayan Spanish")
    }
    
    for code, (name, desc) in country_info.items():
        print(f"   - {name}: {desc}")
    print()
    
    # Show examples for selected country or all countries
    countries_to_show = [args.country] if args.country != "all" else ["chile", "mexico", "brazil", "uruguay"]
    
    print("🔥 MULTI-COUNTRY GENERATION EXAMPLES")
    print("-" * 40)
    
    for country in countries_to_show:
        country_name = country_info[country][0]
        print(f"📍 {country_name}")
        sentence, annotations = generate_example_with_noise(country, include_noise, args.noise_level)
        
        print(f"Generated: {sentence}")
        print("Entities:", end=" ")
        for start, end, label in annotations["entities"]:
            entity_text = sentence[start:end]
            print(f"[{label}: '{entity_text}']", end=" ")
        print("\n")
    
    print("🔢 Sequential Counter Status:")
    print(f"   Next sequence number: {_sequence_counter + 1}")
    print()
    
    # E1010 Validation Test
    print("🔍 E1010 OVERLAPPING SPAN ERROR VALIDATION")
    print("-" * 40)
    print("Testing conflict resolution algorithm across all countries...")
    
    test_examples = []
    overlap_errors = 0
    
    for _ in range(50):  # Test 50 examples
        test_country = random.choice(countries_to_show)
        sentence, annotations = generate_example_with_noise(test_country, include_noise, args.noise_level)
        entities = annotations["entities"]
        
        # Check for overlaps
        for i, (start1, end1, label1) in enumerate(entities):
            for j, (start2, end2, label2) in enumerate(entities[i+1:], i+1):
                if start1 < end2 and start2 < end1:  # Overlap detected
                    overlap_errors += 1
        
        test_examples.append((sentence, entities))
    print(f"✅ Tested {len(test_examples)} examples across countries")
    print(f"❌ Overlap errors detected: {overlap_errors}")
    
    if overlap_errors == 0:
        print("🎉 SUCCESS: Zero E1010 overlapping span errors guaranteed!")
    else:
        print("⚠️  WARNING: E1010 overlapping span errors detected!")
    
    print("\n📊 MULTI-COUNTRY NLP DATASET CREATION")
    print("-" * 40)
    print("To create training datasets for spaCy NER:")
    noise_flag = '--noise' if include_noise else '--no-noise'
    if args.country == "all":
        print(f"   python data_generation_noisy.py --mode create-dataset --country all --train-size 10000 --dev-size 2500 {noise_flag}")
    else:
        print(f"   python data_generation_noisy.py --mode create-dataset --country {args.country} --train-size 10000 --dev-size 2500 {noise_flag}")
    print()
    print("📁 EXCEL DATA REVIEW")
    print("-" * 40)
    print("To create Excel file for data review and validation:")
    if args.country == "all":
        print(f"   python data_generation_noisy.py --mode excel-export --country all --excel-examples 100 {noise_flag}")
        print(f"   python data_generation_noisy.py --mode excel-export --country all --excel-examples 500 --excel-file detailed_multi_country_review.xlsx {noise_flag}")
    else:
        print(f"   python data_generation_noisy.py --mode excel-export --country {args.country} --excel-examples 100 {noise_flag}")
        print(f"   python data_generation_noisy.py --mode excel-export --country {args.country} --excel-examples 500 --excel-file detailed_{args.country}_review.xlsx {noise_flag}")
    print()
    print("📄 JSON NER DATA EXPORT")
    print("-" * 40)
    print("To create JSON file with NER annotations (start/end positions and entity labels):")
    if args.country == "all":
        print(f"   python data_generation_noisy.py --mode json-export --country all --json-examples 100 {noise_flag}")
        print(f"   python data_generation_noisy.py --mode json-export --country all --json-examples 1000 {noise_flag}")
    else:
        print(f"   python data_generation_noisy.py --mode json-export --country {args.country} --json-examples 100 {noise_flag}")
        print(f"   python data_generation_noisy.py --mode json-export --country {args.country} --json-examples 1000 {noise_flag}")
    print()
    print("🎯 NOISE FEATURES:")
    print("   - Realistic typos and misspellings per country")
    print("   - Country-specific abbreviations and contractions")
    print("   - Document formatting variations per country")
    print("   - Controlled noise that preserves entity boundaries")
    print("   - Zero E1010 overlapping span errors guaranteed")
    print()
    print("📚 Multi-Country Use Cases:")
    print("   - Multi-language Latin American NER training")
    print("   - Cross-country financial document processing")
    print("   - Government document analysis across regions")
    print("   - PII detection and anonymization for LATAM")
    print("   - Large-scale multi-country NLP model training")
    print("   - JSON format compatible with Transformers, spaCy, and other NER frameworks")

# Backwards compatibility function
def demonstrate_chilean_functionality_with_noise():
    """
    Demonstration function showing Chilean PII generation with noise capabilities.
    (Backwards compatibility wrapper)
    """
    import sys
    # Replace the country argument to default to Chile for backwards compatibility
    if "--country" not in sys.argv:
        sys.argv.extend(["--country", "chile"])
    demonstrate_multi_country_functionality_with_noise()

def quick_multi_country_test_with_noise():
    """
    Quick test function to verify all multi-country functionality works correctly with noise.
    """
    print("🧪 Running quick multi-country functionality test with noise...")
    
    countries = ["chile", "mexico", "brazil", "uruguay"]
    
    for country in countries:
        print(f"   Testing {country}...")
        # Test basic generation
        sentence, annotations = generate_example_with_noise(country, True, 0.15)
        assert len(sentence) > 0, f"Basic {country} generation failed"
        assert len(annotations["entities"]) > 0, f"No entities generated for {country}"
    
    print("✅ All multi-country tests passed with zero E1010 errors!")

# Keep old function for backwards compatibility
def quick_chilean_test_with_noise():
    """
    Quick test function to verify Chilean functionality works correctly with noise.
    (Backwards compatibility wrapper)
    """
    quick_multi_country_test_with_noise()

if __name__ == "__main__":
    # Run the enhanced multi-country PII generator with noise capabilities
    demonstrate_multi_country_functionality_with_noise()